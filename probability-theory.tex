
\section{Raw material}


\paragraph{Monotonic transformations of RVs.} A monotonic transformation of
a real valued random variable with continuous distribution function has
continuous distribution function. 

\url{http://math.stackexchange.com/questions/78101/monotonic-transformation-of-continuous-random-variable-are-continuous}



\paragraph{Independence. }
Let $(\Omega, \mathcal A, P)$ be a probablity space and let $\mathcal F$ and
$\mathcal G$ be two independent sub-$\sigma$-algebras of $\mathcal A$.

\begin{enumerate} 
\item If $X: \Omega \to (\R, \mathcal B)$ is a
    real-valued random variable and $X$ is both $\mathcal F$-measurable
    and $\mathcal G$-measurable then $X$ is a.s.~constant.
\end{enumerate}
\paragraph{} $X$ und $Y$ seien unabhängige Zufallsvariablen. $X$ sei symmetrisch, 
d.h.\ $X$ und $-X$ sind gleichverteilt. Dann gilt
\begin{equation}
    \E \| X \|^p \leq \E \| X + Y \|^p.
\end{equation}
Gilt die obige Ungleichung, wenn $X$ und $Y$ nicht unabhängig sind oder wenn 
$X$ nicht symmetrisch verteilt ist?

\paragraph*{Solution. } The real problem here is to show that if $X \upmodels Z$
and $Y \upmodels Z$ the $X+Z \sim Y+Z$. 


\section{Discrete distributions}

\paragraph{Bernoulliverteilung. Momente. }
Sei $X$ Bernoulliverteilt mit dem Parameter $p \in \left( 0,1 \right)$. Es gilt also
$X: \Omega \to \left\{ 0,1 \right\}$ und $P(X=1)=p$. 
\begin{enumerate}
    \item Die Momente von $X$ sind gegeben durch
        \begin{equation*}
            \E X^n = p, \quad n\in\bN.
        \end{equation*}
    \item Die momentenerzeugende Funktion von $X$ ist 
        \begin{equation*}
            \psi(u) = \E \exp \left( u X \right) = p\exp (u)+1-p.
        \end{equation*} 
\end{enumerate}
\paragraph*{Lösung.} Aus $X = X^n$ für alle $n\in \bN$, erhalten wir  
\begin{align*}
    \E X = \E X^n = 1\, p + 0\, (1-p) = p.
\end{align*}
Auf direktem Wege erhalten wir auch
\begin{equation*}
    \psi(u) = \E \exp (uX) = \exp\left( u\, 1 \right)p + \exp(0)(1-p).
\end{equation*}



\paragraph{Binomialverteilung als Summe von unabhängigen Bernoulli Zufallsvariablen.}
Seien $X_1, \cdots, X_n$, $n\in\bN$, i.i.d.\ und Bernoulliverteilt mit dem Parameter $p\in (0,1)$. 
Dann ist
\begin{equation*}
    Y=\sum_{i=1}^{n} X_i \sim \text{Bin}(n,p)
\end{equation*}
binomialverteilt mit Parametern $(n,p)$. D.h., $P(Y = k) = \binom{n}{k}  p^{k}(1-p)^{n-k}$, 
für $k\in \left\{ 0, \cdots, n \right\}$.

\paragraph{Binomialverteilung. Momente und momentenerzeugende Funktion.}
Sei $X$ binomialverteilt mit den Parametern $(n,p)$, $n\in\N$ und $p\in(0,1)$. Dann gilt
\begin{enumerate}
    \item \begin{equation*}
            \E X = np. 
        \end{equation*}
    \item Die momentenerzeugende Funktion von $X$ ist gegeben durch
        \begin{equation*}
            \psi(u) = \E \exp( uX) = (1 -p + p e^u)^{n}.
        \end{equation*}
\end{enumerate}
\paragraph*{Lösung. }
Nachdem $X$ die Summe unabhängiger Bernoulliverteilter Zufallsvariablen $B_1,
\cdots, B_n$ ist, können wir die Linearität des Erwartungswertes benutzen, und
erhalten
\begin{equation*}
    \E X = \E B_1 + \cdots \E B_n = np. 
\end{equation*}
Zufallsvariablen $A$ und $B$ sind genau dann unabhängig, wenn 
\begin{equation*}
    \E f(A)g(B) = \E f(A)\, \E g(B)
\end{equation*}
für alle beschränkte messbare Funktionen gilt.\footnote{Beachten Sie, dass $\E
AB = \E A\, \E B$ in meisten Fällen nicht ausreichend für Unabhängigkeit ist!}
Wir können also die zuvor benutzte Methode wieder einsetzen. 
\begin{equation*}
    \E \exp (u X) = \E \prod_{i=1}^{n} e^{u B_i} 
    = \prod_{i=1}^{n} \E e^{u B_i} 
    = \left[ e^{u}p + 1 -p \right]^{n}.
\end{equation*}
Dieses Ergebnis lässt sich auch mit Hilfe der Wahrscheinlichkeitsfunktion der 
Binomialverteilung herleiten. 
\begin{align*}
    \E \exp (u X) &= \sum_{k=0}^{n} e^{u k} \binom{n}{k} p^{k}(1-p)^{n-k} \\
    &= \sum_{k=0}^{n} \binom{n}{k} (p e^{u})^{k}(1-p)^{n-k} \\
    &= \left[ e^u p + 1-p \right]^n.
\end{align*}

\paragraph{Poisson-Verteilung. Momente. } 
Sei $X$ Poisson-verteilt mit dem Parameter $\lambda>0$. Das bedeutet, $X:\Omega\to\bN$
hat die Wahrscheinlichkeitsfunktion 
\begin{equation*}
    P(X = k) = \frac{\lambda^{k}}{k!} e^{-\lambda}.
\end{equation*}
Es gilt
\begin{enumerate}
    \item 
        \begin{equation*}
            \E X = \Var X = \lambda.
        \end{equation*}
    \item Die momentenerzeugende Funktion von $X$ ist gegeben durch
        \begin{equation*}
            \psi(u) = \E \exp(uX) = \exp \left( \lambda(e^u - 1) \right).
        \end{equation*}
\end{enumerate}


\section{Continuous distributions}

\paragraph{Momente der Exponentialverteilung.} 
Sei $X$ exponentialverteilt mit dem Parameter $\lambda>0$. $X$ hat also die Dichte
\begin{equation*}
    p(x) = \lambda e^{-\lambda x } 1_{\R_{\geq 0}}(x)
\end{equation*}
\begin{enumerate}
    \item Die Verteilungsfunktion von $X$ ist gegeben durch
        \begin{equation*}
            P( X \leq x) = \left[ 1 - e^{-\lambda x} \right] 1_{\R_{\geq 0}}(x).
        \end{equation*}
    \item Die Momente der Exponentialverteilung sind für $n\in\bN$ durch
        \begin{equation*}
            \E X^n = \frac{n!}{\lambda^{n}}
        \end{equation*}
        gegeben. 
    \item Der Erwartungswert und die Varianz der Exponentialverteilung sind
        \begin{align*}
            \E X    & = \frac{1}{\lambda} & 
            \Var X  & = \frac{1}{\lambda^2}.
        \end{align*}
\end{enumerate}
\paragraph*{Lösung. } 
Wir berechnen
\begin{align*}
    P(X \leq x) & = 
    \int_{-\infty}^{x} \lambda e^{-\lambda x } 1_{\R_{\geq 0}}(x) dx \\
    &= \int_{0}^{x} \lambda e^{-\lambda x } dx \\
    &= \left[  - e^{-\lambda x}  \right] |_{0}^{x} \\
    &= \left[ 1 - e^{-\lambda x} \right] 1_{\R_{\geq 0}}(x).
\end{align*}
Die Momente erhalten wir mit Hilfe der partiellen Integration. 
\begin{align*}
    \E X^n &= \int_{0}^{\infty} x^n \lambda e^{- \lambda x} dx \\
    &= \int_{0}^{\infty} x^n\left( -e^{-\lambda x} \right)' dx \\
    &= x^n \left( - e^{-\lambda x} \right)|_{0}^{\infty} 
    - \int_{0}^{\infty} n x^{n-1} \left( -e^{-\lambda x} \right) dx  \\
    &= \frac{n}{\lambda} \int_{0}^{\infty} x^{n-1}\, \lambda e^{-\lambda x} dx \\
    & = \frac{n}{\lambda} \E X^{n-1}.
\end{align*}
Mit
\begin{equation*}
    \E X^{0} = \E 1 = 1
\end{equation*}
erhalten wir $\E X^n = \frac{n!}{\lambda^n}$ und daraus 
\begin{align*}
    E X &= \frac{1}{\lambda} &   \Var X &= \E X^2 - \left( \E X \right)^2 = 
    \frac{1}{\lambda^2}. 
\end{align*}


\paragraph{Gedächtnislosigkeit der Exponentialverteilung.}
Sei $X$ exponentialverteilt mit dem Parameter $\lambda>0$. Dann gilt für
$x,h>0$ 
\begin{equation*}
    P\left( X > x+h \vb X > x \right) = P( X > h).
\end{equation*}
Warum wird diese Eigenschaft von $X$ Gedächtnislosigkeit genannt?

\paragraph{Momentenerzeugende Funktion der Exponentialverteilung. }
Sei $X$ exponentialverteilt mit dem Parameter $\lambda>0$. 
\begin{enumerate}
    \item Die momentenerzeugende Funktion von $X$ ist 
        \begin{equation*}
            \psi (u) = \E \exp \left( u X \right) = - \frac{\lambda}{u - \lambda}
        \end{equation*}
        für $|u| < \lambda$. 
    \item Berechnen Sie mit Hilfe von $\psi(u)$ den Erwartungswert und die Varianz von $X$. 
\end{enumerate}
\paragraph*{Lösung.} 
Wir berechnen
\begin{align*}
    \psi(u) = \E \exp \left( u X \right) &= 
    \int_{\R}^{} \exp \left( ux \right) \lambda e^{-\lambda x} 1_{\R_{\geq 0}}(x) dx \\
    &= \lambda \int_{0}^{\infty} e^{(u-\lambda)x} dx \\
    &= \lambda \left( \frac{1}{u-\lambda} e^{(u-\lambda)x} \right) |_{0}^{\infty}\\
    &= -\frac{\lambda}{u- \lambda}. 
\end{align*}
Um die Momente zu erhalten, brauchen wir nur noch die momentenerzeugende Funktion 
zu differenzieren. 
\begin{align*}
    \E X = \frac{\partial}{ \partial u} \psi (u) |_{u=0} &= \frac{1}{\lambda} \\
    \E X^2 = \frac{\partial^2}{\partial u^2} \psi (u) |_{u=0} &=
    \frac{\partial}{\partial u} \frac{\lambda}{(u-\lambda)^2} = \frac{2}{\lambda^2}. 
\end{align*}

\paragraph{Minimum von exponentialverteilten Zufallsvariablen.}
Seien $X_1, \cdots, X_n$ i.i.d.\ und exponentialverteilt mit $X\sim \text{Exp}(\lambda_i)$
und $\lambda_i>0$. Dann ist die Zufallsvariable 
\begin{equation*}
    Y = \min \left\{ X_1, \cdots, X_n \right\} 
\end{equation*}
wieder exponentialverteilt und es gilt $Y \sim
\text{Exp}(\lambda_1+\cdots+\lambda_n)$.

\paragraph{Gleichverteilung und Exponentialverteilung. } Sei $X$ gleichverteilt
auf dem Intervall $[0,1]$, dann ist $Y = -\log X$ exponentialverteilt.

\paragraph*{Lösung. } $Y$ ist exponentialverteilt mit $\lambda=1$.


\paragraph{Stetige Gleichverteilung.}
Sei $X$ stetig gleichverteilt auf dem Intervall $\left( a,b \right)\subset \R$.  
Dies kann als $X \sim U(a,b)$ notiert werden. 
\begin{enumerate}
    \item Der $n$-te Moment $\E X^{n}$, für $n\in\bN$, ist 
        \begin{equation*}
            \E X^n = \frac{1}{n+1}\left( b^{n}a^0 + \cdots + b^{0}a^{n} \right).
        \end{equation*}
    \item Der Erwartungswert und die Varianz von $X$ sind gegeben durch
        \begin{align*}
            \E X    & = \frac{1}{2} \left( b+a \right) & 
            \Var X  & = \frac{1}{12} \left( b-a \right)^{2}.  
        \end{align*}
    \item Die momentenerzeugende Funktion von $X$ ist 
        \begin{equation*}
            \psi(u) = \E \exp \left( u X \right) = \frac{1}{u(b-a)}\left( e^{ub} - e^{ua} \right).
        \end{equation*}
\end{enumerate}
\paragraph*{Lösung.} Berechnen wir zunächst die $n$-ten Momente
\begin{align*}
    \E X^n &= \int_{\R}^{} x^n \frac{1}{b-a} 1_{(a,b)}(x) dx \\
    &= \frac{1}{b-a} \int_{a}^{b} x^n dx \\
    &= \frac{1}{b-a} \left( \frac{x^{n+1}}{n+1} \right) |_{a}^{b} \\
    &= \frac{1}{b-a}\frac{1}{n+1} \left( b^{n+1}-a^{n+1} \right) \\
    &= \frac{a^n b^{0} +\cdots+ a^0 b^{n}}{n+1}.
\end{align*}
Wir brauchen nur noch $n=1,2$ einzusetzen.
\begin{align*}
    \E X &= \frac{a+b}{2} & \E X^2 &= \frac{b^2 + ab + a^2}{3} & 
    \Var X = \frac{(b-a)^2}{12}.
\end{align*}
Die momentenerzeugende Funktion kann folgendermassen berechnet werden. 
\begin{align*}
    \psi(u) = \E \exp\left( uX \right) &= 
    \int_{\R} \exp \left( ux \right) \frac{1}{b-a} 1_{(a,b)}(x) dx \\
    &= \frac{1}{b-a} \int_{a}^{b} \exp (ux) dx \\
    &= \frac{e^{ub}-e^{ua}}{u(b-a)}.
\end{align*}
Beachten Sie, dass diese Funktion an Null beliebig oft differenzierbar ist. 


\paragraph{Normalverteilung. Momentenerzeugende Funktion und Momente.}
Sei $X \sim \cN \left( \mu, \sigma^{2} \right)$ mit $\mu\in\R$ und $\sigma>0$. 
Die Dichte von $X$ ist also
\begin{equation*}
    f(x) = \frac{1}{\sigma \sqrt{2\pi}}
    \exp \left( -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2\right).
\end{equation*}
Dann gilt
\begin{enumerate}
    \item Die momentenerzeugende Funktion von $X$ ist gegeben durch 
        \begin{equation*}
            \psi(u) = \exp \left( \frac{1}{2} \sigma u^2 + \mu u \right).
        \end{equation*}
    \item 
        \begin{align*}
            \E X &= \mu, & \Var X &= \sigma^2.
        \end{align*}
    \item Die Zufallsvariable $Z = \frac{X-\mu}{\sigma}$ ist standardnormalverteilt.
\end{enumerate}
\paragraph*{Lösung.} Wir berechnen die momentenerzeugende Funktion der
Normalverteilung indem wir das Integral $\E \exp (u X)$ auf ein Integral der
Dichte einer Normalverteilung zurückführen. 
\begin{align*}
    \psi(u) = \E \exp (u X)
    &= \int_{\R}^{} e^{ux } \frac{1}{\sigma \sqrt{2\pi}} 
    e^{ -\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2} dx \\
    &= \int_{\R}^{} \frac{1}{\sigma \sqrt{2\pi}} 
    \exp \left( ux - \frac{1}{2} \frac{(x-\mu)^2}{\sigma^2} \right) dx
\end{align*}
Nun gilt für den Ausdruck im Exponenten
\begin{align*}
    ux - \frac{1}{2} \frac{(x-\mu)^2}{\sigma^2} &= 
    -\frac{1}{2\sigma^2} \left( -2 u\sigma^2 x + x^2 - 2\mu x + \mu^2 \right) \\
    &= -\frac{1}{2\sigma^2} \left( x^2 - 2( \sigma^2 u +\mu  )x + \mu^2  \right)  \\
    &= -\frac{1}{2\sigma^2} \left( \left( x - (\sigma^2 u + \mu) \right)^2 
    - (\sigma^2 u + \mu)^2 + \mu^2  \right).
\end{align*}
Eingesetzt in die obige Formel für $\psi(u)$ ergibt das 
\begin{align*}
    \psi(u) &=  \exp \left( \frac{ (\sigma^2 u + \mu)^2 - \mu^2 }{2 \sigma^2} \right)
    \int_{\R}^{} \frac{1}{\sigma \sqrt{2\pi}} 
    \exp \left( - \frac{1}{2} \frac{(x- \sigma^2 u + \mu)^2}{\sigma^2} \right) dx \\
    &= \exp \left( \frac{1}{2} \sigma^2 u^2 + u \mu \right).
\end{align*}
Der Erwartungswert und die Varianz der Normalverteilung lassen sich nun durchs
Differenzieren berechnen. 
\begin{align*}
    \partial_u \psi(u) &= \psi(u) \left( \sigma^2 u + \mu \right) &\impl && \E X &= \mu \\
    \partial_{uu} \psi(u) &= \psi(u) \left( \sigma^2 u + \mu \right)^2 + \psi(u) \sigma^2
    & \impl && \E X^2 &= \mu^2 + \sigma^2
\end{align*}
Daraus erhalten wir 
\begin{align*}
    \Var X = \sigma^2.
\end{align*}
Wenden wir uns nun $Z = \frac{X - \mu}{\sigma}$ zu. Wir zeigen $Z \sim \cN(0,1)$ 
indem wir den Erwartungswert und die Varianz von $Z$ berechnen. 
\begin{align*}
    \E Z &= \frac{1}{\sigma} \E (X- \mu) = \frac{1}{\sigma} ( \mu - \mu) = 0 \\
    \Var Z &= \E \left\{ \left( Z - \E Z \right)^2 \right\} \\ 
    &= \E \left( \frac{X - \mu}{\sigma} \right)^2 \\
    &= \frac{1}{\sigma^2} \Var X = \frac{\sigma^2}{\sigma^2} = 1.
\end{align*}

\paragraph{Gerade Momente der Standardnormalverteilung. } Zeigen Sie, dass für eine
standardnormalverteilte Zufallsvariable $X$ und $n \in \N$
\begin{equation*}
    E\left( X^{2n} \right) = \frac{(2n)!}{2^n \cdot n!}
\end{equation*}
gilt. %\cite{czadoschmidt} % A1.11
\paragraph*{Lösung.}
Die momentenerzeugende Funktion der Standardnormalverteilung ist 
\begin{equation*}
    \psi(u) = \exp \left( \frac{u^2}{2} \right).
\end{equation*}
Diese Funktion lässt sich leicht ableiten. 
\begin{align*}
    \psi^{(1)} &= \psi u \\
    \psi^{(2)} &= \psi^{(1)} u + \psi \\
    \psi^{(3)} &= \psi^{(2)} u + 2 \psi^{(1)} \\
    \psi^{(4)} &= \psi^{(3)} u + 3 \psi^{(2)} \\
    \psi^{(2n)} &= \psi^{(2n-1)} u + (2n-1) \psi^{(2n-2)}.
\end{align*}
Wenn wir $u=0$ einsetzen erhalten wir die Relation
\begin{equation*}
    \left. \psi^{(2n)} \right|_{u=0}^{} =
    \psi_{0}^{(2n)} = (2n-1)\psi^{2n-2}_{0}
\end{equation*}
und daraus
\begin{equation*}
    \psi_{0}^{(2n)} = (2n-1)(2n-3) \cdots 1 = \frac{ 2n!}{ (2n)(2n-2)\cdots 2  } = \frac{2n!}{ 2^n n!}.
\end{equation*}



\paragraph{Summen von normalverteilten Zufallsvariablen. }
Sind die Zufallsvariablen $X_1,\ldots,X_n$ unabhängig und normalverteilt
mit $X_i \sim \mathcal N (\nu_i, \sigma_i^2)$, so ist die Summe
wieder normalverteilt mit 
\begin{equation*}
    \sum_{i=1}^{n} X_i \sim 
    \mathcal N \left( \sum_{i=1}^{n} \nu_i, \sum_{i=1}^{n} \sigma_i^2 \right). 
\end{equation*} %\cite{czadoschmidt} %(A 1.31)

\paragraph*{Lösung. }
Wir berechnen die momentenerzeugende Funktion der Zufallsvariable $\sum_{i=1}^{n} X_i$
und verwenden dabei die Unabhängigkeit von $X_1, \cdots, X_n$.
\begin{align*}
    \E \exp \left( u \sum_{i=1}^{n} X_i \right) 
    &= \prod_{i=1}^{n} \E \exp \left( u X_i \right) \\
    &= \prod_{i=1}^n \exp \left( \frac{\sigma_i^2 u^2 }{2} + \nu_i u \right) \\
    &= \exp \left( \frac{ u^2 \sum_{i=1}^{n} \sigma_i^2 }{2} + u \sum_{i=1}^{n} \nu_i \right).
\end{align*}
Die momentenerzeugende Funktion, wenn sie in einer Umgebung von $0$ existiert, 
charakterisiert eine Verteilung eindeutig. Die oben berechnete momentenerzeugende Funktion
gehört zu einer Normalverteilung mit dem Erwartungswert $\sum_{i=1}^{n} \nu_i$ und
der Varianz $\sum_{i=1}^{n} \sigma^2_i$. 


\paragraph{Dichte der multivariaten Normalverteilung. } Zeigen Sie, dass 
$X \sim \mathcal N_p\left( \mu, \Sigma \right) $ folgende Dichte hat, falls 
$\textrm{Rang}(\Sigma)=p$: 
\begin{equation*}
    p(x) = \frac{1}{ \textrm{det}(\Sigma)^{1/2} (2\pi)^{p/2}} \exp\left( - \frac{1}{2} (x  - \mu)^\top \Sigma^{-1} (x - \mu) \right).
\end{equation*} %\cite{czadoschmidt} %(A 1.37)

\paragraph{Gausschen Zufallsvariablen und Unabhängigteit. } Seien $X$ und $Y$
standard normalverteilt und unabhängig. Dann sind die Zufallsvariablen $U =
\frac{X-Y}{\sqrt{2}}$ und $V=\frac{X+Y}{\sqrt{2}}$ ebenfalls standard
normalverteilt und unabhängig. 


\paragraph{Beta-verteilung.} Sei $X$ eine Beta$(2,2)$-verteilte Zufallsvariable.
\begin{enumerate}
    \item Geben Sie die Dichte und Verteilungsfunktion von $X$ explicit an. 
    \item Geben Sie die Dichte und Verteilungsfuntion der Zufallsvariable
        $Y = \frac{1}{X} - 1$ an. 
    \item Berechnen Sie $\E Y$.  
\end{enumerate}


\paragraph{Gamma-Verteilung. Momente.}
Gegeben seien i.i.d.\ Gamma-verteilte Zufallsvariablen $X_1,\ldots,X_m$ mit der Dichte
\begin{equation*}
    p_{a,\lambda}(x) = \frac{\lambda^a}{\Gamma(a)}x^{a-1}e^{-\lambda x} 1_{\R_{\geq 0}}(x)
\end{equation*}
und Parametern $a>0$ und $\lambda>0$.
Zeigen Sie für ein positives $n\in\mathbb N$
\begin{equation*}
    E X_i^n = \frac{a\cdot\ldots\cdot (n+a-1)}{\lambda^n}.
\end{equation*}
Dabei können Sie die Identität 
$\Gamma(a+1)=a \Gamma(a)$, $a\in\R \setminus \left\{ -1,-2,\ldots \right\}$, verwenden.
[Klausur WS11]

\paragraph*{Lösung.}
\begin{eqnarray}
    E X^n &=& \int_{0}^{\infty} x^n \frac{\lambda^a}{\Gamma(a)}x^{a-1}e^{-\lambda x} dx \\
    &=& \frac{a \cdot\ldots\cdot \left( a+n-1 \right)}{\lambda^n} 
    \int_{0}^{\infty} \frac{\lambda^{a+n}}{\Gamma(n+a)} x^{a+n-1} e^{-\lambda x} dx \\
    &=& \frac{a\cdot \ldots \cdot \left( a+n-1 \right)}{\lambda^n} \\
    &=& \frac{\Gamma\left( a+n \right)}{ \lambda^n \Gamma\left( a \right)}.
\end{eqnarray}


\paragraph{Momentenerzeugende Funktion einer Gamma-Verteilung. }
Sei $X \sim \textrm{Gamma}(a, \lambda)$. Zeigen Sie, dass für $s<\lambda$
\begin{equation*}
    \Psi_X(s) = E \left( e^{sX} \right) = \frac{\lambda^a}{ (\lambda-s)^a}
\end{equation*}
gilt. Bestimmen Sie damit den Erwartungswert und die Varianz von $X$. %A 1.12

\paragraph{Transformationen der Gamma-Verteilung. } Seien $X \sim
\textrm{Gamma}(\alpha_1, \beta)$ und $Y \sim \textrm{Gamma}(\alpha_2, \beta)$
zwei unabhängige Zufallsvariablen. Beweisen Sie:
\begin{enumerate}
    \item Ist $c>0$ so gilt
        \begin{equation*}
            c X \sim \text{Gamma}(\alpha, \frac{\beta}{c}).
        \end{equation*}
    \item \begin{equation*}
            X + Y \sim \text{Gamma}(\alpha_1 + \alpha_2, \beta).
        \end{equation*}
    \item \begin{equation*}
            \frac{X}{X+Y} \sim \text{Beta}(\alpha_1, \alpha_2).
        \end{equation*}
    \item $\frac{X}{X+Y}$ und $X+Y$ sind unabhängig. 
\end{enumerate}
Für die Beweise der letzten zwei Aussagen können Sie den Transformationssatz
verwenden. 
% \cite{czadoschmidt}

\paragraph{Mittelwertvergleich bei Gamma-Verteilungen. }
Seien $X_1,\ldots,X_n$ i.i.d.\ und $\textrm{Gamma}(a, \lambda_1)$-verteilt sowie
$Y_1,\ldots,Y_n$ i.i.d.\ und $\textrm{Gamma}(a,\lambda_2)$-verteilt. Man nehme an,
dass die Vektoren $(X_1,\ldots,X_n)$ und $(Y_1,\ldots,Y_n)$ unabhängig sind. 
Bestimmen Sie die Verteilung von $\bar X/\bar Y$.

\paragraph{Dichte der $\chi^2$-Verteilung. }   Seien $X_1,\ldots,X_n$ unabhängige und
standardnormalverteilte Zufallsvariablen. Dann folgt $Y= \sum_{i=1}^{n} X_i^2$ einer
$\chi^2$-Verteilung mit $n$ Freiheitsgraden. Zeigen Sie, dass die Dichte von $Y$ durch
\begin{equation*}
    p(x) = \frac{1}{ 2^{\frac{n}{2}} \Gamma(\frac{n}{2})}e^{-\frac{x}{2}} x^{\frac{n-2}{2}} 1_{\R>0}(x)
\end{equation*}
gegeben ist. Verwenden Sie hierfür die Faltungsformel und die Beta-Funktion.

\paragraph{Verteilung der Stichprobenvarianz. }
Seien $X_1, \ldots, X_n$ i.i.d., normalverteilt und $\Var\left( X_1 \right)=\sigma^2$.
Für das zweite zentrierte empirische  Moment 
$\hat \sigma^2 \left( X \right)= \frac{1}{n} \sum_{i=1}^{n} \left( X_i - \bar X \right)^2 $
gilt, dass
\begin{equation*}
    \frac{n \hat \sigma^2 \left( X \right)}{\sigma^2 } = 
        \sum_{i=1}^{n} \left( \frac{X_i - \bar X}{ \sigma} \right)^2 \sim \chi^2_{n-1}.
\end{equation*}


\paragraph{Rayleigh-Verteilung. }  Seien $X$ und $Y$ unabhängig und 
$\mathcal N (0, \sigma^2)$-verteilt. Dann ist
\begin{equation*}
    Z= \sqrt{X^2 + Y^2} 
\end{equation*}
Rayleight-verteilt, d.h. 
$Z$ hat die Dichte $\frac{z}{\sigma^2}e^{-\frac{z^2}{2\sigma^2}}1_{\R>0}(z)$.
Es gilt $E\left( Z \right) = \sigma \sqrt{\pi/2}$ und 
$\Var\left( Z \right) = \sigma^2 \left( 2 - \frac{2}{\pi} \right)$.


\section{Counterexamples}

\paragraph{Cauchy-Verteilung. Ein Gegenbeispiel.}
Sei $X$ Cauchyverteilt mit der Dichte
\begin{equation*}
    p(x) = \frac{1}{\pi (1+x^2)}.
\end{equation*}
Zeigen Sie:
\begin{enumerate}
    \item $p$ ist wirklich eine Dichte.
    \item Der Erwartungswert $\E X$ existiert nicht.
    \item $\E X^n$ existiert nicht, für alle $n\in \bN$. 
    \item Die momenenerzeugende Funktion von $X$ existiert für alle $t \neq 0$ nicht. 
\end{enumerate}



\section{Theoretical computations}

\paragraph{Unabhängigkeit.} 
Sei $X$ eine Zufallvariable. $X$ ist von $X$ unabhängig, genau dann wenn
$P(X = c)=1$ für eine Konstante $c$ gilt. %\cite{jacod2003probability}

\paragraph{Eine Darstellung des Erwartungswertes. } Sei $X$ eine
positive Zufallsvariable mit $\E X < \infty$. 
\begin{enumerate}
    \item Ist $X \in \bN$ diskret, so gilt
        \begin{equation}
            E X = \sum_{n\in\bN}^{} P(X>n).
        \end{equation}
    \item Ist $X$ reellwertig, so gilt
        \begin{equation}
            E X = \int_{0}^{\infty} P(X>\lambda) d\lambda.
        \end{equation}
    \item Ist $X$ reellwertig und $E X^p<\infty$ für ein $p>0$, so gilt
        \begin{eqnarray}
            E X^p = \int_{0}^{\infty} p\lambda^{p-1} P(X>\lambda) d\lambda.
        \end{eqnarray}
\end{enumerate} 

\paragraph*{Lösung.} Das ist eine Anwendung des Satzes von Fubini.

\paragraph{Erwartungstreue der Stichprobenvarianz. } 
Seien $X_1,\ldots,X_n$ i.i.d.\ mit Varianz $\sigma^2$. Die Stichprobenvarianz
ist definiert durch
\begin{equation*}
    s^2(X) := \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar X)^2.
\end{equation*}
Dann gilt $E(s^2(X)) = \sigma^2$, d.h.~die Stichprobenvarianz ist erwartungstreu.
% \cite{czadoschmidt} % (A 1.3)


\paragraph{Faltungsformel. } Haben $X$ und $Y$ die Dichten $p_X$ und $p_Y$ und beide
sind unabhängig, so ist die Dichte von der Summe $Z=X+Y$ gegeben durch
\begin{equation*}
    p_Z(z) = \int_{\R}^{} p_X(x)p_Y(z-x) dx.
\end{equation*}%  \cite{czadoschmidt}
% (A 1.30)


\paragraph{Abzählbare Familien disjunkter Mengen.} Sei ein Wahrscheinlichkeitsraum
$(\Omega, \cA, P)$ und eine Familie $(A_i)_{i\in I}$ mit $A_i \in \cA$ und
$P(A_i)>0 \forall i$ gegeben. Dann ist $I$ höchstens abzählbar. 
\paragraph*{Lösung.} Sortiere die Menge der Größe nach. 



\section{Bedingte Erwartungen}

%Für die Aufgaben über bedingte Erwartungen fixieren wir den Raum $(\Omega, P, \mathcal A)$.

%\emph{Hinweis zu den Lösungen: } Die Gleichheitsrelationen sowohl in den Aufgaben als auch in 
%den Lösungen sind $P$-fast sicher zu verstehen.

\paragraph{Bedingte Erwartungen. Turmeigenschaft. Gegenbeispiel. } Sei $X$ eine integrierbare Zufallsvariable
und $\mathcal H, \mathcal G$ seien Sub-$\sigma$-Algebren von $\mathcal A$. 
Zeigen Sie durch ein Beispiel, dass 
\begin{equation}
	E\left[ E\left[ X | \mathcal G \right] | \mathcal H \right] \neq
	E\left[ E\left[ X | \mathcal H \right] | \mathcal G \right]
\end{equation}
gelten kann. 

\paragraph*{Lösung. } 
Betrachten wir $\Omega=\left\{ 1,2,3 \right\}$, $\mathcal A=2^\Omega$ die Potenzmenge von $\Omega$,
$\mathcal H=\sigma\left( \left\{ 1,2 \right\} \right)$ und 
$\mathcal G=\sigma\left( \left\{ 2,3 \right\} \right)$ sowie 
$P\left( i \right)=\frac{1}{3}$ für alle $i\in\Omega$. Berechnen wir die obigen bedingten
Erwartungen für $X:\Omega\to\mathbb N, i\mapsto 2^i$.
\begin{eqnarray}
	E\left[ X|\mathcal H \right] &=&
	\frac{X\left( 1 \right)+X\left( 2 \right)}{2}1_{\left\{ 1,2 \right\}} + X\left( 3 \right) 1_{ \left\{ 3 \right\} } \\
	&=& 3_{  \left\{ 1,2 \right\} } + 8_{ \left\{ 3 \right\}  } \\ 
	E\left[ E\left[ X|\mathcal H \right]|\mathcal G \right] 
	&=& 3_{ \left\{ 1 \right\} } + \frac{3+8}{2} 1_{ \left\{ 2,3 \right\} } \\
	E\left[ X|\mathcal G \right] &=& 
	X\left( 1 \right) 1_{ \left\{ 1 \right\} } + \frac{X\left( 2 \right)+X\left( 3 \right)}{2}1_{ \left\{ 2,3 \right\} } \\
	&=& 2_{ \left\{ 1 \right\} } + 6_{ \left\{ 2,3 \right\} } \\
	E\left[ E\left[ X|\mathcal G \right]| \mathcal H \right] 
	&=& \frac{2+6}{2}1_{ \left\{ 1,2 \right\} } + 6_{ \left\{ 3 \right\} } \\
	&=& 4_{ \left\{ 1,2 \right\} } + 6_{ \left\{ 3 \right\} }.
\end{eqnarray}
Dabei bezeichnet $4_{ \left\{ 1,2 \right\} }$ eine Funktion, die den Elementen $1$ und $2$
den Wert $4$ zuordnet und allen anderen den Wert $0$.

Damit folgt die Behauptung. 






\paragraph{Bedingte Erwartungen. Turmeigenschaft. } Sei $X$ eine integrierbare Zufallsvariable und
$\mathcal H$ eine Sub-$\sigma$-Algebra von 
der $\sigma$-Algebra $\mathcal G$. D.h.\ $\mathcal H \subset \mathcal G \subset \mathcal A$.
Zeigen Sie
\begin{equation}
	E\left[ E\left[ X | \mathcal G \right] | \mathcal H \right] =
	E\left[ E\left[ X | \mathcal H \right] | \mathcal G \right] =
	E \left[ X | \mathcal H \right].
\end{equation}

\paragraph*{Lösung.} Wir zeigen zunächst 
$E\left[ E\left[ X | \mathcal H \right] | \mathcal G \right]=E \left[ X | \mathcal H \right]$.
Nach Definition des bedingten Erwartungswertes ist $E\left[ X| \mathcal H \right]$ 
$\mathcal H$-messbar und daher $\mathcal G$-messbar. So bekommen wir 
\begin{equation}
	E\left[ E\left[ X | \mathcal H \right] | \mathcal G \right] =
	E\left[ X| \mathcal H \right] E\left[ 1|\mathcal G \right] = 
	E \left[ X | \mathcal H \right ].
\end{equation}
Nun kommen wir zu 
$E\left[ E\left[ X | \mathcal G \right] | \mathcal H \right]=E \left[ X | \mathcal H \right]$.
Dies gilt genau dann wenn 
\begin{equation}
	E\left\{ X 1_A \right\} 
	= E\left\{ E\left[ E\left[ X|\mathcal G \right]|\mathcal H \right] 1_A \right\}  
	\quad \forall A\in\mathcal H.
\end{equation}
Sei $Y=E\left[ X|\mathcal G \right]$ eine $\mathcal G$-messbare Zufallsvariable und
$A\in\mathcal H$. Die Indikatorfunktion $1_A$ ist $\mathcal H$-messbar und daher
$\mathcal G$-messbar. Wir berechnen
\begin{eqnarray}
	E\left\{ E\left[ E\left[ X|\mathcal G \right]|\mathcal H \right] 1_A \right\} &=& 
	E\left\{ E\left[ Y|\mathcal H \right] 1_A \right\} \\
	&=& E\left\{ E\left[ Y 1_A| \mathcal H \right] \right\} \\
	&=& E\left\{ Y 1_A \right\} \\
	&=& E\left\{ E\left[ X|\mathcal G \right] 1_A \right\} \\
	&=& E\left\{ E\left[ X 1_A| \mathcal G \right] \right\} = E\left\{ X 1_A \right\}. 
\end{eqnarray}
Daher ist nach Definition des bedingten Erwartungswertes
\begin{equation}
	E\left[ E\left[ X | \mathcal G \right] | \mathcal H \right]=E \left[ X | \mathcal H \right].
\end{equation}





\paragraph{Bedingte Varianz. } Seien $X,Y$ reelle Zufallsvariable mit $E\left[ X^2 \right] <\infty$. 
Die bedingte Varianz einer Zufallsvariablen $X$ gegeben $Y$ ist definiert durch
\begin{equation}
	\Var \left[ X|Y \right] = E \left[ \left( X - E\left[ X|Y \right] \right)^2|Y \right].
\end{equation}
Dann gelten
\begin{enumerate}
    \item 
        \begin{equation}
        \Var \left( X \right) = 
        \Var \left( E\left[ X|Y \right] \right) + E \left[ \Var\left[ X|Y \right] \right].
        \end{equation}
    \item \begin{equation}
            \Var \left( X | Y \right) = E \left[ X^2 | Y \right] 
            - \left( E\left[ X | Y \right] \right)^2.
        \end{equation}
\end{enumerate}

\paragraph*{Lösung. } 
Die Behauptung ist äquivalent zu 
\begin{equation}
    E\left\{ X^2 \right\} - E\left\{ X \right\}^2 
    = E\left\{ E\left[ X|Y \right]^2 \right\} - E\left\{ E\left[ X|Y \right] \right\}^2
    + E\left\{ E\left[ \left( X - E\left[ X|Y \right] \right)^2 |Y \right] \right\}.
\end{equation}
Wir kürzen den Term $E\left\{ X \right\}^2$ und erhalten 
\begin{eqnarray}
    E\left\{ X^2 \right\}&=& 
    E\left\{ E\left[ X|Y \right]^2 \right\} +
    E\left\{ X^2 - 2X E\left[ X|Y \right] + E\left[ X|Y \right]^2 \right\}
\end{eqnarray}
Diese Gleichung gilt genau dann wenn
\begin{equation}
    E\left\{  2X E\left[ X|Y \right] \right\} = 2 E\left\{ E\left[ X|Y \right]^2 \right\}.	
\end{equation}
Die Turmeigenschaft liefert uns aber
\begin{eqnarray}
    E\left\{  X E\left[ X|Y \right] \right\} &=& 
    E\left\{ E\left[ X E\left[ X|Y \right] | Y \right] \right\} \\
    &=& E\left\{ E\left[ X|Y \right] E\left[ X|Y \right]   \right\} \\
    &=& E\left\{ E\left[ X|Y \right]^2 \right\}.
\end{eqnarray}







\paragraph{Bedingte Erwartungen. Idempotenz. } Sei $X$ eine integrierbare Zufallsvariable. Zeigen Sie
\begin{equation}
	E\left[ X|X \right] = X \quad P-\textrm{f.s.}
\end{equation}

\paragraph*{Lösung. } 
Nach Definition ist $E\left[ X| X \right]=E\left[ X|\sigma\left( X \right) \right]$ und
da $X$ ja $\sigma\left( X \right)$-messbar ist, erhalten wir
\begin{equation}
	E\left[ X|\sigma\left( X \right) \right]= X E\left[ 1 | \sigma\left( X \right) \right] =X.
\end{equation}



\paragraph{Bedingte Erwartungen und Normalverteilung. } Seien $X$ und $Y$ unabhängige
standardnormalverteilte Zufallsvariablen. Berechnen Sie $\E\left[ X \vb X+Y
\right]$.


\paragraph{Bedingte Erwartungen und Normalverteilung II. } Exercise 5.2.7 from Föllmer and Schied.



% vim: set spelllang=de; set spell
