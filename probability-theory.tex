
\section{Raw material}


\problem{Monotonic transformations of RVs.} A strictly monotonic
transformation of a real valued random variable with atom-less distribution
function has an atom-less distribution function. 

\url{http://math.stackexchange.com/questions/78101/monotonic-transformation-of-continuous-random-variable-are-continuous}


\problem{Ordnungsstatistiken. Eigenschaften.} Seien $X_1,\dots ,X_n$
identisch verteilte stochastisch unabhängige Zufallsgrößen mit der
Dichtefunktion $f(x)$ und Verteilungsfunktion $F(x)$. Der Zufallsvektor $\left(
X_{(1)},\dots ,X_{(n)} \right)$ enthält aufsteigend geordnete Zufallsgrößen
$X_1,\dots ,X_n$. Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item Die gemeinsame Dichtefunktion von dem Zufallsvektor $\left( X_{(1)},\dots ,X_{(n)} \right)$
        ist
        \begin{equation*}
            g(x_{1},\dots ,x_n) = \begin{cases}
                n! f(x_1)\dots f(x_n), & \text{falls } x_{1} < \dots < x_n \\
                0, & \text{sonst.}
            \end{cases}
        \end{equation*}

    \item Die Dichtefunktion von $X_{(k)}$ ist
        \begin{equation*}
            f_{(k)}(y) = k \binom{n}{k} f(y) (1-F(y))^{n-k} F(y)^{k-1}.
        \end{equation*}

    \item Die Dichtefunktion von $(X_{(i)},X_{(j)})$ für $i\neq j$ ist
        \begin{equation*}
            g_{i,j}(x,y)= \begin{cases}
                \frac{n! F(x)^{i-1} [F(y)-F(x)]^{j-i-1} [1-F(y)]^{n-j}f(x)f(y) }{ (i-1)! (j-i-1)! (n-j)! }, & x<y \\
                0, & \text{sonst.}
            \end{cases}
        \end{equation*}
\end{enumerate}

\solution
\begin{enumerate}
    \item Ein fixer Punkt des Trägers von $g$ ist ein Bild von $n!$ Punkten in
        $\bR^n$ unter der Abbildung $(x_1,\dots ,x_n)\mapsto (x_{(1)},\dots
        ,x_{(n)})$.
    \item Die Verteilungsfunktion von $X_{(k)}$ is gegeben durch
        \begin{equation*}
            P(X_{(k)} < x) = F_{k}(y) = \binom{n}{k}\, (F(y))^{k} \left( 1-F(y)  \right)^{n-k}
        \end{equation*}
        wobei $F$ die Verteilungsfunktion von $X_1$ ist. Damit $\left\{ X_{
        (k)} < x \right\}$ eintritt mussen $k$ Komponenten die kleiner als $x$
        gewählt werden und der Rest größer oder gleich $x$. 
    \item 
\end{enumerate}



\problem{Independence. }
Let $(\Omega, \mathcal A, P)$ be a probablity space and let $\mathcal F$ and
$\mathcal G$ be two independent sub-$\sigma$-algebras of $\mathcal A$.

\begin{enumerate} 
\item If $X: \Omega \to (\R, \mathcal B)$ is a
    real-valued random variable and $X$ is both $\mathcal F$-measurable
    and $\mathcal G$-measurable then $X$ is a.s.~constant.
\end{enumerate}
\problem{} $X$ und $Y$ seien unabhängige Zufallsvariablen. $X$ sei symmetrisch, 
d.h.\ $X$ und $-X$ sind gleichverteilt. Dann gilt
\begin{equation}
    \E \| X \|^p \leq \E \| X + Y \|^p.
\end{equation}
Gilt die obige Ungleichung, wenn $X$ und $Y$ nicht unabhängig sind oder wenn 
$X$ nicht symmetrisch verteilt ist?

\paragraph*{Solution. } The real problem here is to show that if $X \upmodels Z$
and $Y \upmodels Z$ the $X+Z \sim Y+Z$. 



\section{Theoretical computations}

\problem{Dichtefunktionen. Scheff\'es Identitäten.} Seien $f$ und $g$
Dichtefunktionen und $A = \left\{ x : f(x)>g(x) \right\}$. 
Zeigen Sie folgende Identitäten:
\begin{enumerate}
    \item \begin{align*}
            \int_{\bR} |f(x)-g(x)| dx &= 2 \int_{\bR} \max\left\{ f(x)-g(x), 0 \right\} dx \\
            &= 2 \int_{A} f(x)dx - 2 \int_{A} g(x)dx.
        \end{align*}
    \item \begin{align*}
            \sup_{B\in\cB(\R)} \left| \int_{B} f(x)dx - \int_{B} g(x)dx \right| &=
            \frac{1}{2} \int_{\bR} |f(x)-g(x)|dx.
        \end{align*}
\end{enumerate}


\problem{Unabhängigkeit.} 
Sei $X$ eine Zufallvariable. $X$ ist von $X$ unabhängig, genau dann wenn
$P(X = c)=1$ für eine Konstante $c$ gilt. %\cite{jacod2003probability}

\problem{Eine Darstellung des Erwartungswertes. } Sei $X$ eine
positive Zufallsvariable mit $\E X < \infty$. 
\begin{enumerate}
    \item Ist $X \in \bN$ diskret, so gilt
        \begin{equation}
            E X = \sum_{n\in\bN}^{} P(X>n).
        \end{equation}
    \item Ist $X$ reellwertig, so gilt
        \begin{equation}
            E X = \int_{0}^{\infty} P(X>\lambda) d\lambda.
        \end{equation}
    \item Ist $X$ reellwertig und $E X^p<\infty$ für ein $p>0$, so gilt
        \begin{eqnarray}
            E X^p = \int_{0}^{\infty} p\lambda^{p-1} P(X>\lambda) d\lambda.
        \end{eqnarray}
\end{enumerate} 

\solution Das ist eine Anwendung des Satzes von Fubini.

\problem{Erwartungstreue einfacher Schätzer.} 
Seien $X_1,\ldots,X_n$ i.i.d.\ mit dem Erwartungswert $\mu$ und endlicher
Varianz $\sigma^2$. Der arithmetische Mittelwert von $X=(X_1,\cdots,X_n)$ ist
definiert als
\begin{equation*}
    \bar X = \frac{1}{n} \sum_{i=1}^{n} X_i. 
\end{equation*}
Die Stichprobenvarianz ist definiert durch
\begin{equation*}
    s^2(X) := \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar X)^2.
\end{equation*}
Zeigen Sie: 
\begin{enumerate}
    \item Der Mittelwert $\bar X$ ist ein erwartungstreuer Schätzer für den
        Erwartungswert $\mu$, d.h.\ 
        \begin{equation*}
            \E \bar X = \mu. 
        \end{equation*}
    \item Der Schätzer
        \begin{equation*}
            \eta(X) = \frac{1}{n} \sum_{i=1}^{n} \left( X_i - \mu \right)^2
        \end{equation*}
        ist erwartungstreu für $\sigma^2$. Es gilt also $\E \eta(X) = \sigma^2$.  
    \item Die Stichprobenvarianz ist ein erwartungstreuer Schätzer für $\sigma^2$. 
\end{enumerate}

\solution
Die Erwartungstreue der ersten beiden Schätzer ist leicht zu zeigen. 
\begin{align*}
    \E \bar X &= \E \left[ \frac{1}{n} \sum_{i=1}^{n} X_i \right] 
    = \frac{1}{n} \sum_{i=1}^{n} \E X_i = \mu \\
    \E \eta(X) &= \E \left[ \frac{1}{n} \sum_{i=1}^{n} (X_i -\mu)^2 \right] 
    = \frac{1}{n} \sum_{i=1}^{n} \E (X_i-\mu)^2 = \frac{1}{n} \sum_{i=1}^{n} \sigma^2 = \sigma^2.
\end{align*}
Um den Erwartungswert der Stichprobenvarianz zu zeigen, berechnen wir
\begin{align*}
    \E (n-1) s^2(X) &= \E \sum_{i=1}^{n} \left( X_i - \bar X \right)^2 \\
    &= \E \sum_{i=1}^{n} \left( X_i - \mu + \mu - \bar X \right)^2 \\
    &= \E \left[ \sum_{i=1}^{n} (X_i-\mu)^2 + 2 (X_i-\mu)(\mu-\bar X) + (\mu -\bar X)^2 \right].
\end{align*}
Für die obigen Summanden gilt
\begin{align*}
    \E (X_i-\mu)^2 &= \sigma^2 \\
    \E \left[ \sum_{i=1}^{n} (X_i-\mu)(\mu-\bar X) \right] &= -n \E (\bar X-\mu)^2 \\ 
    \E (\bar X-\mu)^2 &= \E \left( \frac{1}{n} \sum_{i=1}^{n} X_i-\mu \right)^2 \\
    &= \frac{1}{n^2} \sum_{i,j}^{} \E (X_i-\mu)(X_j-\mu) \\
    &= \frac{1}{n^2} \sum_{i=1}^{n} \sigma^2 = \frac{\sigma^2}{n} \\
    \E \left[ \sum_{i=1}^{n} (X_i-\mu)(\mu-\bar X) \right] &= -\sigma^2.
\end{align*}
Wir erhalten also 
\begin{equation*}
    \E (n-1) s^2(X) = (n-1) \sigma^2
\end{equation*}
und daraus
\begin{equation*}
    \E s^2(X) = \sigma^2. 
\end{equation*}



\problem{Faltungsformel. } Haben $X$ und $Y$ die Dichten $p_X$ und $p_Y$ und beide
sind unabhängig, so ist die Dichte von der Summe $Z=X+Y$ gegeben durch
\begin{equation*}
    p_Z(z) = \int_{\R}^{} p_X(x)p_Y(z-x) dx.
\end{equation*}%  \cite{czadoschmidt}
% (A 1.30)


\problem{Abzählbare Familien disjunkter Mengen.} Sei ein Wahrscheinlichkeitsraum
$(\Omega, \cA, P)$ und eine Familie $(A_i)_{i\in I}$ mit $A_i \in \cA$ und
$P(A_i)>0 \forall i$ gegeben. Dann ist $I$ höchstens abzählbar. 
\solution Sortiere die Menge der Größe nach. 



\section{Bedingte Erwartungen}

%Für die Aufgaben über bedingte Erwartungen fixieren wir den Raum $(\Omega, P, \mathcal A)$.

%\emph{Hinweis zu den Lösungen: } Die Gleichheitsrelationen sowohl in den Aufgaben als auch in 
%den Lösungen sind $P$-fast sicher zu verstehen.

\problem{Bedingte Erwartungen. II.}
Seien $\left( X_i \right)_{i\geq 1}$ i.i.d.\ und integrierbar. Definieren wir 
$S_n = \sum_{i=1}^{n} X_i$. Dann gilt:
\begin{enumerate}
    \item $\E \left[ X_j \vb S_n \right] = \frac{1}{n} \sum_{i=1}^{n} X_i$
        für $1\leq j \leq n$.
\end{enumerate}

\solution 
\begin{enumerate}
    \item Es gilt
        \begin{equation*}
            \E \left[ \sum_{i=1}^{n} X_i \vb \sum_{i=1}^{n} X_i \right] 
            = \sum_{i=1}^{n} X_i
        \end{equation*}
        und
        \begin{equation*}
            \E \left[  \sum_{i=1}^{n} X_i \vb S_n \right] =
            \sum_{i=1}^{n} \E \left[ X_i \vb S_n \right] = 
            n \E \left[ X_1 \vb S_n \right].
        \end{equation*}
        Daraus folgt
        \begin{equation*}
             \E \left[ X_1 \vb S_n \right] = \E \left[ X_i \vb S_n \right] =
             \frac{1}{n} \sum_{i=1}^{n} X_i
        \end{equation*}
        für alle $1 \leq i \leq n$. 
\end{enumerate}


\problem{Zusammengesetzte Poissonverteilung. Momentenerzeugende Funktion und Momente.}
Sei $N$ Poissonverteilt mit dem Parameter $\lambda>0$ und $\left( X_i
\right)_{i\geq 1}$ i.i.d.  Zufallsvariablen mit existierender
momentenerzeugender Funktion $\psi_X$. Die Zufallsvariablen $N, X_1, X_2,
\cdots$ sind alle unabhängig und es gilt 
\begin{equation*}
    P(N=n) = e^{-\lambda} \frac{\lambda^n}{ n!}, \ n\in\bN. 
\end{equation*}
Sei
\begin{equation*}
    Y = \sum_{i=1}^{N} X_i
\end{equation*}
wobei wir auf $\left\{ N=0 \right\}$ $Y=0$ setzen. Zeigen Sie folgende Aussagen.
\begin{enumerate}
    \item Die momentenerzeugende Funktion $\psi_Y$ von $Y$ ist
        \begin{equation*}
            \psi_Y(u) = \E \exp \left( u Y \right) = 
            \exp \left( \lambda \left( \psi_X(u)-1 \right)  \right).
        \end{equation*}
    \item Für die ersten zwei Momente gelten die Formeln
        \begin{align*}
            \E Y &= \E N \ \E X_1 = \lambda \E X_1 & 
            \Var Y &= \E N \ \E X_1^2. 
        \end{align*}
\end{enumerate}

\solution Wir berechnen die momentenerzeugende Funktion von $Y$.
\begin{align*}
    \psi_Y(u) &= \E \exp \left( uY \right) \\
    &= \E \exp \left( u\sum_{i=1}^{N} X_i \right) \\
    &= \E \left[\E \left[ \exp\left( u\sum_{i=1}^{N} X_i \right) | N \right] \right] \\ 
    &= \sum_{n \geq 0} \E\left[ \exp\left( u\sum_{i=1}^{n} X_i \right) \right] P(N=n) \\
    &= \sum_{n\geq 0} \left( \E e^{u X_1} \right)^n e^{-\lambda} \frac{\lambda^n}{n!} \\
    &= \sum_{n\geq 0} e^{-\lambda} \frac{\left( \lambda \psi_X(u) \right)^n}{n!} \\
    &= \exp \left( \lambda \left( \psi_X(u) -1 \right) \right).
\end{align*}
Den Erwartungswert und die Varianz erhalten wir aus
\begin{align*}
    \frac{\partial}{\partial u} \psi_Y &= 
    \psi_Y(u) \lambda \psi_X'(u) \\
    \left. \frac{\partial}{\partial u} \psi_Y \right|_{u=0} &= 
    \E N \ \E X = \lambda \E X \\
    \frac{\partial^2}{\partial u^2} \psi_Y &= 
    \frac{\partial}{\partial u} \lambda \psi_Y(u) \psi_X'(u) =
    \lambda \left[ \lambda \psi_Y(u) \left( \psi_X'(u) \right)^2 + \psi_Y(u)\psi_X''(u) \right]   \\
    \left. \frac{\partial^2}{\partial u^2} \psi_Y \right|_{u=0} &= 
    \lambda \left[ \lambda \left( \E X \right)^2 + \E X^2 \right] \\
    \Var Y &= \lambda \left[ \lambda \left( \E X \right)^2 + \E X^2 \right]
    - \lambda^2 \left( \E X \right)^2 = \lambda \E X^2.
\end{align*}






\problem{Bedingte Erwartungen. Turmeigenschaft. Gegenbeispiel. } Sei $X$ eine integrierbare Zufallsvariable
und $\mathcal H, \mathcal G$ seien Sub-$\sigma$-Algebren von $\mathcal A$. 
Zeigen Sie durch ein Beispiel, dass 
\begin{equation*}
	E\left[ E\left[ X | \mathcal G \right] | \mathcal H \right] \neq
	E\left[ E\left[ X | \mathcal H \right] | \mathcal G \right]
\end{equation*}
gelten kann. 

\solution 
Betrachten wir $\Omega=\left\{ 1,2,3 \right\}$, $\mathcal A=2^\Omega$ die Potenzmenge von $\Omega$,
$\mathcal H=\sigma\left( \left\{ 1,2 \right\} \right)$ und 
$\mathcal G=\sigma\left( \left\{ 2,3 \right\} \right)$ sowie 
$P\left( i \right)=\frac{1}{3}$ für alle $i\in\Omega$. Berechnen wir die obigen bedingten
Erwartungen für $X:\Omega\to\mathbb N, i\mapsto 2^i$.
\begin{eqnarray*}
	E\left[ X|\mathcal H \right] &=&
	\frac{X\left( 1 \right)+X\left( 2 \right)}{2}1_{\left\{ 1,2 \right\}} + X\left( 3 \right) 1_{ \left\{ 3 \right\} } \\
	&=& 3_{  \left\{ 1,2 \right\} } + 8_{ \left\{ 3 \right\}  } \\ 
	E\left[ E\left[ X|\mathcal H \right]|\mathcal G \right] 
	&=& 3_{ \left\{ 1 \right\} } + \frac{3+8}{2} 1_{ \left\{ 2,3 \right\} } \\
	E\left[ X|\mathcal G \right] &=& 
	X\left( 1 \right) 1_{ \left\{ 1 \right\} } + \frac{X\left( 2 \right)+X\left( 3 \right)}{2}1_{ \left\{ 2,3 \right\} } \\
	&=& 2_{ \left\{ 1 \right\} } + 6_{ \left\{ 2,3 \right\} } \\
	E\left[ E\left[ X|\mathcal G \right]| \mathcal H \right] 
	&=& \frac{2+6}{2}1_{ \left\{ 1,2 \right\} } + 6_{ \left\{ 3 \right\} } \\
	&=& 4_{ \left\{ 1,2 \right\} } + 6_{ \left\{ 3 \right\} }.
\end{eqnarray*}
Dabei bezeichnet $4_{ \left\{ 1,2 \right\} }$ eine Funktion, die den Elementen $1$ und $2$
den Wert $4$ zuordnet und allen anderen den Wert $0$.


\problem{Bedingte Erwartungen. Turmeigenschaft. } Sei $X$ eine integrierbare
Zufallsvariable und $\mathcal H$ eine Sub-$\sigma$-Algebra von der
$\sigma$-Algebra $\mathcal G$ mit $\mathcal H \subset \mathcal G \subset
\mathcal A$. Zeigen Sie
\begin{enumerate}
    \item $E\left[ E\left[ X | \mathcal G \right] | \mathcal H \right] =
        E \left[ X | \mathcal H \right]$.
    \item $E\left[ E\left[ X | \mathcal H \right] | \mathcal G \right] =
        E \left[ X | \mathcal H \right]$.
\end{enumerate}
%\begin{equation*}
%	E\left[ E\left[ X | \mathcal G \right] | \mathcal H \right] =
%	E\left[ E\left[ X | \mathcal H \right] | \mathcal G \right] =
%\end{equation*}

\solution Wir zeigen zunächst 
$E\left[ E\left[ X | \mathcal H \right] | \mathcal G \right]=E \left[ X | \mathcal H \right]$.
Nach Definition des bedingten Erwartungswertes ist $E\left[ X| \mathcal H \right]$ 
$\mathcal H$-messbar und daher $\mathcal G$-messbar. So bekommen wir 
\begin{equation*}
	E\left[ E\left[ X | \mathcal H \right] | \mathcal G \right] =
	E\left[ X| \mathcal H \right] E\left[ 1|\mathcal G \right] = 
	E \left[ X | \mathcal H \right ].
\end{equation*}
Nun kommen wir zu 
$E\left[ E\left[ X | \mathcal G \right] | \mathcal H \right]=E \left[ X | \mathcal H \right]$.
Dies gilt genau dann wenn 
\begin{equation*}
	E\left\{ X 1_A \right\} 
	= E\left\{ E\left[ E\left[ X|\mathcal G \right]|\mathcal H \right] 1_A \right\}  
	\quad \forall A\in\mathcal H.
\end{equation*}
Sei $Y=E\left[ X|\mathcal G \right]$ eine $\mathcal G$-messbare Zufallsvariable und
$A\in\mathcal H$. Die Indikatorfunktion $1_A$ ist $\mathcal H$-messbar und daher
$\mathcal G$-messbar. Wir berechnen
\begin{eqnarray*}
	E\left\{ E\left[ E\left[ X|\mathcal G \right]|\mathcal H \right] 1_A \right\} &=& 
	E\left\{ E\left[ Y|\mathcal H \right] 1_A \right\} \\
%	&=& E\left\{ E\left[ Y 1_A| \mathcal H \right] \right\} \\
	&=& E\left\{ Y 1_A \right\} \\
	&=& E\left\{ E\left[ X|\mathcal G \right] 1_A \right\} \\
%	&=& E\left\{ E\left[ X 1_A| \mathcal G \right] \right\} \\
    &=& E\left\{ X 1_A \right\}. 
\end{eqnarray*}
Daher ist nach Definition des bedingten Erwartungswertes
\begin{equation*}
    E\left[ E\left[ X | \mathcal G \right] | \mathcal H \right]=E \left[ X |
    \mathcal H \right].
\end{equation*}


\problem{Bedingte Varianz. } Seien $X,Y$ reelle Zufallsvariable mit $E\left[
X^2 \right] <\infty$.  Die bedingte Varianz einer Zufallsvariablen $X$ gegeben
$Y$ ist definiert durch
\begin{equation*}
	\Var \left[ X|Y \right] = E \left[ \left( X - E\left[ X|Y \right] \right)^2|Y \right].
\end{equation*}
Dann gelten
\begin{enumerate}
    \item 
        \begin{equation*}
        \Var \left( X \right) = 
        \Var \left( E\left[ X|Y \right] \right) + E \left[ \Var\left[ X|Y \right] \right].
        \end{equation*}
    \item \begin{equation*}
            \Var \left( X | Y \right) = E \left[ X^2 | Y \right] 
            - \left( E\left[ X | Y \right] \right)^2.
        \end{equation*}
\end{enumerate}

\solution 
Die Behauptung ist äquivalent zu 
\begin{equation*}
    E\left\{ X^2 \right\} - E\left\{ X \right\}^2 
    = E\left\{ E\left[ X|Y \right]^2 \right\} - E\left\{ E\left[ X|Y \right] \right\}^2
    + E\left\{ E\left[ \left( X - E\left[ X|Y \right] \right)^2 |Y \right] \right\}.
\end{equation*}
Wir kürzen den Term $E\left\{ X \right\}^2$ und erhalten 
\begin{eqnarray*}
    E\left\{ X^2 \right\}&=& 
    E\left\{ E\left[ X|Y \right]^2 \right\} +
    E\left\{ X^2 - 2X E\left[ X|Y \right] + E\left[ X|Y \right]^2 \right\}
\end{eqnarray*}
Diese Gleichung gilt genau dann wenn
\begin{equation*}
    E\left\{  2X E\left[ X|Y \right] \right\} = 2 E\left\{ E\left[ X|Y \right]^2 \right\}.	
\end{equation*}
Die Turmeigenschaft liefert uns aber
\begin{eqnarray*}
    E\left\{  X E\left[ X|Y \right] \right\} &=& 
    E\left\{ E\left[ X E\left[ X|Y \right] | Y \right] \right\} \\
    &=& E\left\{ E\left[ X|Y \right] E\left[ X|Y \right]   \right\} \\
    &=& E\left\{ E\left[ X|Y \right]^2 \right\}.
\end{eqnarray*}


\problem{Bedingte Erwartungen. Einfache Eigenschaften.} Sei $X$ eine
integrierbare Zufallsvariable und $\cG \subset \cA$ eine Sub-$\sigma$-Algebra.
Zeigen Sie die folgenden Aussagen.
\begin{enumerate}
    \item $E\left[ X|X \right] = X \ P$-f.s.
    \item Falls $| X| \leq c$ f.s., dann $| \E \left[ X \vb \cG \right] | \leq c$ f.s.
    \item Falls $X=\alpha$ f.s.\ für $\alpha\in\R$, dann gilt 
        $\E \left[ X | \cG  \right]= \alpha $ f.s.
\end{enumerate}


\solution 
\begin{enumerate}
    \item 
        Nach Definition ist $E\left[ X| X \right]=E\left[ X|\sigma\left( X
        \right) \right]$ und da $X$ ja $\sigma\left( X \right)$-messbar ist,
        erhalten wir \begin{equation*} E\left[ X|\sigma\left( X \right)
            \right]= X E\left[ 1 | \sigma\left( X \right) \right] =X.
        \end{equation*}

        Wir können diese Aussage auch anders beweisen. $Z= \E X | X$ gilt genau
        dann, wenn $Z \in \sigma(X)$ und $\E\ Z g(X) = \E\ X g(X)$ für alle
        beschänkte Borelfunktionen $g$ gilt.  Würde nicht $Z = X$ fast sicher
        gelten, so wäre $\Lambda=\left\{ Z< X \right\}$ oder $\Lambda= \left\{
        Z > X \right\}$ $\sigma(X)$-messbar und keine Nullmenge. Dann wäre aber
        $\E Z 1_{\Lambda} \neq \E X 1_{\Lambda}$.
    \item Wir beweisen $X \leq c$ f.s. $\impl$ $ \E X | \cG \leq c$ f.s. Nach
        Definition ist $Z=\E X|\cG$ genau dann, wenn $\E X Y = \E Z Y$ für alle
        beschränkte\footnote{Dies bedeutet, dass $Y$ eine beschränkte
        $\cG$-messbare Zufallsvariable ist} $Y\in \cG$. Nachdem $\Lambda =
        \left\{ Z>c \right\} \in \cG$ ist und $\E X 1_\Lambda = \E Z
        1_\Lambda$, es muss $P\left( \Lambda \right)=0$ gelten.  Nun ist $| X |
        \leq c$ f.s.\ genau dann, wenn $X \leq c$ und $ X \geq -c$ f.s.\ gilt.
    \item $X=\alpha$ f.s.\ genau dann, wenn $X \leq \alpha$ und $ X \geq
        \alpha$ f.s.\ gilt.
\end{enumerate}




\problem{Bedingte Erwartungen und Normalverteilung.} Seien $X$ und $Y$ unabhängige
standardnormalverteilte Zufallsvariablen. Berechnen Sie $\E\left[ X \vb X+Y
\right]$.


\problem{Bedingte Erwartungen. Exponentialverteilung.} Seien $X,Y$ unabhängig
und exponentialverteilt. Berechnen Sie:
\begin{enumerate}
    \item $\E\left[ Y \vb Y \wedge t \right]$.
    \item $\E \left[ X \wedge Y \vb X \right]$.
\end{enumerate}


\problem{Bedingte Erwartungen und Normalverteilung II.} Exercise 5.2.7 from
Föllmer and Schied.



% vim: set spelllang=de; set spell
