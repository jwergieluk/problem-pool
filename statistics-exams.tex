
\section{Prüfungsaufgaben}
\subsection{2012}

\paragraph{Invers-Gauß-Verteilung als exponentielle Familie.}
Seien $X_1,\ldots,X_n$ i.i.d.\ Invers-Gauß-verteilte Zufallsvariablen gegeben. D.h.\ die Dichte
von $X_i$ ist 
\begin{equation*}
    \sqrt{\frac{\lambda}{2\pi}} x^{-\frac{3}{2}} \exp\left( \frac{-\lambda(x-\mu)^2}{2\mu^2 x} \right) 1_{\R_{>0}} (x)
\end{equation*}
wobei $\mu>0$, $\lambda>0$ und $\mu$ bekannt und fix sei.
\begin{enumerate}
    \item Zeigen Sie, dass die von dem Zufallsvektor $( X_1,\ldots,X_n)$
        erzeugte Familie von Verteilungen $\left\{ P_\lambda : \lambda>0
        \right\}$ eine exponentielle Familie ist, indem Sie die gemeinsame
        Dichte des Vektors $(X_1,\ldots,X_n)$ in die Form
        \begin{equation*}
            p_\lambda(x) = \exp\left( c(\lambda) T(x) + d(\lambda) + S(x) \right)1_A (x)
        \end{equation*}
        bringen. Geben Sie dabei explizit $c,T,d,S$ und $A$ an.
    \item Zeigen Sie mit Hilfe des Faktorisierungssatzes, dass die Statistik
        $T(X)$ suffizient ist.
\end{enumerate}

\paragraph*{Lösungsskizze.} Die gemeinsame Dichte von $X_1, \cdots, X_n$ 
lässt sich in folgender Form darstellen.
\begin{equation*}
    \exp\left(  -\frac{\lambda}{2}\left( \frac{x^2}{\mu^2} + \frac{1}{x} \right)    
    + \log x^{-\frac{3}{2}} + \log \sqrt{\frac{\lambda}{2\pi}} + 
    \lambda\mu  \right)1_{\R_{>0}}(x).
\end{equation*}



\paragraph{Pareto-Verteilung. Suffizienz und MLE.} Seien $X_1,\ldots,X_n$ i.i.d.\ und
Paretoverteilt mit der Dichte
\begin{equation}
    p_\theta(x)= \frac{\theta a^\theta}{x^{\theta+1}}1_{\R_{>a}}(x),
\end{equation}
wobei $\theta>0$ und $a$ fix und bekannt sei.
\begin{enumerate}
    \item Finden Sie eine reellwertige suffiziente Statistik $T(X_1,\ldots,X_n)$ für $\theta$.
    \item Finden Sie den Maximum-Likelihood-Schätzer $\hat \theta(X_1,\ldots,X_n)$ für $\theta$.
    \item Zeigen Sie, dass der von Ihnen gefundene Schätzer $\hat \theta$ suffizient ist.
\end{enumerate}

\paragraph*{Lösungsskizze. }
Suffiziente Statistik $T= \prod_i \frac{1}{x_i}$ erhalten wir aus 
\begin{equation}
    p_\theta(X_1,\ldots,X_n) = 
    \theta^n a^{n\theta} \left( \prod_i \frac{1}{x_i} \right)^{\theta-1} \prod_i 1_{\R_{>a}}(x_i)
\end{equation}
mit Hilfe des Faktorisierungstheorems.

Die Berechnung folgt der üblichen Prozedur des Minimierens der Log-Likelihood-Funktion.
\begin{eqnarray}
    \frac{\partial}{\partial \theta} l(x,a,\theta) &=& 
    \frac{n}{\theta} + n \log a -\sum_{j}^{} \log x_i \\
    \frac{\partial^2}{\partial \theta^2} l(x,a,\theta) &=& - \frac{n}{\theta^2} < 0 \\
    \hat\theta &=& \frac{n}{ \sum_{i}^{} \left( \log x_i - \log a \right)} 
    = \frac{n}{ \log \left( \prod_i \frac{x_i}{a}  \right) }
\end{eqnarray}

Die suffiziente Statistik $T=\eta (\hat\theta)$ ist eine Funktion des MLE, wobei 
\begin{equation}
    \eta(x) = \frac{1}{ \exp\left( \frac{n}{x} \right) a^n}.
\end{equation}
Daher ist der MLE $\hat\theta$ auch suffizient.



\paragraph{Diskrete Gleichverteilung. MLE und UMVUE.} Seien $X_1,\ldots,X_n$ i.i.d.\ 
diskret Gleichverteilt auf der Menge $\left\{ 1,2,\ldots,\theta \right\}$. 
\begin{enumerate}
    \item Zeigen Sie, dass der Maximum-Likelihood-Schätzer für $\theta$ gerade
        $\hat \theta(X_1,\ldots,X_n) = X_{(n)} = \max \left\{ X_1,\ldots,X_n \right\}$ ist.  
    \item Zeigen Sie, dass $\hat \theta = X_{(n)}$ vollständig und suffizient jedoch verzerrt ist.
%    \item Bestimmen Sie mit Momentenmethode einen Schätzer der unverzerrt ist.
    \item Konstruieren Sie einen \textsc{umvue}-Schätzer der folgenden Gestalt
        \begin{equation}
            \frac{ X_{(n)}^{n+1} - ( X_{(n)} - 1)^{n+1}  }{ X_{(n)}^n - (X_{(n)} -1)^n }.
        \end{equation}
\end{enumerate}

\paragraph*{Lösungsskizze. }
Die Likelihood-Funktion ist
\begin{equation}
    L(\theta,x) = \frac{1}{\theta^n} \prod_i 1_{[0,\theta]}(x_i).
\end{equation}
Für einen fixen Set von Beobachtungen $(x_1,\ldots,x_n)$ wird die Likelihood-Funktion
von $\hat\theta(X)=X_{(n)}$ minimiert, denn
\begin{equation}
    L(\hat\theta(x),x) = \max \left\{ L(\theta,x) : \theta\in\Theta \right\}
\end{equation}
und für jedes kleinere $\bar\theta<\hat\theta$ ist die Likelihood-Funtion gleich Null, für alle 
größeren $\bar\theta$ entfernen wir uns von dem Maximum wegen der in $\theta$ fallender Funktion
$\frac{1}{\theta^n}$.

Erwartungswert von $X_{(n)}$ ist kleiner als $\theta$ denn 
\begin{equation}
    E X_{(n)} = \sum_{X\in\left\{ 1,\ldots,\theta \right\}^n}^{} X_{(n)} \frac{1}{\theta^n}
\end{equation}
kann geeignet abgeschätzt werden.

Wegen
\begin{equation}
    p_\theta(x) = \frac{1}{\theta^n} 1_{[0,\theta]} \left( X_{(n)} \right)
\end{equation}
ist $X_{(n)}$ auch suffizient. Für den Beweis der Vollständigkeit setzen wir voraus, dass 
\begin{equation}
    E g(X_{(n)}) =0
\end{equation}
für alle $\theta\in\Theta$ und ein messbares $g$ gilt. 
Mit $\theta=1$ erhalten wir $g(1)=0$, $\theta=2$ impliziert
$g(2)=0$ und so weiter. Damit folgt $P( g(X_{(n)} =0)  )=1$ und die Vollständigkeit
von $X_{(n)}$.

%Aus
%\begin{equation}
%    E X_i = \sum_{i=1}^{\theta} i p_\theta(i) = \sum_{i=1}^{\theta} i \frac{1}{\theta} = \frac{\theta+1}{2}
%\end{equation}
%erhalten wir einen unverzerrten Momentenschätzer $\tilde\theta= 2\bar X -1$.

Ein \textsc{umvue}-Schätzer ist eine Funktion $h(X_{(n)})$ mit $E h(X_{(n)})=\theta$.
Diese Überlegung führt uns zu
\begin{eqnarray}
    E h(X_{)(n}) = \sum_{x\in \left\{ 0,\ldots,\theta \right\}^n }^{} h(x_{(n)}) \frac{1}{\theta^n} = \theta \\
    \sum_{m=1}^{\theta} h(m)(m^n - (m-1)^n) = \theta^{n+1}
\end{eqnarray}
und daraus
\begin{equation}
    h(m) = \frac{m^{n+1} - (m-1)^{n+1}}{m^n - (m-1)^{n}}.
\end{equation}



\paragraph{Invers-Gamma-Verteilung als exponentielle Familie.}
Seien $X_1,\ldots,X_n$ i.i.d.\ Invers-Gamma-verteilte Zufallsvariablen gegeben.
D.h.\ die Dichte von $X_i$ ist 
\begin{equation*}
    f_{a}(x) = \frac{\lambda^a}{\Gamma(a)}x^{-a-1} e^{-\frac{\lambda}{x}}1_{\R_{\geq 0}}(x),
\end{equation*}
wobei $a>0$ unbekannt und $\lambda>0$ bekannt und fix sei.
\begin{enumerate}
    \item Zeigen Sie, dass die von dem Zufallsvektor $( X_1,\ldots,X_n)$
        erzeugte Familie von Verteilungen $\left\{ P_a : a>0 \right\}$ eine
        exponentielle Familie ist, indem Sie die gemeinsame Dichte des Vektors
        $(X_1,\ldots,X_n)$ in die Form
        \begin{equation}
            p_a(x) = \exp\left( c(a) T(x) + d(a) + S(x) \right)1_A (x)
        \end{equation}
        bringen. Geben Sie dabei explizit $c,T,d,S$ und $A$ an.
    \item Zeigen Sie mit Hilfe des Faktorisierungssatzes, dass die Statistik
        $T(X)$ suffizient ist.
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item Die gemeinsame Dichte von $\left( X_1,\ldots,X_n \right)$ ist 
        \begin{equation}
            p_a(x_1,\ldots,x_n) = 
            \prod_i \frac{\lambda^a}{\Gamma(a)}x_i^{-a-1} e^{-\frac{\lambda}{x_i}}1_{\R_{\geq 0}}(x_i).
        \end{equation}
        Die Darstellung als exponentielle Familie ist
        \begin{equation}
            \exp \left( \left( -a-1 \right) \sum_{i}^{} \log x_i + \log \frac{\lambda^{na}}{\Gamma(a)^n} - \lambda \sum_{i}^{} \frac{1}{x_i}  \right) 1_{\R_{\geq 0}^n} (x_1,\ldots,x_n).
        \end{equation}
        Dabei ist 
        \begin{eqnarray}
            c(a)&=&  -a-1 \\
            T(X)&=& \sum_{i}^{} \log X_i \\
            d(a)&=& \log \frac{\lambda^{na}}{\Gamma(a)^n} \\
            S(x)&=&  -\lambda \sum_{i}^{} \frac{1}{x_i}\\
            A   &=& \R_{\geq 0}^n.
        \end{eqnarray}

    \item $T(X)$ ist suffizient, denn die Dichte lässt sich in der Form
        $g(T(x),a)h(x)$ darstellen. Ebenso ist $T(X)$ in jeder exponentiellen
        Familie eine suffiziente Statistik. 
\end{enumerate}



\paragraph{Beta-Verteilung. MLE.} Seien $X_1,\ldots,X_n$ i.i.d.\ und Beta$(\theta+1, 1)$-verteilt
mit dem Parameter $\theta\in\Theta= \left\{ \eta\in\R : \eta>-1 \right\}$. Die Dichte von $X_i$ ist also
\begin{equation}
    p_\theta(x) = (\theta+1)x^\theta 1_{ [0,1] }(x).
\end{equation}
\begin{enumerate}

    \item Bestimmen Sie den Maximum-Likelihood-Schätzer für $\theta$ basierend
        auf der Beobachtung von $\left( X_1,\ldots,X_n \right)$. Zeigen Sie
        dabei, dass der von Ihnen gefundene Schätzer $\hat \theta$ das Maximum
        der Likelihood-Funktion ist. 

    \item Kann der Maximum-Likelihood-Schätzwert $\hat \theta\left( x_1,\ldots,x_n \right)$
        auf dem Rand des Parameterraumes liegen? 
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}

    \item Die Likelihoodfunktion für eine Beobachtung $(X_1,\ldots,X_n)$ ist 
        \begin{eqnarray}
            L(\theta,X) &=& \left( \theta+1 \right)^n \left( \prod_i X_i \right)^{\theta}  \\
            l(\theta,X) &=& n \log \left( \theta+1 \right)  + \theta \sum_{i}^{} \log X_i \\
            \frac{\partial}{\partial \theta} l(\theta,X)&=& \frac{n}{\theta+1} + \sum_{i}^{} \log X_i \\
            \frac{\partial^2}{\partial \theta^2} l(\theta,X) &=& 
            - \frac{n}{\left( \theta+1 \right)^2} < 0.
        \end{eqnarray}
        $\frac{\partial}{\partial \theta} l(\theta,X)=0$ liefert
        \begin{equation}
            \hat\theta = -1 - \frac{n}{\sum_{i}^{} \log X_i}.
        \end{equation}
        $\hat\theta$ ist eindeutiger MLE.

    \item Nein, denn $\Theta$ offen ist.

\end{enumerate}



\paragraph{Bernoulli-Verteilung. Neyman-Pearson-Test.} Seien $X_1,\ldots,X_n$ 
i.i.d.\ Bernoulli($p$) Zufallsvariablen mit Wahrscheinlichkeitsfunktion
\begin{equation}
    P(X_i = k) = p^{k} \left( 1-p \right)^{1-k}, \quad k\in\left\{ 0,1 \right\}.
\end{equation}
Für das Testproblem $H_0 : p=p_0$ gegen $H_1 : p=p_1$ mit $0<p_0<p_1<1$ 
soll ein Neyman-Pearson-Test
\begin{equation}
    \delta_k^\star (X) = 1_{ \left\{ L(x,p_0,p_1)\geq k \right\}  }
\end{equation}
entwickelt werden.
\begin{enumerate}
    \item Berechnen Sie die Likelihood-Quotienten-Statistik $L(X,p_0,p_1)$ basierend auf der
        Beobachtung von $\left( X_1,\ldots,X_n \right)$.
    \item Zeigen Sie, dass die Statistik $T(X)= \sum_{i=1}^{n} X_i$ ebenfalls eine
        optimale Statistik für dieses Testproblem ist.
    \item Seien nun $p_0=\frac{1}{2}$ und $p_1=\frac{3}{4}$. Es wurde folgende Stichprobe beobachtet:
        \begin{lstlisting}
0 1 1 0 1 0 1 1 1 1 1 1 1
        \end{lstlisting}
        Testen Sie mit Hilfe von 
        \begin{equation}
            \delta_l(X) = 1_{   \left\{ T(X) \geq l \right\}},
        \end{equation}
        ob die Nullhypothese $H_0 : p_0=\frac{1}{2}$ bei Signifikanzniveau $\alpha=0.05$
        verworfen werden kann. Dabei können Sie die nachstehende Liste der Paare 
        $\left( k, \sum_{i=k}^{n} \binom{n}{i} p_0^i \left( 1-p_0 \right)^{n-i} \right)$ verwenden.
        \begin{lstlisting}
0  1
1  0.9998779296875
2  0.998291015625
3  0.98876953125
4  0.953857421875
5  0.8665771484375
6  0.70947265625
7  0.5
8  0.29052734375
9  0.1334228515625
10 0.046142578125
11 0.01123046875
12 0.001708984375
13 0.0001220703125
\end{lstlisting}            

\end{enumerate}



\paragraph*{Lösung.}
\begin{enumerate}

    \item Die Likelihood-Quotienten-Statistik ist 
        \begin{eqnarray}
            L(x,p_0,p_1) &=& \frac{ \prod_{i} p_1^{x_i}\left( 1-p_1 \right)^{1-x_i} }{ \prod_{i} p_0^{x_i}\left( 1-p_0 \right)^{1-x_i} } \\
            &=& \left( \frac{p_1}{p_0} \right)^{\sum_{i} x_i} \left( \frac{1-p_1}{1-p_0} \right)^{n - \sum_{i} x_i} \\ 
            &=& \left( \frac{1-p_1}{1-p_0} \right)^n 
            \left( \frac{ p_1 \left( 1-p_0 \right)}{ p_0 \left( 1-p_1 \right)} \right)^{\sum_{i}^{} x_i}.
        \end{eqnarray}
    \item Optimale Statistiken sind auch
        \begin{eqnarray}
            T_1(X) &=&  \left( \frac{ p_1 \left( 1-p_0 \right)}{ p_0 \left( 1-p_1 \right)} \right)^{\sum_{i}^{} x_i} \\
            T_2(X) &=& \left( \sum_{i}^{} x_i \right) \log \frac{ p_1 \left( 1-p_0 \right)}{ p_0 \left( 1-p_1 \right)}. 
        \end{eqnarray}
        Die Ungleichung $p_1(1-p_0)> p_0(1-p_1)$ impliziert
        \begin{equation}
            \log \frac{ p_1 \left( 1-p_0 \right)}{ p_0 \left( 1-p_1 \right)} >0 
        \end{equation}
        und
        \begin{eqnarray}
            T(X) &=& \sum_{i}^{} x_i
        \end{eqnarray}
        ist ebenfalls optimale Statistik.
    \item Für die angegebene Beobachtung ist $T(X)=10$. Der zum Signifikanzniveau $\alpha=0.05$ 
        gehörige kritische Wert folgt aus
        \begin{eqnarray}
            P_{p_0} \left( T(X) \geq k \right) &\leq& \alpha \\
            P_{p_0} \left( \sum_{i=1}^{n} X_i \geq k \right) &\leq& \alpha \\
            \sum_{i=k}^{n} \binom{n}{i} p_0^i \left( 1-p_0 \right)^{n-i} & \leq & \alpha
        \end{eqnarray}
        Laut Tabelle wählen wir $k=10$. Die Nullhypothese wird also verworfen.
\end{enumerate}




\paragraph{Konfidenzintervall. Mittelwert der Normalverteilung. }
Seien $X_1,\ldots,X_n$ i.i.d.\ normalverteilt mit $X_i \sim \mathcal N \left(\theta, 1 \right)$,
wobei $\theta\in\R$ ein unbekannter Parameter ist.
\begin{enumerate}
    \item Zeigen Sie, dass die Verteilung von 
        \begin{equation}
            \sqrt{n}\left( \bar X -\theta \right)
        \end{equation}
        unabhängig von dem Parameter $\theta$ ist. Dabei ist $\bar X = \frac{1}{n} \sum_{i=1}^{n} X_i$.
    \item Berechnen Sie ein $\left( 1-\alpha \right)$-Konfidenzintervall für $\theta$.
\end{enumerate}


\paragraph*{Lösung.}
\begin{enumerate}

    \item Die Verteilung von $\sqrt{n} \left( \bar X -\theta \right)$ ist
        \begin{eqnarray}
            \sum_{i=1}^{n} X_i &\sim& \mathcal N \left( n\theta, n \right) \\
            \frac{1}{n} \sum_{i=1}^{n} X_i &\sim& \mathcal N \left( \theta, \frac{ n}{n^2} \right) \\
            \sqrt{n}\left( \bar X -\theta \right) & \sim & \mathcal N \left( 0, 1 \right),
        \end{eqnarray}
        also unabhängig von $\theta$.
    \item Konfidenzintervall
        \begin{eqnarray}
            P_\theta \left( z_{\frac{\alpha}{2}} \leq \sqrt{n}\left( \bar X -\theta \right) \leq z_{1-\frac{\alpha}{2}}  \right) &=&  1-\alpha \\
            P_\theta \left( \theta\in \left[ \bar X - \frac{z_{1-\frac{\alpha}{2}}}{\sqrt{n}}, \bar X - \frac{z_{\frac{\alpha}{2}}}{\sqrt{n}} \right] \right) &=&  1-\alpha.
        \end{eqnarray}

\end{enumerate}



\paragraph{Gamma-Verteilung. Momentenschätzer.}
Gegeben seien i.i.d.\ Gamma-verteilte Zufallsvariablen $X_1,\ldots,X_m$ mit der Dichte
\begin{equation}
    p_{a,\lambda}(x) = \frac{\lambda^a}{\Gamma(a)}x^{a-1}e^{-\lambda x} 1_{\R_{\geq 0}}(x)
\end{equation}
und Parametern $a>0$ und $\lambda>0$.
\begin{enumerate}
    \item Zeigen Sie für ein positives $n\in\mathbb N$
        \begin{equation}
            E X_i^n = \frac{a\cdot\ldots\cdot (n+a-1)}{\lambda^n}.
        \end{equation}
        Dabei können Sie die Identität 
        $\Gamma(a+1)=a \Gamma(a)$, $a\in\R \setminus \left\{ -1,-2,\ldots \right\}$, verwenden.
    \item Berechnen Sie einen Momentenschätzer für die Parameter $a$ und $\lambda$ basierend auf
        der Beobachtung von $\left( X_1,\ldots,X_m \right)$. 
\end{enumerate}


\paragraph*{Lösung.}
\begin{enumerate}

    \item \begin{eqnarray}
            E X^n &=& \int_{0}^{\infty} x^n \frac{\lambda^a}{\Gamma(a)}x^{a-1}e^{-\lambda x} dx \\
            &=& \frac{a \cdot\ldots\cdot \left( a+n-1 \right)}{\lambda^n} 
            \int_{0}^{\infty} \frac{\lambda^{a+n}}{\Gamma(n+a)} x^{a+n-1} e^{-\lambda x} dx \\
            &=& \frac{a\cdot \ldots \cdot \left( a+n-1 \right)}{\lambda^n} \\
            &=& \frac{\Gamma\left( a+n \right)}{ \lambda^n \Gamma\left( a \right)}.
        \end{eqnarray}

    \item Die Momentenschätzer für die Parameter $a$ und $\lambda$ sind
        \begin{eqnarray}
            E X &=& \frac{a}{\lambda} \\
            E X^2 &=&  \frac{a(a+1)}{\lambda^2} \\
            E X^2 &=&  \frac{a(a+1)}{a^2} \left( E X \right)^2 \\
            E X^2 &=& \left( 1 + \frac{1}{a} \right) \left( E X \right)^2 \\
            a &=& \frac{ \left( E X \right)^2 }{ E X^2 - \left( E X \right)^2}  \\
            \lambda &=&  \frac{ EX }{ E X^2 - \left( E X \right)^2} 
        \end{eqnarray}
        Die Schätzer erhält man mit Substitutionen $EX \to \frac{1}{n} \sum_{i}^{} X$ und 
        $E X^2 \to \frac{1}{n} \sum_{i}^{} X^2$.


\end{enumerate}



\paragraph{Vollständigkeit. } Seien die Statistiken $T$ und $S$ mit $S=\eta(T)$ und einer messbaren 
Abbildung $\eta$ gegeben. 
\begin{enumerate}
    \item Zeigen Sie: Falls $T$ vollständig ist, so ist $S$ ebenfalls vollständig.
    \item Geben Sie ein Beispiel einer nicht vollständigen Statistik, in einem von Ihnen
        gewählten statistischen Modell.
    \item Zeigen Sie, dass die Umkehrung der ersten Aussage falsch ist: Aus Vollständigkeit von $S$ 
        folgt nicht die Vollständigkeit von $T$. Konstruieren Sie ein Gegenbeispiel.
\end{enumerate}


\paragraph*{Lösung.}
\begin{enumerate}

    \item $T$ ist vollständig, d.h.\ für ein messbares $g$ gilt, $E_\theta g(T) = 0$ für alle $\theta$ impliziert 
        $P_\theta \left( g(T)=0  \right)=1$ für alle $\theta$. Für ein $g$ ist
        $E g(S)=0 \ \forall \theta$ genau dann wenn $E g(\eta(T))=0 \ \forall \theta$, impliziert
        $P\left( g(\eta(T))=0 \right)=1 \ \forall \theta$. Das ist aber gleichbedeutend mit 
        $P\left( g(S)=0 \right)=1 \ \forall \theta$. $S$ ist also vollständig. 

    \item Sei $\Theta$ einelementig, $X\sim \mathcal N(0,1)$ und $T(X)=X$. Für $g=\textrm{id}$ ist zwar
        $E_\theta g(T(X)) = E_\theta X = 0 \ \forall \theta$ aber nicht 
        $P_\theta \left( g(T(X))=0 \right)=  P_\theta \left( X=0 \right)=1$. $T(X)$ ist also nicht vollständig. 

    \item Für eine beliebige unvollständige Statistik $T$ und $\eta\equiv 0$ ist $S=\eta(T)$ vollständig. 

\end{enumerate}




\paragraph{$3$-Stichprobenproblem. } 
Gegeben sei das Modell
\begin{equation}
    Y_{kl} = \beta_k + \varepsilon_{kl}
\end{equation}
mit $k\in \left\{ 1,2,3 \right\}$, $l\in \left\{ 1,2 \right\}$ und 
$\varepsilon_{kl}\sim \mathcal N(0,\sigma^2)$, i.i.d.\  $\sigma>0$.
\begin{enumerate}
    \item Geben Sie die Designmatrix $X$ für dieses Model explizit an.
    \item Geben Sie eine koordinatenfreie Darstellung dieses Models an.
    \item Berechnen Sie Kleinste-Quadrate-Schätzwerte basierend auf der Beobachtung
        \begin{equation}
            \left( Y_{11},Y_{12},Y_{21},Y_{22},Y_{31},Y_{32} \right) = \left(74, 92, 16, 22, 98, 16 \right).
        \end{equation}
\end{enumerate}


\paragraph*{Lösung.}
\begin{enumerate}

    \item Das Gleichungsystem in der Kurzschreibweise:
        \begin{equation}
            Y = X \beta + \varepsilon
        \end{equation}
        Dabei ist $Y= \left( Y_1,Y_2,Y_3 \right)^\top$, $\beta = \left( \beta_1,\beta_2,\beta_3 \right)^\top$,
        $\varepsilon \sim\mathcal N\left( 0, \sigma^2 I_6 \right)$
        und die Designmatrix
        \begin{equation}
            X = \left( {\begin{array}{ccc} 1&0&0\\ 1&0&1\\ 0&1&0\\ 0&1&0\\ 0&0&1\\ 0&0&1 \end{array}   } \right).
        \end{equation}

    \item Koordinatenfreie Darstellung ist $Y = \zeta + \varepsilon$ mit 
        \begin{equation}
            \zeta^\top = \left( \beta_1,\beta_1,\beta_2,\beta_2,\beta_3,\beta_3 \right).
        \end{equation}

    \item Wegen Rang$(X)=p$ ist der Kleinste-Quadrate-Schätzer für $\beta$ gegeben durch 
        $\hat \beta = \left( X^\top X \right)^{-1} X^\top Y$. Das führt uns auf 
        \begin{eqnarray}
            \hat\beta_1 &=&  \frac{1}{2} \left( Y_1+Y_2 \right)    \\
            \hat\beta_2 &=&  \frac{1}{2} \left( Y_3+Y_4 \right)    \\
            \hat\beta_3 &=&  \frac{1}{2} \left( Y_5+Y_6 \right).
        \end{eqnarray}
        Entsprechende Werte sind also zu mitteln.
        

\end{enumerate}



%\paragraph{Informationsungleichung und Normalverteilung. } Gegeben seien $X_1,\ldots,X_n$
%i.i.d.\ normalverteilte Zufallsvariablen mit $X_i\sim \mathcal N(\theta, \sigma^2)$, wobei $\theta\in\R$
%und $\sigma$ bekannt und fix ist.
%\begin{enumerate}
%    \item Zeigen Sie, dass dieses Modell Cram\'er-Rao-Regularitätsbedingungen erfüllt.
%    \item Berechnen Sie die Fischer-Information für den Parameter $\theta$.
%    \item Berechnen Sie die Cram\'er-Rao-Schranke für die Varianz des Schätzers
%        \begin{equation}
%            T(X)=\bar X = \frac{1}{n} \sum_{i=1}^{n}X_i.
%        \end{equation}
%\end{enumerate}



\paragraph{Gamma-Verteilung. MLE.} Seien $X_1, \ldots,
X_n$ i.i.d.\ Gamma-verteilt mit der Dichte
\begin{equation}
    p_{a,\lambda}(x) = \frac{\lambda^a}{\Gamma(a)}x^{a-1}e^{-\lambda x} 1_{\R_{>0}}(x)
\end{equation}
und Parametern $a$ und $\lambda$. Der Parameter $a>0$ ist bekannt und fix. 
Der Parameter $\lambda$ dagegen unbekannt und $\lambda > 0$.
\begin{enumerate}
    \item Berechnen Sie einen Maximum-likelihood Schätzer für $\lambda$ 
        basierend auf der Beobachtung von dem Vektor
        $(X_1, \ldots, X_n)$. Zeigen Sie, dass der von Ihnen gefundene Schätzer 
        $\hat \lambda$ das Maximum der Likelihood-Funktion ist.
    \item Ist der von Ihnen gefundene Maximum-likelihood Schätzer erwartungstreu? 
        Begründen Sie Ihre Antwort. Dabei können Sie auf die Beziehung 
        \begin{equation}
            X_1+ \ldots + X_n \sim \textrm{Gamma}(na,\lambda)
        \end{equation}
        zugreifen.
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item Der Standardmethode folgend, berechnen wir die Log-likelihood
        Funktion und finden ihr Maximum.
        \begin{eqnarray}
            L(a,\lambda; X) &=& \frac{\lambda^{an}}{\Gamma(a)^n} \prod_i x_i^{ a-1 } e^{-\lambda x_i} \\
            l(a,\lambda; X) &=& na \log \lambda - n \log \Gamma\left( a \right) + 
                                \left( a-1 \right) \sum_{i}^{} \log x_i - \lambda \sum_{i}^{} x_i \\
            \frac{\partial}{\partial \lambda} l(a,\lambda; X) &=&  \frac{na}{\lambda} - \sum_{i}^{} x_i = 0  \\
            \hat \lambda &=& \frac{na}{\sum_{i}^{} x_i} \\
            \frac{\partial^2}{\partial \lambda^2} l(a,\lambda; X) &=& - \frac{na}{ \lambda^2} <0 \quad \forall \lambda>0.
        \end{eqnarray}
        MLE ist also tatsächlich das Maximum der Likelihood Funktion. 
    \item Unter Benutzung von $X_1+ \ldots + X_n \sim \textrm{Gamma}(na,\lambda)$ berechnen wir
        \begin{eqnarray}
            \E \hat \lambda &=& \E \frac{na}{ \sum_{i}^{} X_i }  \\
            &=& \int_{0}^{\infty} \frac{na}{x} \frac{\lambda^{na}}{\Gamma(na)} x^{na-1} e^{-\lambda x} dx \\
            &=& \frac{na \lambda^{na}}{\Gamma(na)} \int_{0}^{\infty} x^{na -1-1} e^{-\lambda x}dx \\
            &=& \frac{na \lambda^{na}}{\Gamma(na)} \frac{\Gamma(na-1)}{\lambda^{na-1}} \\
            &=& \frac{na}{na-1} \lambda.
        \end{eqnarray}
        Der Schätzer $\hat\lambda$ ist somit nicht erwartungstreu.
\end{enumerate}



\paragraph{Lineare Regression mit Schalter.} Gegeben sei das Model
\begin{eqnarray}
    Y_i &=&  \beta_0 x_i + \varepsilon_i \quad\quad i=1,\ldots ,m \\
    Y_i &=& \beta_1 + \varepsilon_i \quad\quad  i=m+1, \ldots , m+n
\end{eqnarray}
mit $m,n \in \bN$ und $\varepsilon_i \in \cN \left( 0, \sigma^2 \right)$, i.i.d.\ $\sigma>0$.
\begin{enumerate}
    \item Geben Sie die Designmatrix $X$ für dieses Model explizit an.
    \item Berechnen Sie den Kleinste-Quadrate-Schätzer $\left( \hat \beta_0, \hat \beta_1 \right)$ 
        basierend auf der Beobachtung von  
        $\left( Y_1, \ldots , Y_{m+n} \right)$ und $\left( X_1, \ldots , X_m \right)$.
    \item Berechnen Sie den Kleinste-Quadrate-Schätzer $\left( \hat \beta_0, \hat \beta_1 \right)$ basierend auf 
        der Beobachtung von
        \begin{eqnarray}
            Y &=&  \left( 1,4,2,8,6,0 \right)^\top \\
            X &=& \left( 3,2,1 \right)^\top.
        \end{eqnarray}
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item Die Desingmatrix für dieses Model ist
            \begin{equation}
                X = \left(
                \begin{array}{cc}
                    x_1 & 0 \\
                    \vdots & \vdots \\
                    x_m & 0 \\
                    0 & 1 \\
                    \vdots & \vdots \\
                    0 & 1 
                \end{array}
                \right).
            \end{equation}
        \item Die allgemeine Form des LSE ist 
            $\hat \beta = \left( X^\top X \right)^{-1} X^\top Y$. Wir berechnen
            \begin{eqnarray}
                X^\top X &=& \left(
                \begin{array}{cc}
                    \sum_{i=1}^{m} x_i^2 & 0 \\
                    0 & n 
                \end{array}
                \right) \\
                \left( X^\top X \right)^{-1} &=& \left(
                \begin{array}{cc}
                    \frac{1}{\sum_{i=1}^{m} x_i^2} & 0 \\
                    0 & \frac{1}{n} 
                \end{array}
                \right) \\
                \left( X^\top X \right)^{-1} X^\top &=& \left(
                \begin{array}{cccccc}
                    \frac{x_1}{\sum_{i=1}^{m} x_i^2} & \cdots & \frac{x_m}{\sum_{i=1}^{m} x_i^2} & 
                        0 & \cdots & 0 \\
                        0& \cdots &0& \frac{1}{n} & \cdots & \frac{1}{n} 
                \end{array}
                \right) \\
                \left( X^\top X \right)^{-1} X^\top Y &=& 
                \left(
                \begin{array}{c}
                    \frac{1}{\sum_{i=1}^{m} x_i^2} \left( x_1 y_1 + \ldots + x_m y_m \right)  \\
                    \frac{1}{n} \left( y_{m+1}+ \ldots + y_{m+n} \right)
                \end{array}
                \right) = \left(
                \begin{array}{c}
                    \hat\beta_0 \\ \hat\beta_1
                \end{array}
                \right).
            \end{eqnarray}
        \item Die Beobachtung impliziert $m=3$ und $n=3$. Mit der obigen Formel erhalten wir
            $\hat \beta_0 = \frac{13}{14}$ und $\hat \beta_1 = \frac{14}{3}$.
\end{enumerate}








\paragraph{UMVUE-Schätzer für Poisson-Verteilung. } Seien $X_1, \ldots, X_n$ i.i.d.\ 
und Poissonverteilt mit der Wahrscheinlichkeitsfunktion
\begin{equation}
    p(k) = \bP \left( X_i = k \right) = e^{-\lambda} \frac{\lambda^k}{ k!} \quad k \in \left\{ 0,1, \ldots \right\}
\end{equation}
und $\lambda>0$.
\begin{enumerate}
    \item Zeigen Sie, dass die Statistik $T\left( X \right) = \sum_{i=1}^{n} X_i$ vollständig und suffizient 
        für dieses Testproblem ist.
    \item Zeigen Sie, dass der Schätzer $S\left( X \right) = \frac{1}{n} \sum_{i=1}^{n} X_i$
        von $\lambda$ unverzerrt ist.
    \item Zeigen Sie, dass $S(X)$ ein eindeutiger \textsc{umvue}-Schätzer für $\lambda$ ist.
\end{enumerate} 

\paragraph*{Lösung.}
\begin{enumerate}
    \item Die Wahrscheinlichkeitsfunktionen von $\left( X_1, \ldots, X_n \right)$ 
        bilden eine exponentielle Familie, denn
        \begin{eqnarray}
            \prod_{i=1}^{n} p(x_i) &=& \prod_{i=1}^n e^{-\lambda } \frac{ \lambda^{ x_i }}{ x_i!} \\
            &=& \exp \left( -\lambda n + \log \lambda \cdot \sum_{i=1}^{ n} x_i - \sum_{i=1}^{n} \log x_i! \right).
        \end{eqnarray}
        Somit ist $T(X)= \sum_{i}^{} X_i$ eine suffiziente Statistik. $T(X)$ ist auch vollständig, denn 
        $\left\{ \log \lambda \ | \ \lambda>0 \right\}$ enthält ein offenes Rechteck. Die 
        Vollständigkeit kann man hier auch leicht aus der Definition herleiten. 
    \item Wir berechnen
        \begin{eqnarray}
            \E X_i &=& \sum_{k\geq 0}^{} k e^{-\lambda} \frac{\lambda^k}{ k!} = \lambda.
        \end{eqnarray}
        Daraus ergibt sich
        \begin{eqnarray}
            \E S(X) &=& \frac{1}{n} \sum_{i}^{} \E X_i = \lambda.
        \end{eqnarray}
        $S(X)$ ist also ein unverzerrter Schätzer von $\lambda$. 
    \item Nachdem $S(X) = \frac{1}{n} T(X)$ gilt, ist 
        \begin{equation}
            \E \left( S(X) | T(X) \right) = S(X)
        \end{equation}
        und somit ist $S(X)$ nach dem Satz von Lehmann-Scheffe ein \textsc{umvue}-Schätzer
        für $\lambda$. Die Eindeutigkeit folgt aus $\Var_\lambda T(X) < \infty$ für alle
        $\lambda >0$. Das ist der Fall denn
        \begin{eqnarray}
            \E X^2 &=& \lambda\left( \lambda +1 \right) \\
            \Var X &=& \lambda.
        \end{eqnarray}
\end{enumerate}









\paragraph{Konfidenzintervall und Test für die Gamma-Verteilung. }
Seien $X_1, \ldots, X_n$ i.i.d.\ Gamma-verteilt mit der Dichte
\begin{equation}
    p_{a,\lambda} (x) = \frac{\lambda^a}{ \Gamma(a)} x^{a-1} e^{-\lambda x} 1_{ \R_{>0}  } (x)
\end{equation} 
und Parametern $a>0$ und $\lambda>0$. Der Parameter $a$ wird als bekannt und fix vorausgesetzt. 
Des weiteren gelten für $c>0$ die Beziehungen
\begin{eqnarray}
    X_i + X_j &\sim& \textrm{Gamma}_{a+a, \lambda}, \\
    c X_i & \sim & \textrm{Gamma}_{a, \frac{\lambda}{c}}.
\end{eqnarray}
\begin{enumerate}
    \item Zeigen Sie, dass \begin{eqnarray}
            \left[  \frac{ \textrm{Gamma}^{-1}_{na, 1} ( \frac{\alpha}{2})  }{X_1 + \ldots + X_n}, 
            \frac{ \textrm{Gamma}^{-1}_{na, 1} ( 1- \frac{\alpha}{2})  }{X_1+ \ldots + X_n}  \right]
        \end{eqnarray} ein $(1-\alpha)$-Konfidenzintervall für $\lambda$ ist.
        Dabei bezeichnet $\textrm{Gamma}^{-1}_{a,\lambda}$ die Inverse der Verteilungsfunktion 
        $\textrm{Gamma}_{a,\lambda}$.
    \item Konstruieren Sie aus dem obigen Konfidenzintervall einen Test zum Signifikanzniveau
        $\alpha$ für $H_0: \lambda=\lambda_0$ gegen $H_1: \lambda\neq \lambda_0$.
\end{enumerate} 

\paragraph*{Lösung.}
\begin{enumerate}
    \item Die Verteilung von $\lambda (X_1 + \ldots + X_n)$ ist $\textrm{Gamma}_{na, 1}$.
    \item Mit der Bezeichnung 
        \begin{equation}
            \left[ L(X), U(X) \right] = 
            \left[  \frac{ \textrm{Gamma}^{-1}_{na, 1} ( \frac{\alpha}{2})  }{X_1 + \ldots + X_n}, 
            \frac{ \textrm{Gamma}^{-1}_{na, 1} ( 1- \frac{\alpha}{2})  }{X_1+ \ldots + X_n}  \right]
        \end{equation}
        erhalten wir die Testfunktion
        \begin{equation}
            \delta(X) = 1_{ \left\{ \lambda \nin \left[ L(X), U(X) \right] \right\}   }
        \end{equation} die das Gewünschte erfüllt. 
\end{enumerate}








\paragraph{Neyman-Pearson-Test. Geometrische Verteilung. } Seien $X_1,\ldots,X_n$ 
i.i.d.\ geometrisch verteilt mit der Wahrscheinlichkeitsfunktion
\begin{equation}
    \pi(k) = \bP(X_i = k) = p \left( 1-p \right)^k, \quad k\in\left\{ 0,1,2, \ldots \right\}
\end{equation}
und Parameter $p\in \left( 0,1 \right)$.
Für das Testproblem $H_0 : p=p_0$ gegen $H_1 : p=p_1$ mit $1>p_0 > p_1 >0$ 
soll ein Neyman-Pearson-Test
\begin{equation}
    \delta_k^\star (X) = 1_{ \left\{ L(x,p_0,p_1)\geq k \right\}  }
\end{equation}
entwickelt werden.
\begin{enumerate}
    \item Berechnen Sie die Likelihood-Quotienten-Statistik $L(X,p_0,p_1)$ basierend auf der
        Beobachtung von $\left( X_1,\ldots,X_n \right)$.
    \item Zeigen Sie, dass die Statistik $T(X)= \sum_{i=1}^{n} X_i$ ebenfalls eine
        optimale Statistik für dieses Testproblem ist.
    \item Seien nun $p_0=\frac{2}{3}$ und $p_1=\frac{1}{3}$. Es wurde folgende Stichprobe ($n=7$) beobachtet:
        \begin{lstlisting}
1 0 1 0 0 0 4
        \end{lstlisting}   % rgeom(7, 0.66666666)
        Testen Sie mit Hilfe von 
        \begin{equation}
            \delta_l(X) = 1_{   \left\{ T(X) \geq l \right\}},
        \end{equation}
        ob die Nullhypothese $H_0 : p_0=\frac{2}{3}$ bei Signifikanzniveau $\alpha=0.05$
        verworfen werden kann. In der nachstehenden Tabelle finden Sie die Werte der Verteilungsfunktion
        von $T(X)$ für $n=7$ und $p=\frac{2}{3}$. D.h.\ jede Zeile enthält ein Paar 
        $\left( k, \bP_{\frac{2}{3}} \left( T(X) \leq k \right) \right)$.
        \begin{lstlisting}
0   0.05852766
1   0.1950922
2   0.3771783
3   0.5592643
4   0.7110027
5   0.8222775
6   0.8964608
7   0.9423837
8   0.9691721
9   0.9840545
10  0.9919918
\end{lstlisting}   % for( q in 0:25 ) { print(c(q, pnbinom(q, 7, 0.66666666666)))     }         
\emph{Bemerkung.} $T(X)$ ist negativ binomialverteilt mit Parametern $\left( n,p \right)$.
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item Die Wahrscheinlichkeitsfunktion von dem Vektor $\left( X_1, \ldots, X_n \right)$ ist
        \begin{eqnarray}
            \pi\left( k_1, \ldots, k_n \right) &=& \prod_i p(1-p)^{k_i} \\
            &=& p^n (1-p)^{\sum_{i}^{} k_i}.
        \end{eqnarray}
        Somit ist die Likelihood-Quotienten-Statistik gegeben durch
        \begin{eqnarray}
            L( (k_i), p_0, p_1) &=& \frac{p_1^n (1-p_1)^{\sum_{i} k_i}}{ p_0^n (1-p_0)^{\sum_{i} k_i}} \\
            &=& \left( \frac{p_1}{p_0} \right)^n \left( \frac{1-p_1}{1-p_0} \right)^{\sum_{i} k_i}.
        \end{eqnarray}
    \item Da 
        \begin{equation}
            \frac{1-p_1}{1-p_0} > 1,
        \end{equation}
        entsteht $T(X) = \sum_{i}^{} X_i$ aus $L( (X_i), p_0, p_1 )$ durch eine monotone Transformation.
        Damit ist $T(X)$ ebenfalls eine optimale Statistik für dieses Testproblem.
    \item Wir berechnen zunächst den zu dem Signifikanzniveau $\alpha=0.05$ gehörigen Verwerfungsbereich
        $[l, +\infty)$. 
        \begin{eqnarray}
            \bP_{p_0} \left( T(X) \geq l \right) & \leq & \alpha \\
            \bP_{p_0} \left( \sum_{i}^{} X_i < l \right) & \geq & 1-\alpha \\ 
            \bP_{p_0} \left( \sum_{i}^{} X_i \leq l-1 \right) & \geq & 0.95 \\ 
        \end{eqnarray}
        Aus der Tabelle lesen wir $l-1=8$ ab und somit $l=9$. Nachdem $T(X)=6$, kann die Nullhypothese
        nicht verworfen werden.
\end{enumerate}






\paragraph{Beta-Verteilung. Momentenschätzer.} Gegeben seien $X_1, \ldots, X_n$ i.i.d. Beta-verteilte
Zufallsvariablen mit der Dichte
\begin{equation}
    p_{a,b}(x) = \frac{1}{B(a,b)} x^{a-1} (1-x)^{b-1} 1_{ \left[ 0,1 \right]  } (x)
\end{equation}
und Parametern $a>0$ und $b>0$. Dabei ist $B(a,b)$ die Betafunktion charakterisiert durch
\begin{equation}
    B(a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}.
\end{equation}
\begin{enumerate}
    \item Zeigen Sie, dass das $k$-te Moment der Beta-Verteilung die Gleichung
        \begin{equation}
            \E X^{k} = \frac{a+k-1}{a+b+k-1} \E X^{k-1}
        \end{equation}
        für $k\in \mathbb N$ und $k\geq 1$ erfüllt. 
    \item Berechnen Sie den Momentenschätzer für die Parameter $a$ und $b$ basierend auf dem ersten und zweiten Moment.
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item Durch direktes Berechnen des $k$-ten Moments erhalten wir 
        \begin{equation}
            \E X^k = \frac{ a \cdot \ldots \cdot (a+k-1) }{ (a+b)\cdot \ldots \cdot (a+b+k-1)}.
        \end{equation}
        Daraus folgt die Identität. 
    \item Die Substitutionen $E X = \hat m_1 = \frac{1}{n} \sum_{i=1}^{n} X_i$ und 
        $E X^2 = \hat m_2 = \frac{1}{n} \sum_{i=1}^{n} X_i^2$ führen zu 
        \begin{eqnarray}
            \hat m_1 &=&  \frac{a}{a+b} \\
            \hat m_2 &=&  \frac{ a(a+1) }{ (a+b)(a+b+1) }.
        \end{eqnarray}
        Die Auflösung nach $a$ und $b$ liefert 
        \begin{eqnarray}
            \hat a &=& \frac{ \hat m_1^2 - \hat m_2^2 }{ \hat m_2 - \hat m_1^2  } \\
            \hat b &=& \frac{a (1-\hat m_1)}{\hat m_1} = \frac{ (\hat m_1^2 - \hat m_2^2)(1-\hat m_1) }{(\hat m_2 - \hat m_1^2)\hat m_1}.
        \end{eqnarray}
\end{enumerate}









\paragraph{Turmeigenschaft der bedingten Erwartung.} Sei ein
Wahrscheinlichkeitsraum $\left( \Omega, P, \mathcal A \right)$, eine
integrierbare Zufallsvariable $X$ und $\mathcal G, \mathcal H$
sub-$\sigma$-Algebren von $\mathcal A$ gegeben. Es gilt $\mathcal H \subset
\mathcal G \subset \mathcal A$. $Y$ ist bedingte Erwartung von $X$ bezüglich $\cA$, notiert
als $Y = \E \left[ X | \cA \right]$, genau dann wenn $\E 1_A Y = \E 1_A X$ für alle $A\in\cA$ gilt.
\begin{enumerate}
    \item Zeigen Sie $\E \left[  \E \left[ X | \mathcal H \right] | \mathcal G \right] = \E \left[ X | \mathcal H \right]$ $P$-fast sicher.
    \item Zeigen Sie $\E \left[  \E \left[ X | \mathcal G \right] | \mathcal H \right] = \E \left[ X | \mathcal H \right]$ $P$-fast sicher.
\end{enumerate}

\paragraph*{Lösung.} Wir zeigen zunächst 
$\E\left[ \E\left[ X | \mathcal H \right] | \mathcal G \right]=\E \left[ X | \mathcal H \right]$.
Nach Definition des bedingten Erwartungswertes ist $\E\left[ X| \mathcal H \right]$ 
$\mathcal H$-messbar und daher $\mathcal G$-messbar. So bekommen wir 
\begin{equation}
	\E\left[ \E\left[ X | \mathcal H \right] | \mathcal G \right] =
	\E\left[ X| \mathcal H \right] \E\left[ 1|\mathcal G \right] = 
	\E \left[ X | \mathcal H \right ].
\end{equation}
Nun kommen wir zu 
$\E\left[ \E\left[ X | \mathcal G \right] | \mathcal H \right]=\E \left[ X | \mathcal H \right]$.
Dies gilt genau dann wenn 
\begin{equation}
	\E\left\{ X 1_A \right\} 
	= \E\left\{ \E\left[ \E\left[ X|\mathcal G \right]|\mathcal H \right] 1_A \right\}  
	\quad \forall A\in\mathcal H.
\end{equation}
Sei $Y=\E\left[ X|\mathcal G \right]$ eine $\mathcal G$-messbare Zufallsvariable und
$A\in\mathcal H$. Die Indikatorfunktion $1_A$ ist $\mathcal H$-messbar und daher
$\mathcal G$-messbar. Wir berechnen
\begin{eqnarray}
	\E\left\{ \E\left[ \E\left[ X|\mathcal G \right]|\mathcal H \right] 1_A \right\} &=& 
	\E\left\{ \E\left[ Y|\mathcal H \right] 1_A \right\} \\
	&=& \E\left\{ \E\left[ Y 1_A| \mathcal H \right] \right\} \\
	&=& \E\left\{ Y 1_A \right\} \\
	&=& \E\left\{ \E\left[ X|\mathcal G \right] 1_A \right\} \\
	&=& \E\left\{ \E\left[ X 1_A| \mathcal G \right] \right\} = \E\left\{ X 1_A \right\}. 
\end{eqnarray}
Daher ist nach Definition des bedingten Erwartungswertes
\begin{equation}
	\E\left[ \E\left[ X | \mathcal G \right] | \mathcal H \right]=\E \left[ X | \mathcal H \right].
\end{equation}




\subsection{2013}

\paragraph{L\'evy Verteilung.} Die Dichte der L\'evy Verteilung mit den
Parametern $(\mu,\sigma)\in \Theta = \R \times \R_{>0}$ ist
\begin{equation*}
    p_{(\mu, \sigma)} = \sqrt{ \frac{\sigma}{2\pi}} (x-\mu)^{-\frac{3}{2}} 
    \exp\left( -\frac{\sigma}{2(x-\mu)} \right) 1_{\R_{\geq \mu}} (x).
\end{equation*}
Zeigen Sie folgende Aussagen.
\begin{enumerate}
    \item Sei $X\sim \cN (0, \frac{1}{\sqrt{\sigma}})$ normalverteilt mit $\sigma>0$.
        Die Zufallsvariable $\frac{1}{X^2}$ ist L\'evy-verteilt mit den Parametern
        $(0,\sigma)$.
    \item Wenn $\mu=0$ festgehalten wird, ist die Familie $(p_{(0,\sigma)})_{\sigma>0}$
        eine exponentielle Familie. 
    \item Die Familie $(p_{\mu,\sigma})_{(\mu,\sigma)\in\Theta}$ ist keine
        (zweiparametrige) exponentielle Familie. 
\end{enumerate}








