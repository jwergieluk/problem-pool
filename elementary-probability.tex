

\section{Elementary probability}


\paragraph{Ereignisse. Mengenoperationen.}
Gegeben seien jeweils die Ergebnismenge $\Omega$ sowie zwei Teilmengen
$A$ und $B$:
\begin{enumerate}
\item $\Omega=\{1,2,...,20\}$, $A=\{4,5,6,7,9,11\}$, $B=\{3,5,9,20\}$
\item $\Omega=[-1,3]$, $A=[0,1)$, $B=(\frac{1}{2},2]$
\item $\Omega=\mathbb R$, $A=\{x\in \mathbb R: |x-1|<3\}$, $B=[0,\infty)$.
\end{enumerate}
Bilden Sie die Mengen $\overline{A}$, $\overline{B}$, $A\cap B$, $A\cup B$,
$\overline{ A\cup B}$, $\overline{A}\cap\overline{B}$, $B\cap\overline{A}$,
$(\overline{A}\cup\overline{B})\cap\overline{B}$,
$B\cup(\overline{B\cap\overline{A}})$.


\paragraph{Ereignisse. Kraftwerk.}
Die Arbeit eines Kraftwerkes wird durch drei unabhängig voneinander arbeitende
Kontrollsysteme überwacht, die jedoch auch einer gewissen Störanfälligkeit
unterliegen. Es bezeichne $S_i$ das Ereignis, dass das $i$-te System
störungsfrei arbeitet $(i=1,2,3)$.
\begin{enumerate}
    \item Finden Sie einen geeigneten Wahrscheinlichkeitsraum, der diese
        Zufallssituation beschreibt. Geben Sie die Ergebnismenge $\Omega$
        explicit an. Ist die Ergebnismenge eindeutig bestimmt?
    \item Drücken Sie folgende Ereignisse mit Hilfe der Ereignisse $S_1$, $S_2$
        und $S_3$ aus:
        \begin{itemize}
            \item[$A$:] Alle drei Systeme arbeiten störungsfrei.
            \item[$B$:] Kein System arbeitet störungsfrei.
            \item[$C$:] Mindestens ein System arbeitet störungsfrei.
            \item[$D$:] Genau ein System arbeitet störungsfrei.
            \item[$E$:] Höchstens zwei Systeme sind gestört.
        \end{itemize}\label{ereignisse-kraftwerk-1}
    \item Welche der unter \ref{ereignisse-kraftwerk-1} genannten Ereignisse
        sind Elementarereignisse?
    \item Aus wie vielen Elementen bestehen die Ereignisse $D$ und $C$?
\end{enumerate}

\paragraph*{Lösung.} Die Ergebnismenge $\Omega$ kann als $\Omega= \left\{ (ijk)
: i,j,k\in \left\{ 0,1 \right\} \right\}$ gewählt werden. Diese Wahl ist aber nicht 
eindeutig. Die Ereignisse $A$ und $B$ sind in diesem Fall Elementarereignisse. 


\paragraph{Ereignisse. Aktienmarkt.}
Beim Monatsvergleich zweier Technologieaktien wird für jede Aktie
festgestellt, ob es zu einem Gewinn von mindestens 3\% kam, ob sich ein Verlust
um mehr als 3\% ergab oder ob sich die jeweilige Aktie innerhalb der 6\%-Spanne
bewegte.
\begin{enumerate}
    \item Finden Sie einen geeigneten Wahrscheinlichkeitsraum, der diese
        Zufallssituation beschreibt. Geben Sie die Ergebnismenge $\Omega$
        explicit an.
    \item Stellen Sie folgende Ereignisse mit Hilfe der Elementarereignisse
        dar:
        \begin{itemize}
            \item[$A$:] Beide Aktien erzielten einen Kursgewinn von mindestens $3\%$.
            \item[$B$:] Die Kurse der beiden Aktien lagen innerhalb der
                $6\%$-Spanne. %Keine der beiden Aktien veränderte sich signifikant.
            \item[$C$:] Der Kurs von höchstens einer der beiden Aktien
                verschlechterte sich um mehr als $3\%$. 
            \item[$D$:] Der Kurs von mindestens einer der beiden Aktien
                verschlechterte sich um mehr als $3\%$. 
        \end{itemize}
    \item Welche Bedeutung haben die Ereignisse\\
        $E_1=A\cup C$, $E_2=A\cup D$,
        $E_3=A\cap C$, $E_4=A\cap\overline{C}$, $E_5= \overline{A\cap D}$ ?
\end{enumerate}



\paragraph{Ereignisse. Fertigungsstraße.}
Eine Fertigungsstraße bestehe  aus einer Maschine vom Typ I, vier Maschinen vom
Typ II und zwei Maschinen vom Typ III. Wir bezeichnen mit $A$, $B_k$ bzw.\
$C_j$ ($k=1,2,3,4$; $j=1,2$) die Ereignisse, dass die Maschine vom Typ I
bzw.~die $k$-te Maschine vom Typ~II bzw.~die $j$-te Maschine vom Typ III intakt
ist. Die Fertigungsstraße sei arbeitsfähig, wenn mindestens eine Maschine von
jedem Maschinentyp intakt ist. Dieses Ereignis werde mit $D$ bezeichnet.

Beschreiben Sie die Ereignisse $D$ und $\overline{D}$ mit Hilfe der Ereignisse
$A$, $B_k$, $C_j$.


\paragraph{Ereignisse. Kosten.}
Drei Betriebsteile werden auf Einhaltung eines bestimmten
Kostenfaktors überprüft. Das Ereignis $A$ liege vor, wenn mindestens ein
Betriebsteil nicht den geforderten Kostenfaktor einhält, das Ereignis $B$ liege
vor, wenn alle drei Betriebsteile den geforderten Kostenfaktor einhalten.

Was bedeuten dann die Ereignisse $A\cup B$ und $A\cap B$ ?


\paragraph{Ereignisse. Würfel und Münze.}
Ein Experiment bestehe aus dem Werfen eines fairen Würfels und einer fairen Münze.
\begin{enumerate}
    \item Geben Sie eine geeignete Ergebnismenge $\Omega$ an.
    \item Zeigt die Münze Wappen, so wird die doppelte Augenzahl des Würfels
        notiert, bei Zahl nur die einfache. Wie groß ist die
        Wahrscheinlichkeit, dass eine gerade Zahl notiert wird?
\end{enumerate}


\paragraph{Ereignisse. Zerlegung des Würfels.}
Ein Würfel, dessen Seitenflächen gleichartig gefärbt sind, werde in 1000
kleine Würfel einheitlicher Größe zerlegt.

Wie groß ist die Wahrscheinlichkeit dafür, dass ein zufällig ausgewählter
Würfel auf mindestens einer Seite gefärbt ist?


\paragraph{Ereignisse. Elementare Wahrscheinlichkeiten.}
Für die Ereignisse $A$ und $B$ seien folgende Wahrscheinlichkeiten bekannt:
$P(A)=0.25$, $P(B)=0.45$, $P(A\cup B)=0.5$. Berechnen Sie die
Wahrscheinlichkeiten:
\begin{enumerate}
    \item $P(A\cap\overline{B})$,
    \item $P(\overline{A}\cap\overline{B})$ und
    \item $P\left((A\cap\overline{B})\cup(\overline{A}\cap B)\right)$.
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item \begin{align*}
            P(A \cup B) &= P( (A \cap \bar B) \cup B ) = 
            P( A \cap \bar B  ) + P(B) \\
            P(A \cup \bar B) &= P(A \cup B) - P(B) = 0.05.
        \end{align*}
    \item \begin{align*}
            P(\bar A \cap \bar B) &= P(\overline{A \cup B}) = 0.5.
        \end{align*}
    \item Die symmetrische Differenz $A \Delta B = (A \setminus B) \cup (B \setminus A)$
        ist eine disjunkte Vereinigung, und daher
        \begin{align*}
            P(A \Delta B) &= P(A \cap \bar B) + P(\bar A \cap B).
        \end{align*}
        Die vorherigen Überlegungen liefern
        \begin{align*}
            P( A \cap \bar B) &= P(A \setminus B) = P(A \cup B) - P(B) = 0.05 \\
            P( \bar A \cap B) &= P(B \setminus A) = P(A \cup B) - P(A) = 0.25.
        \end{align*}
        Insgesamt gilt also $P(A \Delta B) = 0.3$.
\end{enumerate}


\section{Wahrscheinlichkeitsmaße.}

\paragraph{Wahrscheinlichkeitsmaße. Monotonie.} Seien ein Wahrscheinlichkeitsraum \linebreak
$(\Omega, \fA, P)$ und die Ereignisse $A,B \in \fA$ gegeben. Zeigen Sie
\begin{equation*}
A \subseteq B \quad  \impl \quad P(A) \leq P(B).
\end{equation*}

\paragraph*{Lösung.} $P(B) = P(A \cup (B \setminus A))= P(A) + P(B\setminus A) \geq P(A)$. 


\paragraph{Wahrscheinlichkeitsmaße. Subaditivität.} Seien ein
Wahrscheinlichkeitsraum $(\Omega, \fA, P)$ und die Ereignisse $A_1, A_2, \dots
\in \fA$ gegeben. 
\begin{enumerate}
    \item Zeigen Sie, dass für alle  $n\in \bN$ 
        \begin{equation*}
            P \left(  \bigcup \limits_{i=1}^n A_i \right) \leq \sum_{i=1}^{n} P\left( A_i \right)
        \end{equation*}
        gilt. 

    \item Zeigen Sie, dass sogar gilt 
        \begin{equation*}
            P \left(  \bigcup \limits_{i=1}^\infty A_i \right) \leq \sum_{i=1}^{\infty} P\left( A_i \right).
        \end{equation*}
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item Betrachte folgende Darstellung der Vereinigung als Summe disjunkter
        Elemente von $\fA$.
        \begin{align*}
            \bigcup_{i=1}^n A_i &= A_1 \cup (\bar A_1 \cap A_2) \cup 
            (\bar A_1 \cap \bar A_2 \cap A_3) \cup \dots \cup
            (\bar A_1 \cap \bar A_2 \cap \dots \cap \bar A_{n-1} \cap A_n).
        \end{align*}

    \item Genau das gleiche Argument funktioniert für $n=\infty$. Das ist 
        die Konsequenz der $\sigma$-Additivität der Wahrscheinlichkeitsfunktion $P$. 
\end{enumerate}


\paragraph{Wahrscheinlichkeitsmaße. Zerlegung der Vereinigung.} Seien ein
Wahrscheinlichkeitsraum $\left( \Omega, \fA, P \right)$ und die Ereignisse
$A,B,C\in \fA$ sowie $A_1, A_2, \dots, A_n \in \fA$ gegeben.
Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item \begin{equation*}
            P \left( A \cup B \cup C \right) = 
            P(A)+P(B)+P(C) - P(A\cap B) - P(A\cap C) - P(B\cap C) + P(A\cap B \cap C). 
        \end{equation*}

    \item
        \begin{align*}
            P\left( \bigcup \limits_{i=1}^{n} A_i \right) =&
            \sum_{i=1}^{n} P(A_i) - \sum_{i<j} P(A_i \cap A_j) + \\
            & \sum_{i<j<k} P(A_i \cap A_j \cap A_k) - \dots + 
            (-1)^{n+1} P\left( A_1 \cap \dots \cap A_n \right). 
        \end{align*}
\end{enumerate}

\paragraph*{Lösung.} 
\begin{enumerate}
    \item Hier genügt eine Überlegung mit Hilfe der Venn-Diagrame. 

    \item Die Menge $A_1 \cup \dots \cup A_n$ ist Vereinigung der Mengen der
        Form $A_{i_1} \cap \dots \cap A_{i_k} \cap \bar A_{i_{k+1}} \cap
        \dots \cap \bar A_{i_{n}}$, $k\geq 1$, wobei $i_1,\dots, i_n$ eine
        Umnumerierung von $1, \dots, n$ ist. Nun kommt solche Menge in der
        ersten Summe als Teilmenge von $A_i$ genau $\binom{k}{1}
        $ vor. In der zweiten Summe wird die Menge $\binom{k}{2}$ mal abgezogen. 
        Nachdem $\sum_{i=0}^{k} (-1)^{i} \binom{k}{i} = 0$, erhalten wir 
        $\sum_{i=1}^{k} (-1)^{i+1} \binom{k}{i}=1$.
\end{enumerate}


\paragraph{Arithmetik der Wahrscheinlichkeitsmaße.} Seien ein
Wahrscheinlichkeitsraum $\left( \Omega, \fA, P \right)$ und die Ereignisse $A,
B \in \fA$ gegeben. 
\begin{enumerate}
    \item Angenommen $P(\bar A)=\frac{1}{3}$, $P(A\cap B)=\frac{1}{4}$ und
        $P(A \cup B)=\frac{2}{3}$. Berechnen Sie $P(\bar B)$, $P(A \cap \bar B)$
        und $P( B \cap \bar A )$.
        
    \item Angenommen $P(A \cup B) = \frac{1}{2}$, $P(A \cap B) = \frac{1}{4}$
        und $P(A \cap \bar B) = P(B \cap \bar A)$. Berechnen Sie $P(A)$ und
        $P(A \cap \bar B)$. 
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item $B \subset A$.

    \item $P(A) = \frac{3}{8}$ und $P(A \setminus B) = \frac{1}{8}$.
\end{enumerate}



\section{Kombinatorik.}


\paragraph{Kombinatorik. Passwörter.} Wir betrachten die Menge
$\cA = \left\{ a, b, \dots, z, 0, \dots, 9 \right\}$ als gegebenen Zeichensatz.
\begin{enumerate}
    \item Wie viele voneinander verschiedene Passwörter der Länge 8 können
        aus $\cA$ gebildet werden?
    \item Betrachten wir nun die Passwörter aus dem obigen Zeichensatz, die an der
        letzen Stelle eine Ziffer aufweisen und sonst aus lauter Buchstaben bestehen.
        Wie viele solche Passwörter gibt es?
    \item Vergleichen Sie die Größenordnungen der Mächtigkeiten der obigen
        Passwortmengen. 
\end{enumerate}

\paragraph*{Lösung.} 
\begin{enumerate}
    \item Die Menge $\cA$ enthält $36$ Elemente. Nachdem in einem Passwort die
        Reihenfolge der Zeichen wichtig ist und die Zeichen mehrmals vorkommen
        können, ist die Anzahl der Passwörter gegeben durch die Anzahl der
        Variationen mit Zurücklegen, also durch
        \begin{equation}
            36^{8} = 2.821.109.907.456 \approx 10^{12.45}.
        \end{equation}
    \item Es gibt $26^7$ Passwörter mit Länge $7$, die nur aus Buchstaben bestehen. 
        Wenn wir an jedem solchen Passwort eine Ziffer am Ende hinzufügen, 
        erhalten wir $10\cdot 26^7$ Möglichkeiten. 
        \begin{equation}
            10\cdot 26^7 = 80.318.101.760 \approx 10^{10.9}.
        \end{equation}
    \item Die Mächtigkeiten der obigen Mengen sind also fast zwei Größenordnungen 
        auseinander.
\end{enumerate}



\paragraph{Kombinatorik. PINs und runs.} In einer Folge $\left( a_1, a_2, a_3,
\dots \right)$ wird eine Teilfolge $(a, a, \dots, a)$, die aus $n$-facher
Wiederholung eines Elements $a$ gebildet wird, als ein $n$-run bezeichnet.
Zum Beispiel hat die Folge $(0, 0, 1, 1, 1, 0)$ acht runs, nämlich einen $2$-run
$(0,0)$ und einen $3$-run $(1,1,1)$, sowie sechs $1$-runs.

Betrachten wir nun die Menge der 4-stelligen PINs, die aus den Ziffern $\left\{
0,1, \dots, 9 \right\}$ gebildet werden können. 
\begin{enumerate}
    \item Wie viele solche PINs gibt es?
    \item Wie viele PINs gibt es, die keine $2$-runs, $3$-runs, sowie keine
        $4$-runs enthalten?
    \item Vergleichen Sie die Größenordnungen der Mächtigkeiten der obigen
        PIN-Mengen. 
\end{enumerate}
\textbf{Zusatz:} Führen Sie die obige Berechnung für die $5$-stelligen PINs durch.

\paragraph*{Lösung.}
\begin{enumerate}
    \item Es gibt $10^4$ solche PINs.
    \item Um die Anzahl der PINs ohne runs zu erhalten, zählen wir alle
        möglichen PINs mit runs auf. Erste Spalte gibt die Form des PINs,
        zweite die Anzahl solcher PINs und dritte die Anzahl der symmetrischen
        Fälle.
\begin{lstlisting}
----    10          1
---*    10 9        2
--==    10 9        1
--**    10 9 8      2
*--*    10 9 9      1
\end{lstlisting}
        Das sind insgesamt $2530$ Fälle. Es gibt also $7470$ PINs ohne runs.
    \item Wenn wir PINs mit runs ausschliessen, reduziert sich unser pool
        möglicher PINs um ein Viertel. 
\end{enumerate}


\paragraph{Kombinatorik. Zwei Würfel.}
Wie groß ist die Wahrscheinlichkeit dafür, beim Werfen von zwei Würfeln eine
Augensumme zu erzielen, die größer oder gleich 10 ist?

\paragraph*{Lösung} Wir lösen die Aufgabe einmal unter der Annahme, dass die Würfel
unterschieden werden können und einmal ohne diese Annahme. 
\begin{enumerate}
    \item Wenn wir zwei wohlunterscheidbare Würfel werfen, ist die
        $36$-elementige Ergebnismenge gegeben durch $\Omega = \left\{ (i,j) :
        i,j \in \left\{ 1, \dots, 6 \right\} \right\}$. Die günstigen Fälle
        sind 
        \begin{equation}
            (4,6), (5,5), (5,6), (6,4), (6,5), (6,5).
        \end{equation}
        Nachdem es sich hier um ein Laplace-Modell handelt, ist die
        Wahrscheinlichkeit, dass die Summe größer als $10$ ist, gleich
        $\frac{6}{36}= \frac{1}{6}$.

    \item In Falle der nichtunterscheidbaren Würfel ist das kein
        Laplace-Experiment mehr. Deswegen muss geeignetes
        Wahrscheinlichkeitsmaß $P$ betrachtet werden. 
\end{enumerate}


\paragraph{Kombinatorik. Würfel. 4 vs.~24.}
Was ist wahrscheinlicher:
\begin{enumerate}
    \item Beim Werfen von vier Würfeln auf wenigstens einem eine Sechs zu
        erzielen, oder
    \item bei 24 Würfen von zwei Würfeln wenigstens einmal zwei Sechsen zu
        erhalten?
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item Das Betrachen des komplementären Ereignisses liefert
        \begin{equation}
            1 - \frac{5^4}{6^4} = 1.
            \label{}
        \end{equation}
    \item Ein äquivalentes Model ist, $24$ mal einen $36$-Würfel werfen. Die
        Wahrscheinlichkeit wenigstens ein mal die Zahl $36$ zu erhalten,
        berechnen wir indem wir das komplementäre Ereignis betrachten:
        \begin{equation} 
            1 - \frac{35^{24}}{36^{24}}.  
        \end{equation}
    \item Was ist größer? $\frac{5^4}{6^4} = 0.4822530864$ und
        $\frac{35^{24}}{36^{24}} = 0.5085961239$.
\end{enumerate}




\paragraph{Kombinatorik. Single choice test.} Bei einem single choice test
werden $n$ Fragen gestellt, wobei jeweils genau eine richtige Antwort aus $m$
Möglichkeiten gewählt werden soll. Der Test wird als bestanden angesehen, wenn
mindestens Hälfte der Fragen richtig beantwortet wurden.

Wir testen die Prüfmethode indem wir zufällig jeweils eine Antwort bei jeder Frage
ankreuzen. 
\begin{enumerate}
    \item Finden Sie einen geeigneten Wahrscheinlichkeitsraum, der diese
        Zufallssituation \linebreak beschreibt.
    \item Wie hoch ist die Wahrscheinlichkeit, dass die zufällige Antwortwahl zum 
        Bestehen des Tests führt?
    \item Berechnen Sie die obige Wahrscheinlichkeit explizit für $n=25$ und $m=4$.
\end{enumerate}

\paragraph*{Lösung.} Laplace-Modell. Anzahl der Möglichen: $| \Omega| = m^n$. 
Anzahl der Günstigen:
\begin{equation*}
    \binom{n}{ \lceil\frac{n}{2}\rceil }(m-1)^{n - \lceil\frac{n}{2}\rceil } +
    \binom{n}{   \lceil\frac{n}{2}\rceil +1 }(m-1)^{n -   \lceil\frac{n}{2}\rceil-1   } + \dots +
    \binom{n}{n} (m-1)^{n-n}. 
\end{equation*}
Für den Fall $n=25$ und $m=13$, ist $\lceil \frac{n}{2} \rceil = 13$ und die Anzahl 
der günstigen Fälle gleich 
\begin{equation}
    3794787166756 \approx 10^{12.5}.
\end{equation}
Nachdem Anzahl der möglichen Fälle ist 
\begin{equation}
    m^{n} = 4^{25} = 1125899906842624 \approx 10^{15.05},
\end{equation}
ist die Wahrscheinlichkeit für eine zufällig bestandene Prüfung ungefähr gleich
$0.0033$.




\section{Bedingte Wahrscheinlichkeiten}


\paragraph{Bedingte Wahrscheinlichkeiten. Symmetrie.}
Seien ein Wahrscheinlichkeitsraum $\left( \Omega, \fA, P \right)$ und die
Ereignisse $A,B\in \fA$ mit $P(A)>0$ und $P(B)>0$ gegeben.  Zeigen Sie die
Äquivalenz:
\begin{equation*}
    P(A | B ) > P(A) \quad \iff \quad P(B | A) > P(B).
\end{equation*}

\paragraph*{Lösung.}
\begin{equation*}
    P(A|B)>P(A) \iff \frac{ P(A \cap B) }{ P(B)} > P(A) \iff P(A \cap B) > P(A)P(B).
\end{equation*}


\paragraph{Bedingte Wahrscheinlichkeiten. 3 Würfel.} Wir werfen drei ideale Würfel. 
\begin{enumerate}
    \item Wie groß ist die Wahrscheinlichkeit, dass dabei kein Sechser geworfen wurde? 
    \item Wie groß ist die Wahrscheinlichkeit, dass dabei kein Sechser geworfen wurde, wenn
        bekannt ist, dass drei paarweise verschiedene Zahlen geworfen wurden. 
\end{enumerate}

\paragraph*{Lösung.} Die Menge aller Tripel aus Zahlen $1,\dots,6$ ist die
Ergebnismenge.  Sei $A$ das Ereignis, dass keine Sechser geworfen wurden und
$B$ das Ereignis, dass geworfene Zahlen verschieden sind. Nun ist $P(A) =
\frac{5^3}{6^3}$ und $P(B)= 6\cdot 5\cdot 4$. Mit $P(A \cap B) = 5\cdot 4\cdot
3$ erhalten wir $P(A|B)=\frac{1}{2}$.


\paragraph{Bedingte Wahrscheinlichkeiten. 2 Münzen.} Wir werfen zwei ideale Münzen.
\begin{enumerate}
    \item Die erste Münze zeigt Kopf. Berechnen Sie die Wahrscheinlichkeit,
        dass beide Münzen Kopf zeigen.

    \item Eine der Münzen zeigt Kopf. Berechnen Sie die Wahrscheinlichkeit,
        dass beide Münzen Kopf zeigen.
\end{enumerate}

\paragraph*{Lösung.} Bezeichne mit $K_1$ bzw.\ $K_2$ das Ereignis, dass die erste
bzw.\ die zweite Münze Kopf zeigt. 
\begin{enumerate}
    \item \begin{equation*}
            P( K_1 \cap K_2 | K_1 ) = \frac{ P( K_1 \cap K_2 \cap K_1) }{ P(K_1)} = \frac{1}{2}.
        \end{equation*}

    \item \begin{equation*}
            P( K_1 \cap K_2 | K_1 \cup K_2 ) = \frac{ P( K_1 \cap K_2) }{ P(K_1 \cup K_2) }
            = \frac{1}{3}. 
        \end{equation*}
\end{enumerate}


\paragraph{Bedingte Wahrscheinlichkeit ist ein Wahrscheinlichkeitsmaß.}
Seien ein Wahrscheinlichkeitsraum $(\Omega, \fA, P)$ und ein Ereignis $B\in\fA$
mit $P(B)>0$ gegeben. 
\begin{enumerate}
    \item Zeigen Sie, dass die Funktion $Q: \fA \to \bR, A \mapsto P(A|B)$ ein
        Wahrscheinlichkeitsmaß auf $(\Omega, \fA)$ ist.
    \item Sei $\fA_B = \left\{ A \cap B : A\in \fA \right\}$. Zeigen Sie, dass
        die Funktion $R: \fA_B \to \bR, A \mapsto P(A|B)$ ein Wahrscheinlichkeitsmaß
        auf $\left( B, \fA_B \right)$ ist. 
    \item \textbf{Zusatz:} Zeigen Sie, dass $\fA_B$ alle Eigenschaften eines Ereignisfeldes ($\sigma$-Algebra) aufweist.
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item Es sollen die Axiome von Kologorov gelten: (A1) $0 \leq P(A) \leq 1$
        für alle $A\in \fA$, (A2) $P(\Omega)=1$, (A3) $P\left(
        \cup_{i=1}^{\infty} A_i \right) = \sum_{i=1}^{\infty} P(A_i)$ für
        paarweise disjunkte Mengen $(A_i)_{i\in \bN}$. 

        Nun ist $Q(A) = P( A | B) =  \frac{P(A \cap B)}{ P(B)}$.

    \item Ein Ereignisfeld erfüllt folgende Axiome: (1) $\Omega \in \fA$, (2)
        $A \in \fA \impl \bar A \in \fA$, (3) $A_1, A_2, \dots \in \fA \impl
        \cup_{i\in\bN} A_i \in \fA$. 
\end{enumerate}


\paragraph{Bedingte Wahrscheinlichkeiten. Urne mit Kugeln.} In einer Urne
befinden sich $w$ weiße Kugeln und $s$ schwarze Kugeln. Es wird eine Kugel aus
der Urne gezogen, beiseite gelegt und anschließend eine weitere Kugel gezogen.
Berechnen Sie die Wahrscheinlichkeiten der folgenden Ereignisse:
\begin{enumerate}
    \item Beide Kugeln sind weiß.
    \item Die erste Kugel ist weiß und die zweite Kugel ist schwarz.
\end{enumerate}

\paragraph*{Lösung. } Wir bezeichnen mit $W_1$ das Ereignis, dass eine weiße Kugel als
erste gezogen wird, und mit $W_2$ ein Ereignis, dass eine weiße Kugel als zweite
gezogen wird. Wir sind daran interessiert, die Wahrscheinlichkeit $P(W_1 \cap W_2)$ zu
berechnen. Wir wissen aber $P(W_1 \cap W_2) = P(W_2 | W_1) P(W_1)$ und
$P(W_1) = \frac{w}{w+s}$. Es gilt auch $P(W_2 | W_1) = \frac{w-1}{w-1+s}$.
Insgesamt ist also \begin{equation*}
    P(W_1 \cap W_2) = \frac{w(w-1)}{(w+s)(w-1+s) }.
\end{equation*}

Ähnlich bezeichnen wir mit $S_2$ das Ereignis, dass eine schwarze Kugel als zweite
gezogen wird. Damit ergibt sich
\begin{equation*}
    P(W_1 \cap S_2)= P(S_2 | W_1) P(W_1) = \frac{ w s }{(w+s)(w+s-1)}.
\end{equation*}


\paragraph{Bedingte Wahrscheinlichkeiten. Urnenmodell von Polya.} Eine Urne
enthält $s$ schwarze und $w$ weiße Kugeln. Es wird eine Kugel zufällig gezogen,
ihre Farbe notiert und die Kugel gemeinsam mit $d$ weiteren Kugeln von der
gleichen Farbe in die Urne gelegt. Die Prozedur wird anschließend wiederholt. 
\begin{enumerate}
    \item Wie groß ist die Wahrscheinlichkeit, dass die zweite gezogene Kugel weiß ist?
    \item Wie groß ist die Wahrscheinlichkeit, dass die erste gezogene Kugel
        weiß ist, unter der Bedingung, dass die zweite gezogene Kugel weiß ist?
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item \begin{align*}
            P(W_2) &= P(W_2 | W_1) P(W_1) + P(W_2 | S_1) P(S_1) \\
            &= \frac{w+d}{w+s+d}\ \frac{w}{w+s} + \frac{w}{w+s+d}\ \frac{s}{w+s} = \frac{w}{w+s}.
        \end{align*}

    \item \begin{align*}
            P(W_1 | W_2 ) &= \frac{ P(W_2 | W_1) P(W_1)  }{ P(W_2) } = \frac{w+d}{w+s+d}.
        \end{align*}
        
\end{enumerate}




\paragraph{Bedingte Wahrscheinlichkeiten. Unfaire Münzen.} 
In einem Wurfexperiment werden $n$ Münzen verwendet. Davon sind $k$ Münzen
symmetrisch und zeigen Kopf mit Wahrscheinlichkeit $1/2$. Die restlichen $n-k$
Münzen zeigen Kopf mit Wahrscheinlichkeit $1/3$. 
\begin{enumerate}
    \item Es wurde zufällig eine Münze gewählt und geworfen: Kopf. Wie groß ist 
        die Wahrscheinlichkeit, dass eine asymmetrische Münze gewählt wurde?
    \item Eine zufällig gewählte Münze wird zweimal geworfen und in beiden 
        Würfen fällt Kopf. Wie groß ist die Wahrscheinlichkeit, dass eine asymmetrische
        Münze gewählt wurde. 
\end{enumerate}

\paragraph*{Lösung. } Wir bezeichnen mit $S$ bzw. $A$ das Ereignis, dass eine
symmetrische bzw. asymmetrische Münze gewählt wurde. Es gilt $P(S) = \frac{k}{n}$
und $P(A)=\frac{n-k}{n}$. Sei $K$ das Ereignis, dass mit einer festen zufällig
gewählten Münze Kopf geworfen wird. Wir suchen $P(A | K)$. Es gilt aber nach dem
Satz von Bayes
\begin{equation*}
    P( A | K) = \frac{ P( K | A) P(A) }{ P(K | A) P(A) + P(K | S) P(S) }
    = \frac{\frac{1}{3} \frac{n-k}{n} }{ \frac{n-k}{3 n} + \frac{1}{2} \frac{k}{n}} 
    = \frac{n-k}{2n + k}.
\end{equation*}

Wenn zwei Münzen geworfen werden, müssen wir das Ereignis $K$ entsprechend erweitern und 
in die obige Berechnung nochmal einsetzen. 



\paragraph{Bedingte Wahrscheinlichkeiten. Urne mit Loch.} In einer Urne befinden
sich $s$ schwarze und $w$ weiße Kugeln. Während die Urne transportiert wird,
gehen $d$ Kugeln verloren. Es wird eine Kugel zufällig aus der Urne gezogen.
\begin{enumerate}
    \item Angenommen $d=1$. Wie groß ist die Wahrscheinlichkeit, dass die
        gezogene Kugel weiß ist?
    \item Nun ist $d<s$ und $d<w$. Wie groß ist die Wahrscheinlichkeit, dass die
        gezogene Kugel weiß ist?
    \item Wie groß ist die Wahrscheinlichkeit, dass die gezogene Kugel weiß
        ist, wenn nur noch $d<w+s$ gefordert wird?
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item Im Fall $d=1$ können wir annehmen, dass eine Kugel entnommen wurde und
        den Satz von der vollständigen Wahrscheinlichkeit verwenden. 
        \begin{equation*}
            P(W_2) = P(W_2 | W_1 ) P(W_2 ) + P(W_2 | S_1) P(S_1).
        \end{equation*}

    \item Alle Arten auf die $i$ weiße und $k-i$ schwarze Kugeln für $i\in
        {0,\dots, k}$ verloren gehen können, sind gleichwahrscheinlich.
        Bezeichnen wir dieses Ereignis als $Z_i$.  Daher haben wir
        \begin{align*}
            P(W) &= \sum_{i=0}^{d} P(W | Z_i ) P(Z_i) \\
            &= \sum_{i=0}^{d} \frac{ w -i}{w+s-i} P(Z_i).
        \end{align*}
        Nun ist 
        \begin{align*}
            P(Z_i) &= \binom{k}{i} P( W_1\cap \dots\cap W_i\cap S_{i+1}\cap \dots\cap S_k ) \\
            &= \binom{k}{i} P(W_1) P(W_2 | W_1) \dots P( S_k | W_1\cap \dots\cap S_{i+1}\cap \dots\cap S_k) \\
            &= \binom{k}{i} \frac{w}{w+s}\frac{w-1}{w+s-1}\dots \frac{w-i+1}{s+w-i-1}
            \frac{s}{s+w-i}\dots\frac{s-(k-i)+1}{s+w -k+1} \\
            &= \binom{k}{i} \frac{ (w)_{i} (s)_{k-i} }{ (w+s)_{k}}.
        \end{align*}

\end{enumerate}




\paragraph{Bedingte Wahrscheinlichkeiten. Urnen mit unbekanntem Inhalt.} Es
sind zwei Urnen mit Kugeln gegeben. Eine der Urnen enthält $k_1$ weiße und
$n_1$ schwarze Kugeln und die andere enthält $k_2$ weiße und $n_2$ schwarze
Kugeln. Die Urnen sind äußerlich nicht unterscheidbar.
\begin{enumerate}
    \item Geben Sie einen Ziehungsprozedur an, die die Wahrscheinlichkeit, dass
        zwei weiße Kugeln gezogen werden unter allen Ziehungsprozeduren maximiert.

    \item Seien nun $k_1=3, n_1=7, k_2=2$ und $n_2=8$. Wie groß ist die
        Wahrscheinlichkeit, dass unter Verwendung der von Ihnen gefundenen
        Prozedur zwei weiße Kugeln gezogen werden?
\end{enumerate}

\paragraph*{Lösung.}
Nachdem die Urnen äußerlich nicht unterscheidbar sind, können wir eine
Urne nur zufällig und nur mit Wahrscheinlichkeit $1/2$ wählen. Wir wählen 
also eine Urne zufällig und ziehen eine Kugel. Es kann nur auf folgende Weisen
weiterfahren werden:

\begin{enumerate}
    \item Für die zweite Ziehung wählen wir die Urnen zufällig.
    \item Zweite Kugel ziehen wir aus der anderen Urne. 
    \item Zweite Kugel ziehen wir aus derselben Urne.
    \item Zweite Kugel ziehen wir aus derselben Urne falls die erste weiß war.
    \item Zweite Kugel ziehen wir aus anderen Urne falls die erste weiß war.
\end{enumerate}
Bezeichnen wir mit $U_{ij}$, $i,j\in \left\{ 1,2 \right\}$ das Ereignis, dass
die $j$-te Urne bei der $i$-ten Ziehung gewählt wurde. Nun berechnen wir die 
Wahrscheinlichkeit, dass zwei weiße Kugel gezogen werden, also $P(W_1 \cap W_2)$.
\begin{align*}
    P_1 = P(W_1 \cap W_2) = 
\end{align*}


\paragraph{Bedingte Wahrscheinlichkeiten. Versicherungsunternehmen.} Ein
Versicherungsunternehmen versichert $n$ Autofahrer, davon $f$ Frauen und $m$
Männer. Die Wahrscheinlichkeit, dass innerhalb eines Jahres ein zufällig
gewählter männlicher Autofahrer einen Unfall hat, ist $\alpha$. Entsprechende
Wahrscheinlichkeit für einen weiblichen Autofahrer ist $\beta$. Die Unfälle in
verschiedenen Jahren sind unabhängig. Ein Autofahrer wird zufällig gewählt. 
Berechnen Sie folgende Wahrscheinlichkeiten:
\begin{enumerate}
    \item Wie groß ist die Wahrscheinlichkeit, dass der gewählte Autofahrer 
        innerhalb des nächsten Jahres einen Unfall hat?

    \item Wie groß ist die Wahrscheinlichkeit, dass der gewählte Autofahrer
        Unfälle in zwei aufeinanderfolgenden Jahren hat?

    \item Der gewählte Autofahrer hat im letzten Jahr einen Unfall gehabt. Wie
        groß ist die Wahrscheinlichkeit, dass der Autofahrer männlich ist?
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item $\frac{\alpha m + \beta f}{n}$.
    \item $\frac{\alpha^2 m + \beta^2 f}{n}$.
    \item $\frac{\alpha m}{\alpha m + \beta f}$.
\end{enumerate}



\paragraph{Bedingte Wahrscheinlichkeiten. Urnenmodell von P\'olya. II.} Eine Urne
enthält $s$ schwarze und $w$ weiße Kugeln. Es wird eine Kugel zufällig gezogen,
ihre Farbe notiert und die Kugel gemeinsam mit $d$ weiteren Kugeln von der
gleichen Farbe in die Urne gelegt. Die Prozedur wird anschließend wiederholt.
\begin{enumerate}
    \item Berechnen Sie die Wahrscheinlichkeit $p_{k,n}$, dass in einer Sequenz von $n$
        Ziehungen $k$ schwarze Kugeln gezogen werden.
\end{enumerate}

\paragraph*{Lösung.}
%    \item Bezeichne mit $S_n$ das Ereignis, dass bei der $n$-ten Ziehung
%        schwarze Kugel gezogen wird. Zeigen Sie, dass $P(S_1)=P(S_n)$ für alle
%        $n\in\bN$ gilt.
\begin{enumerate}
    \item Es gibt $\binom{k}{n}$ mögliche Ziehungen von $n$ Kugeln, die $k$
        schwarze Kugeln beinhalten. Alle diese Ziehungen sind
        gleichwahrscheinlich, denn eine Ziehung $(I_1, \dots, I_n)$ kann in
        eine andere durch einfache Vertauschungen der Form $(I_1, \dots,
        I_{k}, I_{k+1}, \dots, I_n) \to \left( I_1, \dots, I_{k+1}, I_{k},
        \dots, I_n \right)$ überführt werden. Diese Transformationen lassen
        die Wahrscheinlichkeiten der Ziehungen unverändert. Man kann noch 
        genauer Argumentieren und zeigen
        \begin{equation*}
            P( I_1, \dots, S_k, W_{k+1}, \dots, I_n) = 
            P( I_1, \dots, W_{k}, S_{k+1}, \dots, I_n).
        \end{equation*}
        Mit Hilfe der Multiplikationsregel erhalten wir
        \begin{align*}
            P( I_1, \dots, S_k, W_{k+1}, \dots, I_n) = \\
            P(I_1)\dots P(S_k | I_{k-1}\cap\dots\cap I_1) 
            P( W_{k+1} | S_k\cap I_{k-1}\dots) \dots P(I_n | I_{n-1}\cap \dots)
        \end{align*}
        Es sind nur die Wahrscheinlichkeiten in der Mitte des obigen Produkts
        interessant, da die übrigen von der Ersetzung $(S_k\cap W_{k+1} ) \to
        (W_k \cap S_{k+1})$ nicht beeinflusst werden. Wir definieren 
        \begin{equation*}
            Q(A) = P(A | I_{k-1} \cap \dots\cap I_1)
        \end{equation*}
        und zeigen
        \begin{equation*}
            Q( S_k ) Q(W_{k+1} | S_k) = Q(W_k) Q( S_{k+1} | W_k)
        \end{equation*}
        indem wir die konkrete Zahlen einsetzen.

        Demzufolge können wir Ziehungen betrachten in denen zuerst $k$
        schwarze Kugeln und dann die $n-k$ weiße Kugeln gezogen werden.
        Wir verwenden die Multiplikationsregel:
        \begin{align*}
            P( S_1 \cap \dots \cap S_k \cap W_{k+1} \cap \dots \cap W_{n} ) = \\
            P(S_1) \dots P(S_k | S_1\cap\dots\cap S_{k-1})\dots
            P(W_{n} | S_1\cap \dots\cap W_{n-1} ) = \\
            \frac{s}{s+w} \frac{s+d}{s+w+d}\dots\frac{s+(k-1)d}{s+w+(k-1)d}
            \frac{w}{s+w+ kd}\dots\frac{w + (n-k-1)d}{ w+s+(n-1)d}
        \end{align*}
        Wir erhalten also insgesamt
        \begin{equation*}
            p_{k,n} = \binom{k}{n} P( S_1 \cap \dots \cap S_k \cap W_{k+1} \cap \dots \cap W_{n} ).
        \end{equation*}
\end{enumerate}


\paragraph{Bedingte bedingte Wahrscheinlichkeiten.} 
Seien ein Wahrscheinlichkeitsraum $(\Omega, \fA, P)$ und die Ereignisse
$A,B,C\in\fA$ mit $P(A)>0$, $P(B)>0$ und $P(C)>0$ gegeben. Wir definieren
$P(A|B|C)= Q(A|B)$ für ein Wahrscheinlichkeitsmaß $Q$ mit $Q(D)=P(D|C) \ \forall
D\in\fA$. Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item $P(A|B|C) = P(A| B \cap C) = P(A|C|B)$. 
    \item $P(A|B|C) = P(A| C)$ falls $C \subset B$.
\end{enumerate}



\section{Unabhängigkeit}

\paragraph{Unabhängigkeit. Disjunkte Ereignisse.} Seien $(\Omega, \fA, P)$ ein
Wahrscheinlichkeitsraum und die Ereignisse $A,B \in \fA$ gegeben. Zeigen Sie
folgende Aussagen:
\begin{enumerate}
    \item Für $A\cap B=\emptyset$ sind die Ereignisse $A$ und $B$ sind genau dann
        unabhängig, wenn $P(A)=0$ oder $P(B)=0$ gilt.

    \item Sind $A$ und $B$ unabhängig und ist $A \cup B = \Omega$, dann gilt entweder
        $P(A)=1$ oder $P(B)=1$. 
\end{enumerate}


\paragraph{Unabhängigkeit. Gleichwahrscheinliche Ereignisse.} Seien $(\Omega,
\fA, P)$ ein Wahrscheinlichkeitsraum und $A_1,\dots, A_n \in\fA, n\in\bN$
vollständig stochastisch unabhängige Ereignisse mit $P(A_i)=p\in[0,1]\ \forall
i\in \left\{ 1,\dots,n \right\}$ gegeben. Berechnen Sie die
Wahrscheinlichkeiten der folgenden Ereignisse:
\begin{enumerate}
    \item Alle Ereignisse $A_1,\dots, A_n$ treten ein.
    \item Keines der Ereignisse $A_1, \dots, A_n$ tritt ein. 
    \item Genau eines der Ereignisse $A_1, \dots, A_n$ tritt ein. Warum 
        ist diese Wahrscheinlichkeit kleiner als $1$?
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item $P(A_1 \cap \dots \cap A_n) = p^n$.
    \item $P(\overline{A_1 \cup \dots \cup A_n}) = (1-p)^n$.
    \item \begin{align*}
            P( (A_1 \cap \bar A_2 \cap \dots \cap \bar A_n)  \cup\dots\cup (\bar A_1 \cap \dots \cap \bar A_{n-1} \cap A_n) &= \\
            \sum_{i=1}^{n} P(\bar A_1 \cap \dots \cap A_i \cap \dots \cap \bar A_n) &= n p (1-p)^{n-1}. 
        \end{align*}
\end{enumerate}



\paragraph{Unabhängigkeit. Münzen.} Eine ideale Münze wird $n$ mal geworfen.
Bezeichnen wir als $K_i, i\in \left\{1,\dots,n\right\}$ das Ereignis, dass
beim $i$-ten Wurf Kopf fällt. 
\begin{enumerate}
    \item Finden Sie einen geeigneten Wahrscheinlichkeitsraum, der diese
        Zufallssituation beschreibt.
    \item Zeigen Sie, dass die Ereignisse $K_1,\dots, K_n$ vollständig 
        stochastisch unabhängig sind.
\end{enumerate}


\paragraph{Unabhängigkeit. Karten.} Aus einem gut gemischten Spielkartensatz
bestehend aus 52 Karten wird zufällig eine Karte gezogen. Bezeichnen wir mit
$A$ das Ereignis, dass ein Ass gezogen wird und mit $K$ das Ereignis, dass eine
Karte mit der Farbe Karo gezogen wird. Zeigen Sie, dass die Ereignisse $A$ und
$K$ unabhängig sind. 


\paragraph{Unabhängigkeit. Abschätzung mit der Exponentialfunktion.} Seien
$(\Omega, \fA, P)$ ein Wahrscheinlichkeitsraum und $A_1, \dots, A_n \in\fA$
vollständig stochastisch unabhängige Ereignisse  gegeben. Zeigen Sie:
\begin{equation*}
    P( \overline{A_1 \cup \dots \cup A_n} ) \leq \exp \left( - \sum_{i=1}^{n} P(A_i) \right).
\end{equation*}

\paragraph*{Lösung.} Benutze $1-x \leq e^{-x}$ und vollständige Induktion.


\section{Elementare Maßtheorie}

\paragraph{Maßtheorie. Borel-messbare Abbildungen.} Sei ein messbarer Raum
$(\Omega, \fA)$ mit $\Omega=\left\{ \omega_1, \omega_2, \omega_3, \omega_4
\right\}$ und der $\sigma$-Algebra $\fA = \left\{ \emptyset, \Omega, \left\{
\omega_1, \omega_2 \right\}, \left\{ \omega_3, \omega_4 \right\} \right\}$
gegeben. Wir betrachten Abbildungen von $\Omega$ nach $\bR$, wobei $\bR$ mit
der Borel'schen $\sigma$-Algebra $\cB(\bR)$ ausgestattet ist.
\begin{enumerate}
    \item Zeigen Sie, dass die Abbildung $f: \Omega \to \bR, \omega \mapsto 5$
        messbar ist.
    \item Zeigen Sie, dass die Abbildung $g: \Omega \to \bR$ mit $g(\omega_i) = i$
        für $i\in \left\{ 1,2,3,4 \right\}$ nicht messbar ist.
    \item Beschreiben Sie nun alle messbaren Abbildungen $h: \Omega \to \bR$. 
\end{enumerate}



\paragraph{Maßtheorie. Dirac-Maß.} Sei ein messbarer Raum $(\bR, \cB(\bR))$
gegeben. Für ein festes $x\in\bR$ definieren wir die Mengenfunktion $\delta_x
: \cB(\bR) \to \R$ als
\begin{equation*}
    \delta_{x} (A) =
        \begin{cases}
            1 & \textrm{ falls } x\in A \\
            0 & \textrm{ sonst. }
        \end{cases}
\end{equation*}
\begin{enumerate}
    \item Zeigen Sie, dass $\delta_x$ für alle $x\in\bR$ ein Maß ist.

    \item Betrachten wir ein $n$-Tupel $(x_1, \dots, x_n)$ mit $x_i>0$ für
        alle $i\in \left\{ 1, \dots, n \right\}$ und eine Mengenfunktion
        $\delta: \cB(\bR) \to \bR, A \mapsto \sum_{i=1}^{n} \delta_{x_i} (A)$.
        Zeigen Sie, dass $\delta$ ein Maß auf $\left( \bR, \cB(\bR)
        \right)$ ist.

    \item Sei nun $(c_1, \dots, c_n)$ ein $n$-Tupel positiver reeller Zahlen
        mit $\sum_{i=1}^{n} c_i = 1$. Zeigen Sie, dass die Mengenfunktion
        $\delta$ ein Wahrscheinlichkeitsmaß auf $\left( \bR, \cB(\bR) \right)$
        ist. 
\end{enumerate}


\section{Einfache Zufallsgrößen}


\paragraph{Maßtheorie. Bildmaß.}
Seien als Ma{\ss}raum ein Wahrscheinlichkeitsraum $\left( \Omega, \fA, P \right)$ und eine
Zufallsgröße $X: \Omega \to \bR$ gegeben, welche als messbare Abbildung zwischen den messbaren R\"aumen $\left( \Omega,\fA\right)$ und  $\left( \bR,\cB(\bR) \right)$ definiert ist.
Dabei bezeichnet $\cB(\bR)$ die Borel'sche $\sigma$-Algebra auf den reellen Zahlen. Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item Die Mengenfunktion $P_{X}: \cB(\bR) \to [0,1], A \mapsto P( X \in A
        )$ ist ein Wahrscheinlichkeitsmaß auf $(\bR, \cB(\bR))$. 

    \item Ist die Ergebnismenge $\Omega = \left\{ \omega_{1},\dots ,\omega_{n} \right\}$ endlich,
        so gibt es $n$-Tupel reeller Zahlen $(x_1,\dots ,x_n)$ und $(p_1,\dots ,p_n)$ mit 
        $p_i>0 \ \forall i\in \left\{ 1,\dots ,n \right\}$ und $\sum_{i=1}^{n} p_i = 1$, sodass 
        \begin{equation*}
            P_X = \sum_{i=1}^{n} p_i \delta_{x_i}.
        \end{equation*}
        Dabei ist $\delta_x : \cB(\bR) \to \R$ für ein festes $x\in\bR$ ein
        Diracmaß mit $$\delta_x(B)=\left\{\begin{array}{lll} 1 & \mbox{falls} & x \in B\\ 0 & \mbox{falls} & x \notin B \end{array}\right., \quad B \in \cB(\bR).$$
        Sind die Tupel
        $(x_1,\dots ,x_n)$ und $(p_1,\dots ,p_n)$ eindeutig durch $X$ bestimmt?
        Welche Bedeutung haben sie?
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item $P_X$ ist wohldefiniert, normiert und $\sigma$-additiv: $P_X \left( \bigcup_{k\geq 0} A_i \right) = P\left( X \in \bigcup_{k\geq 0} A_i \right)= P\left( \bigcup_{k\geq 0} \left\{ X\in A_i \right\} \right) = \sum_{k\geq 0}^{} P_X\left( X \in A_i \right)$.
\end{enumerate}

\paragraph{Maßtheorie. Ereignisfelder und Information.} Die Ergebnismenge
$\Omega = \left\{ (i,j) : i,j \in \left\{ 1,\dots ,6 \right\} \right\}$
gemeinsam mit dem Wahrscheinlichkeitsmaß $P( (i,j) ) = \frac{1}{36}$ beschreiben
einen Wurfexperiment, in dem zwei ideale wohlunterscheidbare Würfel geworfen
werden. 
Für ein Ergebnis $\omega = (i,j) \in \Omega$ definieren wir die Funktionen
\begin{align*}
    X_0(\omega) &= 0 \\
    X_1(\omega) &= i+j, \\
    X_2(\omega) &= i, \\
    X_3(\omega) &= 
    \begin{cases}
        1 & \text{ falls } i+j=6 \\
        0 & \text{ sonst.}
    \end{cases}
\end{align*}
\begin{enumerate}
    \item Finden Sie die kleinsten Ereignisfelder $\fA_{i}$, $i\in \left\{
        0,1,2,3 \right\}$, sodass $X_i$ Zufallsgrößen auf dem Wahrscheinlichkeitsraum
        $(\Omega, \fA_i, P)$ sind. 
    \item Beschreiben Sie den Zusammenhang zwischen den Ereignisfeldern $\fA_i$
        und der Information, die die Zufallsgrößen $X_i$ über die Ergebnisse
        $\omega \in \Omega$ liefern.
\end{enumerate}

\paragraph*{Lösung.} 
\begin{align*}
    \fA_0 &= \left\{ \emptyset, \Omega \right\} \\
    \fA_1 &= \sigma \left\{ \left\{ 11 \right\}, \left\{ 21,12 \right\}, \left\{ 13,22,31
    \right\},\dots , \left\{ 16,25,34,\dots ,61 \right\} \right\} \\
    \fA_2 &= \sigma \left\{ \left\{ 11,12,13,14,15,16 \right\},\dots ,\left\{ 61,62,\dots ,66
    \right\} \right\} \\
    \fA_3 &= \sigma \left\{ \left\{ 15,24,\dots ,51 \right\}, 
    \overline{ \left\{ 15,24,\dots ,51 \right\}  } \right\}.
\end{align*}


\paragraph{Zufallsgrößen. Bernoulli-Verteilung.} Eine Zufallsgröße $X$ mit den
Werten in der Menge $\left\{ 0,1 \right\} \subset \bR$ und der
Wahrscheinlichkeitsfunktion $P(X=1)= p$, $P(X=0)=1-p$ mit $p\in(0,1)$ heißt
Bernoulli-verteilt mit dem Parameter $p$. 
\begin{enumerate}
    \item Geben Sie die Verteilungsfunktion $F: \bR\to [0,1], x\mapsto F(x) =
        P(X<x)$ von $X$ an.

    \item Berechnen Sie den Erwartungswert $\bfE X$ und die Varianz $\bfD^2X$ von $X$. 

    \item Berechnen Sie den Erwartungswert $\bfE Y$ und die Varianz $\bfD^2Y$ der Zufallsgröße \linebreak
        $Y = \exp\left( X \right)$.

    \item Berechnen Sie die $k$-ten Momente $m_k=\bfE X^k$ und die $k$-ten zentralen Momente $\mu_k=\bfE (X-\bfE X)^k$ von 
        $X$ für alle $k\in \bN$. 
\end{enumerate}


\paragraph{Zufallsgrößen. Diskrete Gleichverteilung.} Eine Zufallsgröße heißt
diskret gleichverteilt auf $\left\{ 1,\dots ,n \right\}$ falls
$P(X=i)=\frac{1}{n}$ für alle $i\in\left\{ 1,\dots ,n \right\}$.
\begin{enumerate}
    \item Geben Sie die Verteilungsfunktion $F: \bR\to [0,1], x\mapsto F(x) =
        P(X<x)$ von $X$ an.

    \item Berechnen Sie den Erwartungswert und die Varianz von $X$. 

    \item Berechnen Sie den Erwartungswert und die Varianz der Zufallsgröße
        $Y = \log\left( X \right)$.

    \item Berechnen Sie das $k$-te Moment und das $k$-te zentrale Moment von 
        $X$ für alle $k\in \bN$. 
\end{enumerate}

\paragraph*{Lösung.}
$\bfE X = \frac{n+1}{2}$, $\sum_{k=1}^{n} k^2 = \frac{n(n+1)(2n+1)}{6}$,
$\bfE X^2 = \frac{(n+1)(2n+1)}{6}$, $\bfD^2 X = \bfE X^2 - \left( \bfE X \right)^2$.

$\bfE \log X = \frac{1}{n} \log n!$.

Die Berechnung der $k$-ten Momente führt zu der Formel von Faulhaber: \url{https://en.wikipedia.org/wiki/Faulhaber\%27s\_formula}


\paragraph{Zufallsgrößen. Funktionen der Augenzahl.} In einem Wurfexperiment
werden zwei ideale wohlunterscheidbare Würfel geworfen. Die Zufallsgrößen
$X_i$ geben die geworfene Augenzahl des $i$-ten Würfels an.
\begin{enumerate}
    \item Berechnen Sie die Wahrscheinlichkeiten $\;P(X_1-X_2 > 2)$,\; $P(X_1/X_2 > 1)$
        und \linebreak $P(|X_1-X_2| \leq 1)$. 
    \item Berechnen Sie die Verteilungsfunktion $F$ der Zufallsgröße $Y = X_1 + X_2$. 
\end{enumerate}

\paragraph*{Lösung.} $P(X_1-X_2 > 2)=\frac{6}{36}$, $P(X_1/X_2>1)=\frac{15}{36}$, 
$P(|X_1-X_2| \leq 1) =\frac{6+5+5}{36}$. 


\paragraph{Erwartungswert. Einfache Eigenschaften.} Sei $X$ eine diskrete
Zufallsgröße. Beweisen Sie folgende Aussagen.
\begin{enumerate}
    \item $\bfE \left( X - \bfE X \right)^2 = \bfE X^2 - \left( \bfE X \right)^2$, falls alle diese
        Erwartungswerte existieren.
    \item $\left( \bfE X \right)^2 \leq \bfE X^{2}$, falls beide Erwartungswerte existieren. 
    \item Angenommen $X$ hat Werte in $\bN$. Dann gilt
        \begin{equation*}
            \bfE X = \sum_{n=0}^{\infty} P(X > n). 
        \end{equation*}
\end{enumerate}

\paragraph*{Lösung.} 
\begin{align*}
    \bfE X &= \sum_{n\geq 0} n\, P\left( X=n \right) \\
    &= P(X=1)+P(X=2)+P(X=2)+\dots+ \underbrace{P(X=n)+\dots+P(X=n)}_{n \text{times}} + \dots \\
    &= \sum_{n\geq 0}^{} P(X>n).
\end{align*}



\paragraph{Geometrische Verteilung. Erwartungswerte.} Sei $X$ geometrisch
verteilt mit dem Parameter $p\in (0,1)$. Zeigen Sie folgende Aussagen: 
\begin{enumerate}
    \item \begin{equation*}
            \bfE \left[ \frac{1}{1+X} \right] = \log \left( (1-p)^{\frac{p}{p-1}} \right).
        \end{equation*}

    \item Für alle $n\in \left\{ 2,3, \dots \right\}$ gilt
        \begin{equation*}
            \bfE \left[ X(X-1)\cdot \dots \cdot (X - n+1) \right] = \frac{n!\ p^n}{(1-p)^n}.
        \end{equation*}
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item \begin{align*}
            \log (1+z) &= z - \frac{z^2}{2} + \frac{z^3}{3} - \cdots \\
            E \left( \frac{1}{1+X} \right) &= \frac{p}{1-p} \sum_{k\geq 0} \frac{(1-p)^{k+1}}{k+1}
            = \frac{p}{p-1} \log \left( 1-p \right) = \log (1-p)^{\frac{p}{p-1}}. 
        \end{align*}

    \item MISSING!!!
\end{enumerate}

\paragraph{Geometrische Verteilung. Gedächtnislosigkeit.} Sei $X$ geometrisch
verteilt mit dem Parameter $p$.
\begin{enumerate}
    \item Zeigen Sie, dass 
        \begin{equation*}
            P \left( X > i+j \,|\, X \geq i \right) = P\left( X > j \right) 
        \end{equation*}
        für alle $i,j>0$ gilt.

    \item Finden Sie eine Interpretation für die obige Beziehung.
\end{enumerate}


\paragraph{Binomialverteilung. Eigenschaften.} Sei $X \sim \bfB(n,p)$ 
binomialverteilt mit den Parametern $n$ und $p$. Zeigen Sie folgende Aussagen: 
\begin{enumerate}
    \item Das Maximum der Wahrscheinlichkeitsfunktion von $X$ wird an der Stelle
        \begin{equation*}
            \operatorname{argmax}_{k\in \left\{ 0,\dots ,n \right\}} P(X=k) = \lfloor (n+1)p \rfloor.
        \end{equation*}
        angenommen. Dabei bezeichnet $\lfloor x \rfloor$ für ein $x\in\bR$ die
        größte ganze Zahl, die nicht größer als $x$ ist. 

    \item Zeigen Sie, dass die folgende Formel gilt:
        \begin{equation*}
            P\left( X \text{ ist eine gerade Zahl} \right) = \frac{1}{2}\left( 1 + (1-2p)^n \right). 
        \end{equation*}
\end{enumerate}

\paragraph*{Lösung.} 
\begin{enumerate}
    \item Wir berechnen
        \begin{align*}
            P(X=k)/P(X=k-1) &= \frac{n+1-k}{k} \frac{p}{1-p}.
        \end{align*}
        Dieser Quotient ist größer als $1$ genau dann, wenn 
        \begin{equation*}
            (n+1)p > k. 
        \end{equation*}
    \item Sei $p_n$ die Wahrscheinlichkeit, dass eine
        $\bfB(n,p)$-verteilte Zufallsgröße gerade ist. $p_n$ erfüllt folgende rekursive
        Relation
        \begin{align*}
            p_n &= p_{n-1}(1-2p) + p, &  p_0 &= 1.
        \end{align*}
        Nun ist es einfach zu überprüfen, dass $\frac{1}{2}(1+ (1-2p)^n)$ die Lösung
        dieser Rekursion ist. 
\end{enumerate}


\paragraph{Poisson-Verteilung. Eigenschaften.} Sei $X$
Poisson-verteilt mit der Wahrscheinlichkeitsfunktion $P(X = k) =
\frac{\lambda^k}{k!} e^{-\lambda}$, $k\in \left\{ 0,1,\dots \right\}$ und dem
Parameter $\lambda>0$. Beweisen Sie folgende Aussagen: 
\begin{enumerate}
    \item Die Varianz von $X$ ist 
        \begin{equation*}
            \bfD^2 X = \lambda.
        \end{equation*}
    \item  Das Maximum der Wahrscheinlichkeitsfunktion von $X$ wird an der Stelle
        \begin{equation*}
            \operatorname{argmax}_{k\in \bN} P(X= k) = \lfloor \lambda \rfloor 
        \end{equation*}
        angenommen.

    \item Für $n\in \left\{ 2,3,\dots  \right\}$ gilt 
        $\bfE \left[ X(X-1)\cdot\dots\cdot (X-n+1) \right] = \lambda^n$.
\end{enumerate}

\paragraph*{Lösung.}


\paragraph{Binomialverteilung. Additionstheorem.} 
Sei $b(k, n, p)=P(X=k)$ die Wahrscheinlichkeitsfunktion einer binomialverteilten
Zufallsgröße $X\sim\bfB(n,p)$.
\begin{enumerate}
    \item Zeigen Sie, dass für $n_1>0$ und $n_2>0$ die Gleichung
        \begin{equation*}
            \sum_{i=0}^{k} b(i, n_1, p)\, b( k-i, n_2, p) = b( k, n_1+n_2, p ).
        \end{equation*}
        gilt. 

    \item Geben Sie die wahrscheinlichkeitstheoretische Interpretation der
        obigen Aussage.
\end{enumerate}


\paragraph{Poisson-Verteilung. Additionstheorem.} Seien $X_1\sim
\pi_{\lambda_1}$ und $X_2\sim \pi_{\lambda_2}$ unabhängige Poisson-verteilte
Zufallsgrößen mit den Parametern $\lambda_1>0$ und $\lambda_2>0$. Zeigen Sie,
dass die Zufallsgröße
\begin{equation*}
    Y = X_1 + X_2
\end{equation*}
ebenfalls Poisson-verteilt mit dem Parameter $\lambda=\lambda_1 + \lambda_2$ ist. 


\paragraph{Binomialverteilung. Verteilungsfunktion.} Seien $X\sim\bfB(n,p)$ binomialverteilt
und $B(k, n, p) = P( X <k)$ die Verteilungsfunktion von $X$. Zeigen Sie:
\begin{equation*}
    B(k+1, n, p) = (n-k) \binom{n}{k} \int_{0}^{1-p} t^{n-k-1} (1-t)^k dt.
\end{equation*}

\paragraph*{Lösung.}
\begin{enumerate}
    \item Die Formel kann mit Hilfe der partiellen Integration hergeleitet werden.
        \begin{align*}
            B(k+1, n, p) &= (n-k) \binom{n}{k} \int_{0}^{1-p} t^{n-k+1} (1-t)^{k} dt \\
            &= (n-k)\binom{n}{k} \left[  \frac{1}{n-k}(1-p)^{n-k} p^{k} + 
            \frac{k}{n-k} \int_{0}^{1-p} t^{n-k} (1-t)^{k-1} dt \right] \\
            &= \binom{n}{k} (1-p)^{n-k} p^{k} + 
            (n-(k-1)) \binom{n}{k-1} \int_{0}^{1-p} t^{n-(k-1)-1} (1-t)^{k-1} dt \\
            &= \binom{n}{k} (1-p)^{n-k} p^{k} + B(k, n, p). \\
            B(1,n,p) &= n \binom{n}{0} \int_{0}^{1-p} t^{n-1} dt = (1-p)^n.
        \end{align*}
        Damit erhalten wir
        \begin{equation*}
            B(k+1, n,p) = \sum_{i=0}^{k} \binom{n}{k} p^{i}(1-p)^{n-i}.
        \end{equation*}
\end{enumerate}

\paragraph{Geometrische Verteilung. Zusammenhang mit der Gleichverteilung.}
Die Zufallsgrößen $X$ und $Y$ sind stochastisch unabhängig und geometrisch verteilt mit dem Parameter $p\in(0,1)$.
\begin{enumerate}
    \item Berechnen Sie die Wahrscheinlichkeiten der Ereignisse $X=Y$ und
        $X\geq 2Y$.

    \item Zeigen Sie, dass für $k,n\in\bN$ und $n>k$
        \begin{equation*}
            P \left( X = k \,|\, X+Y = n \right) = \frac{1}{n+1}
        \end{equation*}
        gilt. Mit anderen Worten: Die bedingte Verteilung von $X$ bei gegebenem Wert von 
        $X+Y$ ist die Gleichverteilung.
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item \begin{align*}
            P\left( X = Y \right) &= \sum_{k=0}^{\infty} P\left( X=k \wedge Y=k \right) = \frac{p}{1+p} \\
            P\left( X \geq 2y \right) &= \left( 1-p \right)^{2y} \\
            P\left( X \geq 2Y \right) &= \frac{p}{1- (1-p)^{3}}.
        \end{align*}

    \item Für $X_1 \sim \text{Poiss}(p_1)$ und $X_2\sim\text{Poiss}(p_2)$ erhalten wir
        \begin{align*}
            P\left( X_1 = k \,|\, X_1+X_2 = n \right) &= 
            \frac{(1-p_1)^{k} (1-p_2)^{n-k}  }{ \sum_{i=0}^{n} (1-p_1)^i (1-p_2)^{n-i} }. 
        \end{align*}
\end{enumerate}


\paragraph{Negative Binomialverteilung. Konstruktion.} $X$ ist negativ
binomialverteilt mit den Parametern $n\in\bN$ und $p\in \left( 0,1 \right)$, wenn
für $k\in\bN$
\begin{equation*}
    P(X = k) = \binom{k+n-1}{n-1} p^{n} (1-p)^{k} 
\end{equation*}
gilt. Seien $Z_1,Z_2\dots$ stochastisch unabhängige und geometrisch verteilte Zufallsgrößen mit 
$Z_i \sim \text{Geom}(p)$. Zeigen Sie folgende Aussagen: 
\begin{enumerate}
    \item Die Zufallsgröße $Z_1+Z_2$ ist negativ binomialverteilt mit den
        Parametern $(2,p)$. 
    \item Für $k, n \in\bN$ gilt
        \begin{equation*}
            \binom{k+n}{n} = \sum_{j=0}^{k} \binom{j+n-1}{n-1}.
        \end{equation*}
    \item Die Zufallsgröße $Z_1+\dots+Z_n$ ist negativ binomialverteilt mit den
        Parametern $(n,p)$. 
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item \begin{equation*}
            P\left( Z_1+Z_2 = k \right) = (k+1)p^{2}(1-p)^k.
        \end{equation*}
    \item Beweisen wir zuerst
        \begin{align*}
            \binom{n}{k} &= \binom{n-1}{k-1} + \binom{n-1}{k} \\
            \binom{n+k}{k} &= \binom{n+k}{n}
        \end{align*}
        und den Rest mit Induktion.
    \item \begin{align*}
            P\left( \sum_{i=1}^{n+1} Z_i = k \right) &= 
            \sum_{j=0}^{k} P\left( \sum_{i=1}^{n} Z_i = j \right) P\left( Z_{n+1} =k-j \right) \\
            &= \sum_{j=0}^{k} \binom{j+n-1}{n-1} p^{n}(1-p)^{j} (1-p)^{k-j}p \\
            &= \sum_{j=0}^{k} \binom{j+n-1}{n-1} p^{n+1}(1-p)^{k} = \binom{k+n}{n} p^{n+1}(1-p)^{k}. 
        \end{align*}
\end{enumerate}


\paragraph{Diskrete Gleichverteilung. Ordnungsstatistiken.} Die Zufallsgrößen
$X_1,\dots ,X_m$ seien stochastisch unabhängig und diskret gleichverteilt auf der Menge
$\left\{ 1,\dots ,n \right\}$, mit $n>1$. Wir bezeichnen mit $X_{(1)} = \min
(X_1,\dots ,X_m)$ das Minimum und mit $X_{(m)} = \max \left( X_1,\dots ,X_n
\right)$ das Maximum der Zufallsgrößen $X_1,\dots ,X_m$.
%\begin{enumerate}
%    \item 
Finden Sie die Wahrscheinlichkeitsfunktion der Zufallsgrößen
        $X_{(1)}$ und $X_{(n)}$.
%    \item Finden Sie die Wahrscheinlichkeitsfunktion der Zufallsgröße $X_{(n)}-X_{(1)}$.
%\end{enumerate}

\paragraph*{Lösung.}
\begin{align*}
    P\left( X_{(1)} \leq k \right) &= 1 - \left( \frac{n-k}{n} \right)^m \\
    P\left( X_{(m)} \leq k \right) &= \left( \frac{k}{n} \right)^{m}. 
\end{align*}
Daraus Wahrscheinlichkeitsfunktion berechnen. 


\paragraph{Geometrische Verteilung. Minima und Maxima.} Seien $Y_1,\dots ,Y_m$
stochastisch unabhängig und geometrisch verteilt mit dem Parameter $p\in(0,1)$.  Finden Sie
die Wahrscheinlichkeitsfunktion der Zufallsgrößen $Y_{(1)}$ und $Y_{(n)}$.

\paragraph*{Lösung.}
\begin{align*}
    P\left( Y_{(1)} < k \right) &= 1 - \left( 1-p \right)^{mk} \\
    P\left( Y_{(n)} < k \right) &= \left( \left( 1-p \right)^{k-1} p \right)^n.
\end{align*}


\paragraph{Geometrische Verteilung. Summen.} Seien $X$ und $Y$ unabhängig und geometrisch
verteilt mit den Parametern $p_1, p_2 \in (0,1)$. Zeigen Sie folgende Aussagen: 
\begin{enumerate}
    \item Die Wahrscheinlichkeitsfunktion der Summe $X+Y$ ist gegeben durch
        \begin{equation*}
            P(X+Y = n ) = \frac{p_1 p_2}{p_1 - p_2} \left( (1-p_2)^{n+1}- (1-p_1)^{n+1} \right).  
        \end{equation*}

    \item 
\end{enumerate}


\paragraph{Exponentialverteilung. Minima.}
Seien $X_1, \dots, X_n$ stochastisch unabhängig und exponentialverteilt mit
$X_i\sim \mathbf{Ex}(\lambda_i)$ und $\lambda_i>0$. Zeigen Sie, dass die
Zufallsgröße 
\begin{equation*}
    X_{(1)} = \min \left\{ X_1, \dots, X_n \right\} 
\end{equation*}
exponentialverteilt ist und dabei $X_{(1)} \sim \mathbf{Ex}(\lambda_1+\dots+\lambda_n)$ gilt.


\paragraph{Stetige Gleichverteilung. Momente.}
Sei $X$ stetig gleichverteilt auf dem Intervall $\left[ a,b \right]\subset \R$.
Zeigen Sie folgende Aussagen: 
\begin{enumerate}
    \item Für $k=1,2,\dots$ ist das $k$-te Moment $\bfE X^{k}$ gegeben durch
        \begin{equation*}
            \bfE X^k = \frac{1}{k+1}\left( b^{k}a^0 + \cdots + b^{0}a^{k} \right).
        \end{equation*}
    \item Der Erwartungswert und die Varianz von $X$ sind gegeben durch
        \begin{align*}
            \bfE X    & = \frac{1}{2} \left( b+a \right) & 
            \bfD^2 X  & = \frac{1}{12} \left( b-a \right)^{2}.  
        \end{align*}
\end{enumerate}

\paragraph*{Lösung.} Berechnen wir zunächst die $n$-ten Momente
\begin{align*}
    \bfE X^n &= \int_{\R}^{} x^n \frac{1}{b-a} 1_{(a,b)}(x) dx \\
    &= \frac{1}{b-a} \int_{a}^{b} x^n dx \\
    &= \frac{1}{b-a} \left( \frac{x^{n+1}}{n+1} \right) |_{a}^{b} \\
    &= \frac{1}{b-a}\frac{1}{n+1} \left( b^{n+1}-a^{n+1} \right) \\
    &= \frac{a^n b^{0} +\cdots+ a^0 b^{n}}{n+1}.
\end{align*}
Wir brauchen nur noch $n=1,2$ einzusetzen.
\begin{align*}
    \bfE X &= \frac{a+b}{2} & \bfE X^2 &= \frac{b^2 + ab + a^2}{3} & 
    \bfD^2 X = \frac{(b-a)^2}{12}.
\end{align*}


\paragraph{Cauchy-Verteilung. Gegenbeispiel.}
Sei $X$ Cauchy-verteilt mit der Dichtefunktion
\begin{equation*}
    f(x) = \frac{1}{\pi (1+x^2)}.
\end{equation*}
Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item Die Funktion $f$ ist wirklich eine Dichtefunktion.
    \item Der Erwartungswert $\bfE X$ existiert nicht.
    \item Die Momente $\bfE X^k$ existieren für alle $k=1,2,\dots$ nicht.
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item $\int \frac{dx}{a^2+x^2} = \frac{1}{a} \arctan \frac{x}{a}$.
    \item $\bfE X$ ist nicht definiert im Sinne von Lebesque, weil 
        $\bfE X^{+}=+\infty$ und $\bfE X^{-}=+\infty$ gilt. 
    \item Gleiches folgt für $\bfE X^{n}$ mit einer einfachen Abschätzung.
\end{enumerate}


\paragraph{L\'evy-Verteilung. Dichtefunktion.} Die Dichtefunktion der
L\'evy-Verteilung mit den Parametern $(\mu,\sigma)\in \bR\times\bR$, $\sigma>0$ ist
\begin{equation*}
    f(x) = 
    \begin{cases}
    \sqrt{ \frac{\sigma}{2\pi}} (x-\mu)^{-\frac{3}{2}} \exp\left( -\frac{\sigma}{2(x-\mu)} \right), & x\geq 0 \\
    0, & x<0.
    \end{cases}
\end{equation*}
Sei $X\sim \bfN (0, \frac{1}{\sigma})$ normalverteilt mit $\sigma>0$.  Zeigen
Sie, dass die Zufallsgröße $\frac{1}{X^2}$ L\'evy-verteilt mit den
Parametern $(0,\sigma)$ ist.

\paragraph*{Lösung.} 
Für ein $y>0$ gilt
\begin{align*}
    P \left( \frac{1}{X^{2}} \leq y \right) &= P \left( 1 \leq y X^{2}   \right) \\
    &= P \left( \sqrt{\frac{1}{y}} \leq | X | \right) \\
    &= 2 \left( 1 - P \left( X < \frac{1}{\sqrt{y}} \right) \right) \\
    &= 2 \left( 1 - \Phi\left( \sqrt{ \frac{\sigma}{y}} \right) \right).
\end{align*}
Die Dichte von $\frac{1}{X^2}$ erhalten wir durchs Differenzieren:
\begin{align*}
    \frac{\partial}{\partial y} P \left( \frac{1}{X^2} \leq y \right) &=
    -2 \phi \left( \sqrt{\frac{\sigma}{y}} \right) \sqrt{\sigma} 
    \left( -\frac{1}{2} y^{-\frac{3}{2}} \right) \\
    &= \sqrt{\frac{\sigma}{2 \pi}} \exp \left( - \frac{\sigma}{2 y} \right) y^{-\frac{3}{2}}.
\end{align*}
Daher ist $\frac{1}{X^2} \sim \text{L\'evy}(\sigma)$.


\paragraph{Mellin-Transformation.} Sei $X$ eine positive Zufallsgröße.
Die Mellin-Transformierte $T_{X}(\theta)$ von $X$ ist definiert durch
\begin{equation*}
    T_{X}(\theta) = \bfE X^{\theta}
\end{equation*}
für alle $\theta\in\bR$ für die der obige Erwartungswert existiert. Zeigen Sie
folgende Aussagen:
\begin{enumerate}
    \item Es gilt
        \begin{equation*}
            T_{X}(\theta) = \varphi_{ \log X} (\theta/i)
        \end{equation*}
        wenn alle obigen Ausdrucke wohldefiniert sind.
    \item Sind $X$ und $Y$ positive und stochastisch unabhängige Zufallsgrößen, so gilt
        \begin{equation*}
            T_{XY}(\theta) = T_{X}(\theta) \ T_{Y}(\theta). 
        \end{equation*}
    \item Es gilt
        \begin{equation*}
            T_{b X^{a}}(\theta) = b^{\theta} T_{X} (a\theta)
        \end{equation*}
        für alle $b>0$ und alle $a$ für die $T_{X}(a\theta)$ existiert. 
\end{enumerate}


\paragraph{Log-Normalverteilung. Dichtefunktion.} Ist $X\sim\bfN(\mu,
\sigma^2)$ normalverteilt, dann ist die Zufallsgröße $Y=e^{X}$
log-normalverteilt mit Parametern $\mu$ und $\sigma^{2}$. Zeigen Sie, dass die
Dichtefunktion von $Y$ durch
\begin{equation*}
    f(x) = 
    \begin{cases}
        \frac{1}{\sigma x \sqrt{2\pi}} \exp \left( -\frac{1}{2} \left( \frac{\log x -\mu}{\sigma} \right)^2  \right), & x\geq 0 \\
        0, & x<0
    \end{cases}
\end{equation*} 
gegeben ist.


\paragraph{Log-Normalverteilung. Momente.} Ist $X\sim\bfN(\mu, \sigma^2)$
normalverteilt, dann ist die Zufallsgröße $Y=e^{X}$ log-normalverteilt mit
Parametern $\mu$ und $\sigma^{2}$. Zeigen Sie folgende Aussagen: 
\begin{enumerate}
    \item Für die Mellin-Transformierte $T_{Y}$ von $Y$ gilt
        \begin{equation*}
            T_{Y}(k) =  \bfE \, Y^{k}, \ k=1,2,\dots.
        \end{equation*}

    \item Die Momente $m_k$ von $Y$ sind gegeben durch
        \begin{equation*}
            m_{k} = \bfE \, Y^{k} = \exp \left( \mu k + \frac{ \sigma^2 k^{2}}{2} \right).
        \end{equation*}
\end{enumerate}


\paragraph{Normalverteilung. Abweichungen vom Mittelwert.}
Wir betrachten die normalverteilte Zufallsgr\"o{\ss}e $X\sim\bfN(\mu, \sigma^2)$. 
\begin{enumerate}
    \item Berechnen Sie die Wahrscheinlichkeiten $P(X - \mu \geq n \sigma)$ für
        $n\in \left\{ 1,2,6 \right\}$.
    \item Berechnen Sie die Wahrscheinlichkeiten $P( | X-\mu| \geq n \sigma)$ für 
        $n\in \left\{ 1,2,6 \right\}$.
    \item \"Andern sich die Ergebnisse in (a) und (b), wenn $\ge$ durch $>$ ersetzt w\"urde? 
\end{enumerate}

\paragraph*{Lösung.} $P( |X-\mu| \geq n\sigma ) = 2 - 2P(Z \leq n)$.
Standarisierung und folgende Tabelle liefert die Lösung.
\begin{lstlisting}
In [1]: from scipy.stats import norm
In [5]: [ norm.cdf(i) for i in range(7) ]
Out[5]: 
[0.5,
 0.84134474606854293,
 0.97724986805182079,
 0.9986501019683699,
 0.99996832875816688,
 0.99999971334842808,
 0.9999999990134123]
\end{lstlisting}


\paragraph{Symmetrische Zufallsgrößen.} Eine Zufallsgröße $X$ nennen wir
symmetrisch, wenn die Verteilungen von $X$ und $-X$ gleich sind. 
Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item Hat $X$ eine Dichtefunktion $f$, so ist $X$ symmetrisch genau
        dann, wenn $f(x)= f(-x)$ für alle $x\in\bR$ gilt. 
    \item Die charakteristische Funktion $\varphi_{X}(t)$ einer symmetrischen
        Zufallsgröße $X$ ist reellwertig. 
    \item Seien $X^{+} = \max\left\{ X,0 \right\}$ und $X^{-} = \max\left\{ -X,0 \right\}$.
        $X$ ist symmetrisch genau dann, wenn die Verteilungen von $X^{+}$ und $X^{-}$ 
        gleich sind.
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item 

    \item Das folgt mit Hilfe der Euler'schen Formel.

    \item 
\end{enumerate}


\paragraph{Lebensdauer der Glühlampen.} Die Lebensdauer einer Glühlampe sei
eine exponentialverteilte Zufallsgröße.  Es sei bekannt, dass im Durchschnitt
$75\%$ der Glühlampen eine Mindestbrenndauer von $500$ Stunden erreichen.
Berechnen Sie Erwartungswert und Standardabweichung der Lebensdauer. 

\paragraph*{Lösung.} Aus $X \sim \mathbf{Ex}(\lambda)$ mit $P(X>500) = 0.75$
folgt $\lambda = - \frac{\log 0.75}{ 500} = 0.0005753641$.  $\bfE X = \sqrt{
\bfD^2 X } = \frac{1}{\lambda} = 1738.03$


\paragraph{Radioaktivität.} Eine radioaktive Substanz gebe im Verlauf
von $7.5 s$ im Mittel $3.87$ $\alpha$-Teilchen ab. Bestimmen Sie die
Wahrscheinlichkeit dafür, dass
\begin{enumerate}
    \item diese Substanz während einer Sekunde mindestens ein
        $\alpha$-Teilchen emittiert,
    \item zwischen der Emission zweier Teilchen mindestens zwei
        Sekunden vergehen,
    \item die Zeit zwischen zwei Emissionen zwischen einer und
        zwei Sekunden liegt.
\end{enumerate}

\paragraph*{Lösung.} $X_t$ ist Anzahl der Teilchen emmitiert im Laufe von
$[0,t]$. $X_t$ kann als Poisson-Verteilung modelliert werden. $X_t \sim \pi_t$
ist $\text{Poiss}(\mu t)$ mit $P(X_t = k) = \frac{ (\mu t)^{k} }{k!} e^{- \mu
t}$. Es gilt $\bfE X_t = \bfD^2 X_t = \mu t$. $\bfE X_{7.5} = 3.87 = 7.5 \mu$
impliziert $\mu = \frac{3.87}{7.5} = 0.516$.
\begin{enumerate}
    \item Wir nutzen die Homogenität der Poisson-Verteilung und berechnen
        \begin{align*}
            P(X_1 > 0) &= 1 - P(X_1=0) \\
            &= 1 - \frac{\mu^0}{0!}e^{-\mu} \\ 
            &= 1-e^{-0.516} \approx 0.4030966.
        \end{align*}
    \item Die Zwischenankunftszeiten $T_i$ sind Exponentialverteilt mit dem Parameter $\mu$
        und es gilt
        \begin{align*}
            P(T_i \geq 2) &= \exp( -2 \,\mu ) \approx 0.3562937.
        \end{align*}
    \item Analog erhalten wir
        \begin{align*}
            P( 1 \leq T_i \leq 2) &= e^{-\mu}-e^{-2\mu} \approx 0.2406097.
        \end{align*}
\end{enumerate}


\paragraph{Entfernungsmessung.} Die Entfernung eines Objekts werde mit Hilfe
eines Längenmessgerätes ermittelt. Dabei sei der gemessene Wert die Realisierung
einer normalverteilten Zufallsgröße mit dem Erwartungswert $\mu$ und der
Standardabweichung von $\sigma = 40\,m$.  Es sei bekannt, dass bei der Messung
kein systematischer Fehler auftritt.  Berechnen Sie die Wahrscheinlichkeit
dafür, dass bei einem $2\,500\,m$ entfernten Objekt bei der Messung ein um
$60\,m$ bis $80\,m$ zu großes Messergebnis ermittelt wird. 

\paragraph*{Lösung.} Für $X \sim \bfN(2500, 160)$ berechne 
\begin{equation*}
    P(2500 + 40 \leq X \leq 2500 + 80) = P(Z \leq 2) - P(Z \leq \frac{3}{2}).
        \approx 0.04405707.
\end{equation*}


\paragraph{Ersatzteillieferung.} Eine Ersatzteillieferung enthalte eine Kiste
Kugellager, zwei Kisten Zahnräder und drei Kisten Schrauben.  Die Masse der
Kisten (Bruttogewicht in kg) kann durch unabhängige und normalverteilte
Zufallsgrößen $X, Y_1, Y_2, Z_1, Z_2, Z_3$ mit
\begin{align*}
    X&\sim \bfN(125, 1), & Y_i&\sim \bfN(84, 4)\;\;(i=1,2), & Z_j&\sim \bfN(65, 3)\;\; (j=1,2,3)
\end{align*}
beschrieben werden.
\begin{enumerate}
    \item Berechnen Sie die Wahrscheinlichkeit dafür, dass die Masse einer
        Ersatzteillieferung größer ist als 500 kg.

    \item Wie viele solcher Ersatzteillieferungen darf man maximal auf einem
        Lastwagen laden, wenn die Gesamtmasse aller Ersatzteillieferungen mit
        einer Wahrscheinlichkeit von mindestens $99\%$ unter $18$ Tonnen liegen
        soll?
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item $V := X + Y_1 + Y_2 + Z_1+Z_2+Z_3 \sim  \bfN(488, 18)$. $\frac{500 -
        488}{\sqrt{18}} \approx 2.8284271$, und $P(V>500)= 1 - P(V\leq 500) =
        0.002338867$.
    \item Größes $n\in \bN$ für das
        \begin{equation*}
            P\left( W \leq \frac{18.000 - 488 n }{\sqrt{18 n}} \right) \geq 0.99
        \end{equation*}
        gilt, ist $n=36$. 
\end{enumerate}


\paragraph{Grenzverteilungsätze. Schaltkreise.} Ein spezieller
Schaltkreis sei mit der Wahrscheinlichkeit $p=0.7$ voll funktionsfähig.
Berechnen Sie die Wahrscheinlichkeit dafür, dass in einer 
gefertigten Serie von $1\,000$ Schaltkreisen 
\begin{enumerate}
    \item wenigstens $680$ Schaltkreise voll funktionsfähig sind,

    \item die Anzahl der voll funktionsfähigen Schaltkreise mindestens
        $675$ aber höchstens $725$ Stück beträgt.
\end{enumerate}

\paragraph*{Lösung.} Wir verwenden den Grenzverteilungsatz in der
Moivre-Laplace Version.  Dazu betrachten wir $X_1,\dots ,X_{1000} \sim B(p)$
und $\sum_{i} X_i \sim \text{Bin}(1000,p)$.  Dann ist die Zufallsgröße
\begin{equation*}
    Y = \frac{ \sum_{i} X_i - np}{ \sqrt{np(1-p)} } 
\end{equation*}
annähernd normalverteilt. Wir berechnen
\begin{align*}
    P( \sum_{i} X_i \geq 680 ) &= 1 - P\left( \sum_{i} X_i < 680 \right) \\
    &= 1- P\left( \frac{\sum_{i} X_i - np }{\sqrt{np(1-p)}} < \frac{680-np}{\sqrt{np(1-p)}} \right)\\
    &= 1-P( Y < - 1.380131 ) \approx 0.9162269.
\end{align*}
Analog ist 
\begin{align*}
    P( 675 \leq \sum_{i} X_i \leq 725 ) &\approx 0.9155021.
\end{align*}

\paragraph{Grenzverteilungsätze. Münzen.} Eine ideale Münze werde $n$-mal 
geworfen. Berechnen Sie unter Nutzung von Grenzverteilungseigenschaften die Wahrscheinlichkeit dafür, dass 
\begin{enumerate}
    \item bei $100$ Würfen die relative Häufigkeit für das Ereignis 
        ``Zahl'' um weniger als $0.1$ Einheiten von der Wahrscheinlichkeit
        $0.5$ abweicht.
    \item Wie oft müsste man die ideale Münze werfen, um mit mindestens
        $99\%$-iger Sicherheit eine Abweichung der relativen Häufigkeit
        von der Wahrscheinlichkeit zu erreichen, die kleiner ist als $0.1$
        Einheiten. 
    \item Lösen Sie die Aufgaben (a) und (b) alternativ mit Hilfe der Tschebyscheff-Ungleichung und vergleichen Sie die Ergebnisse. 
\end{enumerate}

\paragraph*{Lösung.} Die Anwendung des Satzes von Moivre-Laplace liefert
\begin{align*}
    P\left( | H_n(A) p | \leq \varepsilon \right) &= 
    2 \Phi\left ( \frac{\sqrt{n} \varepsilon}{ \sqrt{ p(1-p) }  } \right) -1 \approx 0.9544997.
\end{align*}
Für $n=166$ wird gerade die $99\%$ Wahrscheinlichkeit erreicht. Mit Hilfe der Tschbyschffschen
Ungleichung erhält man die Abschätzung
\begin{equation}
    P \left( | \frac{1}{n} \sum_{i}^{} X_i - 0.5 | \leq \varepsilon \right) > 1 - \frac{1}{4 n \varepsilon^2}.
\end{equation}
Für $n=100$ und $\varepsilon=0.1$ die obige Abschätzung gleich $0.8502994$. Die $99\%$-ige
Sicherheit erreichen wir erst bei $n=2500$. 

\paragraph{Grenzverteilungsätze. Kondensatoren.} Die Wahrscheinlichkeit dafür,
dass ein Kondensator während der Zeit $T$ ausfällt, betrage $0.5$. 
\begin{enumerate}
    \item Bestimmen Sie die Wahrscheinlichkeit dafür, dass von 100 solchen
        unabhängig voreinander arbeitenden Kondensatoren mehr als $60$ Stück in
        der Zeit $T$ ausfallen. 
    \item Wie viele Kondensatoren muss man mindestens in Reserve haben, damit
        nach der Zeit $T$ mit einer Wahrscheinlichkeit von mindestens $95\%$
        alle ausgefallenen Kondensatoren ersetzt werden können?
\end{enumerate}

\paragraph*{Lösung.} 
\begin{enumerate}
    \item $X\sim B(p)$, $Y=\sum_{i=1}^{100} \sim \text{Bin}(100,0.5)$, $E Y =
        np$, $\Var Y = np(1-p)= 25$. Wende den Grenzwertsatz von Moivre-Laplace
        da die Fausregel $np(1-p)>9$ erfüllt ist.
        \begin{equation*}
            P(Y>60) = P(Y < 40) \approx 
            \Phi\left( \frac{40 - np}{ \sqrt{np(1-p)}} \right) =
            \Phi(-2) = 0.02275013.
        \end{equation*}

    \item $P(Y \leq x) = 0.95$, $\Phi\left( \frac{x-50}{5} \right) \geq 0.95$
        liefert $x=59$. 
\end{enumerate}

\paragraph{Grenzverteilungsätze. Schaltgehäuse.} Beim Gießen spezieller
Schaltgehäuse trete ein Ausschussprozentsatz von $10\%$ auf. Für einen
reibungslosen Produktionsverlauf müssen je Gießvorgang (mindestens) $100$
brauchbare Gehäuse hergestellt werden.  Wie viel Gehäuse müssen mindestens bei
einem Vorgang gegossen werden, um mit mindestens $99\%$-iger Sicherheit diese
Forderung zu erfüllen?

\paragraph*{Lösung.} $Y\sim \sum_{i=1}^{n} X_i$, $X_i \sim B(p)$,
$p=1-0.1=0.9$.  $P(Y\geq 100) \geq 0.99$. $\Phi(x) \geq 0.99$, wenn $x\geq
2.326348$. 
\begin{align*}
    \frac{np - 100}{ \sqrt{np(1-p)} } &\geq 2.3263 \\
    n - 2.3263 \sqrt{p(1-p)} \sqrt{n} -100 &\geq 100 \\
    \Phi^{-1}(0.99) \sqrt{p(1-p)} &= 0.6979044
\end{align*}
Optimierung liefert $n=108$.


\section{Mehrdimensionale Verteilungen.}

\paragraph{Korrelationskoeffizient. Eigenschaften.} Seien $X$ und $Y$
Zufallsgrößen mit existierenden und endlichen Varianzen $\sigma_{X}$, $\sigma_{Y}$ und bezeichne mit
$\rho_{X,Y}$ den Korrelationskoeffizienten von $X$ und $Y$. Zeigen Sie folgende
Aussagen: 
\begin{enumerate}
    \item Für $a>0$, $c>0$ und $b\in\bR$ gilt
        \begin{equation*}
            \rho_{aX + b, cY + b} = \rho_{X,Y}.
        \end{equation*}
        Geben Sie eine Interpretation dieser Aussage an. 
    \item Für $a\neq 0$ und $b\in\bR$ gilt
        \begin{equation*}
            \rho_{X, aX+b} = \frac{a}{|a|}.
        \end{equation*}

    \item Sei
        \begin{equation*}
            Z = \frac{Y}{\sigma_{Y}} - \frac{\rho_{X,Y}}{\sigma_{X}} X.
        \end{equation*}
        Dann gilt
        \begin{equation*}
            \sigma^2_{Z} = 1 - \rho^{2}_{X,Y}.
        \end{equation*}
\end{enumerate}

\paragraph*{Lösung.}$\operatorname{Cov}(aX+b, cY + b)= ac
\operatorname{Cov}(X,Y)$. $\Var aX = a^2 \Var X$. 

\paragraph{Bivariate Normalverteilung. Maximum.} Seien $(X,Y)$ bivariat normalverteilt
mit $\bfE X=\bfE Y=0$, $\bfD^{2}Y=\bfD^{2}X=1$ und $\rho_{XY}$ dem Korrellationskoeffizienten
von $X$ und $Y$
Zeigen Sie, dass
\begin{equation*}
    \bfE \left[ \max\left\{ X,Y \right\} \right] = \sqrt{\frac{1-\rho}{\pi}}
\end{equation*}
gilt.
% @biNormalMax #bivariateNormal #multivariateNormal #unsolved &shaoExercises 

\paragraph{Korrelation und Symmetrie.} Eine Zufallsgröße $X$ nennen wir
symmetrisch, wenn die Verteilungen von $X$ und $-X$ gleich sind. Seien $X$ eine
stetige Zufallsgröße mit endlicher Varianz und $Y = |X|$. Beweisen Sie: Ist $X$
symmetrisch, so sind $X$ und $Y$ unkorreliert aber nicht unabhängig.

\paragraph*{Lösung.} Ist $X$ symmetrisch, so sind $X^{+}$ und $X^{-}$
gleichverteilt und $\operatorname{Cov}(X, |X|) = \bfD^2 X^{+} - \bfD^{2} X^{-}
= 0$. Klarerweise sind $X$ und $|X|$ nicht unabhängig. 

\paragraph{Korrelation und Gleichverteilung auf dem Einheitskreis.} Sei $(X,Y)$
ein Zufallsvektor mit der (zweidimensionalen) Dichtefunktion 
\begin{equation*}
    f(x,y) = \begin{cases}
        \pi^{-1}, & \text{falls } x^{2}+y^{2} \leq 1 \\
        0,& \text{sonst.}
    \end{cases}
\end{equation*}
Zeigen Sie, dass die Zufallsgrößen $X$ und $Y$ unkorreliert aber nicht
unabhängig sind. 
% sources: Shao Exercises ch. 1, Jacod/Protter ch. 12.

\paragraph*{Lösung.} $\bfE XY=0$. Das ergibt sich durchs Integrieten. Folglich
$\operatorname{Cov}(X,Y)=0$. Aus $X \upmodels Y$ folgt $f(x,y) = h(x) g(y)$ 
für alle $(x,y) \in \bR^{2}$. Für $x=1$ ergibt sich $g(y) \sim 1_{y=0}$. $g$ ist 
aber keine Dichte. Widerspruch. 

\paragraph{Bedingte Dichte. Ein Rechenbeispiel.} Sei $(X,Y)$ ein 
Zufallsvektor mit der Dichtefunktion
\begin{equation*}
    f(x,y) = \begin{cases}
        \frac{3}{5}y(x+y), & \text{falls } 0<x<2 \text{ und } 0<y<1 \\
        0,& \text{sonst.}
    \end{cases}
\end{equation*}
\begin{enumerate}
    \item Bestimmen Sie die bedingte Dichte $f_{Y|X=x}(y)$, für $x\in (0,2)$. 
%    \item Zeigen Sie, dass $P( Y \leq \frac{1}{2} \,|\, X=1) = \frac{1}{5}$ gilt.
    \item Zeigen Sie, dass $\mathbf{Cov} (X+Y, X-Y) = \frac{73}{100}$ gilt.
\end{enumerate}
% keywords: conditional density

\paragraph*{Lösung.} 
\begin{enumerate}
    \item Randdichte:
        \begin{equation*}
            f_{Y} (y) = \frac{3}{5} y(y+2).
        \end{equation*}
        Bedingte Dichte: 
        \begin{equation*}
            f_{Y \,|\, X=x} = \frac{x+y}{y+2}. 
        \end{equation*}

    \item Direkte Berechnung? Oh Gott nein!
\end{enumerate}

\section{Informatik. Prüfungsfragen. }

\paragraph{Schaltkreise. Qualitätskontrolle.} Bei einer Endkontrolle von
Schaltkreisen sollen nicht voll funktionsfähige Schaltkreise aussortiert
werden. Bekannt sei, dass im Mittel $20\%$ der erzeugten Schaltkreise nicht
voll funktionsfähig sind. Beim Prüfen werden $95\%$ dieser Schaltkreise
aussortiert, aber auch $1\%$ der einwandfreien.
\begin{enumerate}
    \item Mit welcher Wahrscheinlichkeit ist ein nicht aussortierter
        Schaltkreis voll funktionsfähig?
    \item Mit welcher Wahrscheinlichkeit ist ein aussortierter Schaltkreis
        einwandfrei?
\end{enumerate}

\paragraph{Widerstände.} Ein Automat produziert Widerstände, deren Werte
erfahrungsgemäß Realisierungen einer normalverteilten Zufallsgröße mit $\mu =
1000\ \Omega$ und $\sigma = 20\ \Omega$ sind. Widerstände außerhalb der
Toleranzgrenzen von $960\ \Omega$ und $1050\ \Omega$ gelten als Ausschuss.
\begin{enumerate}
    \item Wie viele von $950$ Widerständen sind im Mittel Ausschuss?
    \item Der Anteil der Widerstände mit einem Wert größer aus $1050\ \Omega$
        soll auf durchschnittlich $0.1\%$ gesenkt werden. Wie groß muss dann
        $\mu$ sein, wenn die mittlere quadratische Abweichung $\sigma^2$
        unverändert bleibt?
\end{enumerate}

\paragraph{Komplexes Gerät.} Ein Gerät besteht aus den Baugruppen $B_1,\dots
,B_4$. Es funktioniert, wenn wenigstens eine der Baugruppen $B_1$ oder $B_2$
und wenn außerdem beide Baugruppen $B_3$ und $B_4$ funktionieren. Die
Baugruppen können unabhängig voneinander ausfallen. Die
Ausfallwahrscheinlichkeiten bei $10$-stündiger Betriebsdauer betragen für $B_1$
und $B_2$ jeweils $0.5$, für $B_3$ und $B_4$ jeweils $0.2$. 
\begin{enumerate}
    \item Mit welcher Wahrscheinlichkeit funktioniert das Gerät nach
        $10$-stündigem Betrieb noch?
    \item Mit welcher Wahrscheinlichkeit funktionieren nach dieser Zeit noch
        alle vier Bauelemente?
\end{enumerate}

\paragraph{Geometrische Verteilung. Gedächtnislosigkeit und Erwartungswert.} Sei 
$X$ geometrisch verteilt mit dem Parameter $p\in (0,1)$ und der 
Wahrscheinlichkeitsfunktion
\begin{equation*}
    P(X = k) = p(1-p)^{k}, \ k=0,1,\dots
\end{equation*}
\begin{enumerate}
    \item Zeigen Sie, dass
        \begin{equation*}
            P( X > i+j \,|\, X \geq i) = P(X > j)
        \end{equation*}
        für alle $i,j>0$ gilt.
    \item Zeigen Sie, dass 
        \begin{align*}
            \bfE X = \frac{1-p}{p}
        \end{align*}
        gilt.
    \item Berechnen Sie den Erwartungswert der Zufallsgröße $Y=2\,X -1$.
\end{enumerate}

\paragraph{$\chi^{2}$-Verteilung mit einem Freiheitsgrad.} Sei $X$ 
standarisiert normalverteilt. Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item Die Verteilungsfunktion der Zufallsgröße $X^2$ ist $2\, \Phi(\sqrt{t})-1$,
        wobei $\Phi$ die Verteilungsfunktion der standarisierten Normalverteilung 
        bezeichnet.
    \item Die Dichtefunktion von $X^2$ ist 
        \begin{align*}
            f(x) = \begin{cases}
                \frac{1}{\sqrt{2 \pi x}} e^{-\frac{x}{2}} & \text{falls x>0}, \\
                0 & \text{sonst.}
            \end{cases}
        \end{align*}
\end{enumerate}

\paragraph{Konfidenzintervall und Test. Normalverteilung.} Beim Messen der
Entfernung zwischen zwei Punkten sei der Messwert die Realisierung einer
$\bfN(\mu, \sigma^2)$-verteilten Zufallsgröße. Aus $20$ Messungen einer
bestimmten Entfernung ergaben sich die folgenden Schätzwerte:
\begin{align*}
    \bar x &= 327.3\,m, & s^2 &= 17.3\,m^2.
\end{align*}
\begin{enumerate}
    \item Bestimmen Sie ein $99\%$-iges Konfidenzintervall für den
        Erwartungswert.

    \item Geben Sie ein einseitiges nach oben beschränktes Konfidenzintervall
        für $\sigma$ zum Konfidenzniveau $1-\alpha= 0.95$. 

    \item Lässt sich bei einer zugelassenen Irrtumswahrscheinlichkeit von $5\%$
        die Aussage $\mu > 320\,m$ rechtfertigen?

    \item Darf man bei einer zugelassenen Irrtumswahrscheinlichkeit
        $\alpha=0.05$ auf die Brauchbarkeit des Messgerätes schließen, wenn als
        Kriterium für die Tauglichkeit die Forderung $\sigma < 5.0\,m $ dient?
\end{enumerate}

\paragraph{Modifizierte P\'olya's Urne.} Eine Urne enthält $s=2$ schwarze und
$w=3$ weiße Kugeln. Es wird eine Kugel zufällig gezogen und ihre Farbe notiert.
Falls eine weiße Kugel gezogen wurde, wird diese beiseite gelegt. Falls eine
schwarze Kugel gezogen wurde, wird diese zusammen mit einer weiteren schwarzen
Kugel in die Urne gelegt. Die Prozedur wird anschließend wiederholt.
\begin{enumerate}
    \item Wie groß ist die Wahrscheinlichkeit, dass die zweite gezogene Kugel
        weiß ist?
    \item Wie groß ist die Wahrscheinlichkeit, dass die erste gezogene Kugel
        weiß ist, unter der Bedingung, dass die zweite gezogene Kugel weiß ist?
\end{enumerate}

\paragraph*{Lösung.}
Wir bezeichnen mit $W_i$ bzw.\ mit $S_i$ das Ereignis, dass bei der $i$-ten
Ziehung eine weiße bzw.\ eine schwarze Kugel gezogen wird.
\begin{enumerate}
    \item \begin{align*}
            P(W_2) &= P(W_2 \,|\, W_1)P(W_1) + P(W_2 \,|\, S_1)P(S_1) \\
            &= \frac{w-1}{s+w-1} \frac{w}{s+w} + \frac{w}{s+2+w} \frac{s}{w+s} = \frac{13}{24}. 
        \end{align*}
    \item \begin{align*}
            P( W_1 \,|\, W_2 ) &= \frac{P(W_2 \,|\, W_1) P(W_1)}{P(W_2)} = 
            \frac{1}{P(W_2)} \frac{ w-1 }{w-1+s}\frac{w}{w+w} = \frac{36}{75}. 
        \end{align*}
\end{enumerate}



\paragraph{Rademacher Zufallsgrößen.} Sei $X$ rademacherverteilt mit der
Wahrscheinlichkeitsfunktion
\begin{align*}
    P(X=-1) &= \frac{1}{2}, & P(X=1)& = \frac{1}{2}.
\end{align*}
\begin{enumerate}
    \item Berechnen Sie den Erwartungswert und die Varianz von $X$. 

    \item Zeigen Sie, dass alle ungeraden Momente und alle ungeraden zentralen
        Momente von $X$ verschwinden. 

    \item Berechnen Sie die charakteristische Funktion von $X$. 

    \item Seien $X_1$ und $X_2$ stochastisch unabhängig und rademacherverteilt.
        Berechnen Sie die Wahrscheinlichkeitsfunktion der Zufallsgröße
        \begin{equation*}
            Y = X_1 + X_2.
        \end{equation*}
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item $\bfE X= 0$, $\Var X = \bfE X^2 = 1$.
    \item $k\in 2\bN+1$ impliziert $X^k= X$, daher auch $m_{k} = \bfE X^k = \bfE
        X=0$. Die zentralen Momente $\mu_k$ und Momente $m_{k}$ sind gleich. 
    \item $\phi_{X}(t) = \bfE e^{i t X} = \frac{1}{2} \left(  e^{it} + e^{-it} \right)$.
    \item \begin{equation*}
            P(Y=k) =
            \begin{cases}
                1/4 & k=2 \\
                1/2 & k=0 \\
                1/4 & k=-2.
            \end{cases}
        \end{equation*}
\end{enumerate}

\paragraph{Hochwasserereignisse.} Historiker haben festgestellt, dass im
Großraum Halle im Mittel zwei extreme Hochwasserereignisse pro Jahrhundert zu
verzeichnen waren. Wir setzen voraus, dass dafür die Annahmen des
Poisson-Prozesses (Stationarität, Homogenität und Ordinarität) gelten mögen.
\begin{enumerate}
    \item Was bedeuten diese drei Annahmen für die vorliegende Problematik?
    \item Wie wahrscheinlich ist es, dass in einem Jahrhundert mehr als drei
        extreme Hochwasserereignisse im Großraum Halle auftreten?
    \item Wie wahrscheinlich ist es, dass nach dem extremen Hochwasser 2013
        innerhalb von 20 Jahren ein weiteres extremes Hochwasser in dieser
        Region auftreten wird?  
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
        \setcounter{enumi}{1}
    \item Rechnungseinheit Jahre: $\mu=\frac{2}{100}=0.02, \quad X_t \sim \pi_{\mu t}.$
        
        $t=100,\;\lambda=\mu t=2, \quad P(X_{100}>3)=1-\pi_2(0)-\pi_2(1)-\pi_2(2)-\pi_2(3)=\underline{0.1429}.$

    \item Zeitraum bis zum nächsten Ereignis: $\,T \sim \mathbf{Ex}(\mu)$.

        $P(T \le 20)=1-\exp(-20 \mu)=1-\exp(-0.4)=\underline{0.3297}.$
\end{enumerate}


\paragraph{GPS.} Nachdem die Verbindung zur $k$ Satelliten aufgebaut wurde
liefert ein GPS Gerät jede Sekunde eine Messung der geographischen Länge eines
festen Ortes. Es wird angenommen, dass kein systematischer Fehler vorliegt und
die Messungen mit unabhängigen normalverteilten Zufallsgrößen $X_i \sim
\bfN(\mu_X, \sigma^2)$ mit $\sigma=50\,m$ modelliert werden können.
\begin{enumerate}
    \item Berechnen Sie einen maximum-likelihood oder Momentenschätzer
        $\hat\mu_X\left( X_1,\dots ,X_n \right)$ für $\mu_X$.

    \item Wie viele Sekunden muss mindestens gewartet werden, damit der
        absolute Messfehler $| \mu_{X} - \hat \mu_{X} |$ mit Wahrscheinlichkeit
        von mindestens $95\%$ unter $1\,m$ liegt.
\end{enumerate}



\subsection{Quantile}


\paragraph{Exponentialverteilung. Quantile.} Seien 
$X\sim \mathbf{Ex}(\lambda)$ exponentialverteilt mit dem Paramter $\lambda>0$
und $\gamma\in (0,1)$ gegeben. 
\begin{enumerate}
    \item Berechnen Sie das $\gamma$-Quantil zur $X$. 
    \item Berechnen Sie den Median von $X$. 
\end{enumerate}

\paragraph*{Lösung.} Dichte und Verteilungsfunktionen der Exponentialverteilung
sind
\begin{align*}
    f(x) &= \lambda e^{-\lambda x} 1_{x \geq 0} & F(x) &= \left( 1-e^{-\lambda x} \right)1_{x \geq 0}. 
\end{align*}
Daraus ergibt sich für $\gamma\in (0,1)$ 
\begin{equation*}
    q_{\gamma} = F^{-1}(\gamma) = -\frac{1}{\lambda} \log \left( 1-\gamma \right). 
\end{equation*}
Median ist damit gleich $-\frac{1}{\lambda} \log \frac{1}{2}$. 


\paragraph{Logistische Verteilung. Quantile.} Eine Zufallsgröße heißt
logistisch verteilt mit den Parametern $\alpha\in\bR$ und $\beta>0$, wenn ihre
Dichtefunktion die Gestalt
\begin{equation*}
    f(x) = \frac{e^{-\frac{x-\alpha}{\beta}}}
    {\beta\left( 1+e^{-\frac{x-\alpha}{\beta}} \right)^{2}}
\end{equation*}
hat.
\begin{enumerate}
    \item Zeigen Sie, dass die Verteilungsfunktion von $X$ durch
        \begin{equation*}
            F(x) = \frac{1}{1+e^{-\frac{x-\alpha}{\beta}}}
        \end{equation*}
        gegeben ist.

    \item Berechnen Sie das $\gamma$-Quantil zur $X$ für ein $\gamma\in (0,1)$. 
    \item Berechnen Sie den Median von $X$. 
\end{enumerate}

\paragraph*{Lösung.} Die Quantilfunktion ist gegeben durch
\begin{equation*}
    Q(\gamma) = \alpha - \beta \log\left( \frac{1}{\gamma} - 1 \right). 
\end{equation*}
Der Median ist $Q(0.5) = \alpha$.

\subsection{Erwartungstreue}

\paragraph{Diskrete Gleichverteilung. Erwartungstreuer Schätzer.} Seien
$X_1,\dots ,X_k$ i.i.d.\ und diskret gleichverteilt auf $\left\{ 1,\dots ,N
\right\}$ mit $N>1$. Zeigen Sie, dass der Schätzer
\begin{equation*}
    T\left( X_1,\dots ,X_k \right) = \frac{k+1}{k} X_{(k)}-1
\end{equation*}
erwartungstreu für $N$ ist. $X_{(k)}$ bezeichnet dabei das Maximum von $\left\{
X_1,\dots ,X_k \right\}$. 

\paragraph{Log-Normalverteilung. Erwartungstreuer Schätzer.} Seien $X_1,\dots
,X_n$ i.i.d.\ und log-normalverteilt mit der Dichtefunktion
\begin{equation*}
    f(x) = \begin{cases}
        \frac{1}{ x\sqrt{2\pi} } \exp \left( - \frac{ (\log x - \mu)^2 }{ 2 } \right), & x\geq 0 \\
        0, & x<0
    \end{cases}
\end{equation*}
und dem Parameter $\mu\in\bR$. Zeigen Sie, dass der Schätzer 
\begin{equation*}
    T\left( X_1,\dots ,X_n \right) = \frac{1}{n} \sum_{i=1}^{n} \log X_i
\end{equation*}
erwartungstreu für $\mu$ ist. 

\paragraph*{Lösung.} Aus der Definition der Log-Normalverteilung oder durch
direktes Berechnen, erhalten wir
\begin{equation*}
    \bfE X_1 = \mu.
\end{equation*}
Damit ist $T$ erwartungstreu.


\section{Einfache Schätzer}

\paragraph{Telefonzentrale.} In einer Telefonzentrale wurden für $11$
aufeinanderfolgende Anrufe die Zeitabstände zwischen zwei Anrufen ermittelt. Es
ergaben sich die folgenden Werte:
\begin{lstlisting}
38.1  74.1  17.7  17.7  1.2  13.2  22.8  35.7  108.6  20.7
\end{lstlisting}
Geben Sie auf der Grundlage dieser Stichprobenwerte je eine Schätzung für den
mittleren Zeitabstand und die mittlere quadratische Abweichung der Zeitabstände
vom Erwartungswert an. 

Diskutieren Sie die Eigenschaften dieser Schätzfunktionen.

\paragraph*{Lösung.} Erwartungstreuer Schätzfunktionen für den Erwartungswert und
die Varianz sind
\begin{align*}
    \bar X_n &= \frac{1}{n} \sum_{i=1}^{n} X_n & S_n^{2} &= \frac{1}{n-1} \sum_{i=1}^{n} \left( X_i - \bar X_n \right)^{2}. 
\end{align*}
Für diese Stichprobe gilt
\begin{align*}
    \sum_{i=1}^{10} X_i &= 349.8 \\
    \bar X_n &= 34.98 \\
    S^2_n &= 1058.384.
\end{align*}




\subsection{Konfidenzintervalle}

\paragraph{Kondensatoren. Konfidenzintervall.} Aus einer Lieferung von
Kondensatoren wurden $n$ Stück zufällig ausgewählt und folgende Kapazitäten (in
$\mu$F) festgestellt (Normwert: $220 \mu$F):
\begin{lstlisting}
    281  221  220  221  219  221  220  218  222  219
\end{lstlisting}
\begin{enumerate}
    \item Berechnen Sie je eine Schätzung für den Erwartungswert und für die
        Varianz der Kapazität. 
    \item Berechnen Sie auf der Grundlage dieser Stichprobenergebnisse je ein
        Konfidenzintervall für den Mittelwert und die Varianz der Kapazität zum
        Konfidenzniveau $1-\alpha = 0.95$. Von welchen
        Verteilungsvoraussetzungen gehen Sie dabei aus?
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item $\bar X_n = 226.2 = \bar \mu$, $S^2_n = 372.1778 = \bar \sigma^2$.
    \item Wir nehmen an, dass $X \sim \bfN(\bar \mu, \bar \sigma^2)$ gilt. 
        Für die Berechnung des Konfidenzintervall für $\mu$ bei unbekannter Varianz
        benutzen wir die Beziehung
        \begin{equation*}
            \frac{\bar X_n - \mu}{ S_n} \sim t_{n-1}. 
        \end{equation*}
        Daraus folgt
        \begin{align*}
            P\left( - t_{n-1, 1-\frac{\alpha}{2}} \leq 
            \frac{\bar X_n - \mu}{ S_n}\sqrt{n}
            \leq t_{n-1, 1-\frac{\alpha}{2}} \right) = 1-\alpha. 
        \end{align*}
        Die $t_{n}$-Verteilung ist symmetrisch -- das folgt auf der Konstruktion: Für 
        $X\sim \bfN(0,1)$ und $Y\sim \chi^{2}_{n}$ ist 
        \begin{equation*}
            \frac{X}{\sqrt{Y/n}} \sim t_{n}. 
        \end{equation*}
        Insgesamt bekommen wir
        \begin{align*}
            P\left( \bar X_n - t_{n-1, 1-\frac{\alpha}{2}} \frac{S_{n}}{\sqrt{n}} 
            \leq \mu \leq \bar X_{n} + t_{n-1, 1-\frac{\alpha}{2}} \frac{S_{n}}{\sqrt{n}}
            \right) = 1-\alpha. 
        \end{align*}
        Konkret in unserem Fall heißt das
        \begin{equation*}
            t_{9, 1-\frac{0.05}{2}} = 13.8006
        \end{equation*}
        \begin{equation*}
            P\left( \mu \in \left[ 212.3994 , 240.0006 \right] \right) = 0.95.
        \end{equation*}
\end{enumerate}

\paragraph{Krankenversicherung. Konfidenzintervall.} Einer Krankenversicherung
seien für eine spezielle Gruppe von $15$ Versicherungsnehmern mittlere Kosten
in Höhe von $\bar x_n = 5\,500$ EUR bei einer Stichprobenstandardabweichung
$s_n = 1\, 500$ EUR entstanden.
\begin{enumerate}
    \item Geben Sie ein zweiseitiges Konfidenzintervall für die mittleren
        Kosten zum Konfidenzniveau $1-\alpha=0.95$ an. 
    \item Die Versicherung habe ihren Tarif unter der Annahme kalkuliert, dass
        die mittleren Kosten $5\, 000$ EUR betragen. Diskutieren Sie anhand des
        erhaltenen Konfidenzintervalls, ob eine Neukalkulation des Tarifs
        angezeigt erscheint. 
\end{enumerate}

\paragraph*{Lösung}
\begin{enumerate}
    \item Konfidenzintervall für $\mu$ bei unbekannter Varianz ist 
        \begin{align*}
            P\left( \bar X_n - t_{n-1, 1-\frac{\alpha}{2}} \frac{S_{n}}{\sqrt{n}} 
            \leq \mu \leq \bar X_{n} + t_{n-1, 1-\frac{\alpha}{2}} \frac{S_{n}}{\sqrt{n}}
            \right) = 1-\alpha. 
        \end{align*}
        Konkret heißt das in diesem Fall
        \begin{align*}
            P\left( \mu \in\left[ 5500-830.6723,5000+830.6723 \right]\right) &= 0.95 \\
            P\left( \mu \in\left[ 4669.328, 6330.672 \right]\right) &= 0.95
        \end{align*}

    \item $5000$ liegt im Konfidenzintervall, es gibt also keinen Grund unruhig
        zu werden.
\end{enumerate}

\paragraph{Qualitätssicherung. Konfidenzintervall.} Zur Schätzung des Ausschussanteils eines
umfangreichen Lieferpostens werde diesem eine Stichprobe von $200$ Teilen entnommen. 
Dabei wurden $190$ einwandfreie Teile festgestellt.
\begin{enumerate}
    \item Geben Sie eine Schätzung für den Ausschussanteil an.
    \item Wie groß kann maximal die Varianz der Schätzung sein? 

    \item Berechnen Sie ein Konfidenzintervall für den Ausschussanteil zum
        Konfidenzniveau $0.95$ und interpretieren Sie das Ergebnis.
\end{enumerate}

\paragraph*{Lösung.} 
\begin{enumerate}
    \item Es gilt $\sum_{i=1}^{200} X_i \sim \bfB(200, p)$. Die Schätzung für
        den Anteil ist die Schätzung für $p$. 
        \begin{equation*}
            \hat p = \frac{1}{n} \sum_{i=1}^{n} X_i
        \end{equation*}
        ist ein Erwartungstreuer Schätzer für $p$. Für die obige Stichprobe
        gilt $\hat p = \frac{1}{20}$.
    \item Varianz von $\hat p$ ist 
        \begin{equation*}
            \var \hat p = \frac{1}{n} p(1-p)
        \end{equation*}
        Diese wird maximal für $p=\frac{1}{2}$ und beträgt in diesem Fall
        $V^{2}_{\star} = 1/800$. Maximale Standardabweichung ist demnach
        $V_{\star} = \frac{1}{20\sqrt{2}}$.
    \item Nach dem Satz von Moivre/Laplace gilt 
        \begin{equation*}
            \bar X_{n} \approx \bfN \left( p, \frac{p(1-p)}{n} \right).
        \end{equation*}
        Die Annahme $\bar X_{n} \approx \bfN\left( p, V_{\star} \right)$ führt zur 
        Vergrößerung des Konfidenzintervalls. Wir erhalten so
        \begin{align*}
            P\left( \bar X_{n} -  V_{\star} z_{1-\frac{\alpha}{2}} \leq p
            \leq \bar X_{n} + V_{\star} z_{1-\frac{\alpha}{2}}\right) &= 1-\alpha \\
            P\left(  -0.01929519 \leq p \leq 0.1192952  \right) &= 0.95.
        \end{align*}
        Nachdem $\hat p \geq 0$ gelten muss erhalten wir
        \begin{equation*}
            P\left(  0 \leq p \leq 0.1192952  \right) = 0.95.
        \end{equation*}
        Negatives Intervall kommt durch die normale Approximation zustande. 
\end{enumerate}


\paragraph{Justierung. Konfidenzintervall.} Bei einer Arbeitsstudie wurden folgende
Werte für die Zeitdauer der Justierung eines Gerätes gemessen:
\begin{lstlisting}
    Minuten:       3  4  5  6  8  9
    Häufigkeiten:  2  4  7 12  4  1
\end{lstlisting}
\begin{enumerate}
    \item Geben Sie erwartungstreue Schätzungen für den Erwartungswert und die
        Varianz der Zeitdauer an.

    \item Bestimmen Sie ein zweiseitiges Konfidenzintervall zum Konfidenzniveau
        $1-\alpha=0.9$ für den Erwartungswert der Zeitdauer. 
    \item Ermitteln Sie eine obere Grenze für die Standardabweichung der Zeitdauer,
        so dass mit $99\%$-iger Sicherheit der wahre Wert von $\alpha$ überdeckt
        wird.
\end{enumerate}
Geben Sie dabei alle notwendigen Voraussetzungen an.

\paragraph*{Lösung.} 
\begin{enumerate}
    \item Erwartungstreue Schätzungen:
        \begin{lstlisting}
            > Y <- c(3,4,5,6,8,9) 
            > H <- c(2,4,7,12,4,1)
            > mu <- sum(Y*H)/sum(H)         
            > mu
            [1] 5.666667
            > s <- sqrt( sum(H*(Y - mu)^2)/(sum(H)-1 ) )
            > s
            [1] 1.470007
        \end{lstlisting}
\end{enumerate}


\subsection{Tests}


\paragraph{Waschmittel. Test.} Bei einem Verbrauchertest für Waschmittel werde auch 
die Abfüllmenge kontrolliert. Dabei ergaben sich bei $10$ zufällig ausgewählten $5$ kg
Packungen einer bestimmten Sorte folgende Abfüllungen (in kg):
\begin{lstlisting}
    4.6  4.95  4.8  4.9  4.75  5.05  4.9  5.1  4.8  4.95
\end{lstlisting}
\begin{enumerate}
    \item Bestimmen Sie Schätzungen für den Erwartungswert und die Varianz 
        der Abfüllmenge.
    \item Ist auf der Basis dieser Beobachtung die Auffassung vertretbar, 
        dass die Packungen im Mittel weniger Waschmittel als angegeben enthalten?
\end{enumerate}

\paragraph{Messgerät. Test.} Die Genauigkeit eines Messgerätes verschlechtere
sich im Laufe der Zeit infolge von Abnutzungserscheinungen. Das Messgerät gelte
als brauchbar, solange die Varianz $\sigma^2$ des Messfehlers einen Wert von
$10^{-4}$ nicht überschreitet.

Darf man bei einer zugelassenen Irrtumswahrscheinlichkeit $\alpha=0.05$ auf die
Brauchbarkeit des Messgerätes schließen, wenn aus $25$ Kontrollwerten für die
Varianz eine Schätzung $s^2 = 0.65 \cdot{} 10^{-4}$ ermittelt werde? 


\paragraph{Entfernungsmessung. Konfidenzintervall und Test.} Beim Messen der
Entfernung zwischen zwei Punkten sei der Messwert die Realisierung einer
$\bfN(\mu,\sigma^2)$-verteilten Zufallsgröße. Aus $20$ Messungen einer
bestimmten Entfernung ergaben sich die folgenden Schätzwerte:
\begin{align*}
    \bar x &= 327.0\,\text{m}, & s^2 &= 17.3\, \text{m}^2.
\end{align*}
\begin{enumerate}
    \item Bestimmen Sie ein $99\%$-iges Konfidenzintervall für den Erwartungswert.
    \item Geben Sie ein einseitiges nach oben beschränktes Konfidenzinvervall für 
        $\sigma$ zum Konfidenzniveau $1-\alpha = 0.95$ an. 
    \item Lässt sich bei einer zugelassenen Irrtumswahrscheinlichkeit von $5\%$ die Aussage
        $\mu>320\,\text{m}$ rechtfertigen?
    \item Darf man bei einer zugelassenen Irrtumswahrscheinlichkeit $\alpha=0.05$ auf die
        Brauchbarkeit des Messgerätes schließen, wenn als Kriterium für die Tauglichkeit
        die Forderung $\sigma<5.0\,\text{m}$ dient?
\end{enumerate}






