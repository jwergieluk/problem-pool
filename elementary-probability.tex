% copyright (C) 2011-2014 Julian Wergieluk  <julian@wergieluk.com>

\section{Mengensysteme}

\problem{Ereignisse. Mengenoperationen.}
Gegeben seien jeweils der Grundraum $\Omega$ sowie zwei Teilmengen
$A$ und $B$:
\begin{enumerate}
\item $\Omega=\{1,2,...,20\}$, $A=\{4,5,6,7,9,11\}$, $B=\{3,5,9,20\}$
\item $\Omega=[-1,3]$, $A=[0,1)$, $B=(\frac{1}{2},2]$
\item $\Omega=\mathbb R$, $A=\{x\in \mathbb R: |x-1|<3\}$, $B=[0,\infty)$.
\end{enumerate}
Bilden Sie die Mengen $\overline{A}$, $\overline{B}$, $A\cap B$, $A\cup B$,
$\overline{ A\cup B}$, $\overline{A}\cap\overline{B}$, $B\cap\overline{A}$,
$(\overline{A}\cup\overline{B})\cap\overline{B}$,
$B\cup(\overline{B\cap\overline{A}})$.


\problem{Ereignisse. Kraftwerk.}
Die Arbeit eines Kraftwerkes wird durch drei unabhängig voneinander arbeitende
Kontrollsysteme überwacht, die jedoch auch einer gewissen Störanfälligkeit
unterliegen. Es bezeichne $S_i$ das Ereignis, dass das $i$-te System
störungsfrei arbeitet $(i=1,2,3)$.
\begin{enumerate}
    \item Finden Sie einen geeigneten Grundraum $\Omega$, der diese
        Zufallssituation beschreibt. Ist $\Omega$ eindeutig bestimmt?
%    \item Finden Sie einen geeigneten Wahrscheinlichkeitsraum, der diese
%        Zufallssituation beschreibt. Geben Sie die Ergebnismenge $\Omega$
%        explicit an.
    \item Drücken Sie folgende Ereignisse mit Hilfe der Ereignisse $S_1$, $S_2$
        und $S_3$ aus:
        \begin{itemize}
            \item[$A$:] Alle drei Systeme arbeiten störungsfrei.
            \item[$B$:] Kein System arbeitet störungsfrei.
            \item[$C$:] Mindestens ein System arbeitet störungsfrei.
            \item[$D$:] Genau ein System arbeitet störungsfrei.
            \item[$E$:] Höchstens zwei Systeme sind gestört.
        \end{itemize}\label{ereignisse-kraftwerk-1}
    \item Welche der unter \ref{ereignisse-kraftwerk-1} genannten Ereignisse
        sind Elementarereignisse?
    \item Aus wie vielen Elementen bestehen die Ereignisse $D$ und $C$?
\end{enumerate}

\solution Der Grundraum $\Omega$ kann als $\Omega= \left\{ (ijk)
: i,j,k\in \left\{ 0,1 \right\} \right\}$ gewählt werden. Diese Wahl ist aber nicht 
eindeutig. Die Ereignisse $A$ und $B$ sind in diesem Fall Elementarereignisse. 


\problem{Ereignisse. Aktienmarkt.}
Beim Monatsvergleich zweier Technologieaktien wird für jede Aktie
festgestellt, ob es zu einem Gewinn von mindestens 3\% kam, ob sich ein Verlust
um mehr als 3\% ergab oder ob sich die jeweilige Aktie innerhalb der 6\%-Spanne
bewegte.
\begin{enumerate}
    \item Finden Sie einen geeigneten Grundraum $\Omega$, der diese
        Zufallssituation beschreibt. Ist $\Omega$ eindeutig bestimmt?
%    \item Finden Sie einen geeigneten Wahrscheinlichkeitsraum, der diese
%        Zufallssituation beschreibt. Geben Sie die Ergebnismenge $\Omega$
%        explicit an.
    \item Stellen Sie folgende Ereignisse mit Hilfe der Elementarereignisse
        dar:
        \begin{itemize}
            \item[$A$:] Beide Aktien erzielten einen Kursgewinn von mindestens $3\%$.
            \item[$B$:] Die Kurse der beiden Aktien lagen innerhalb der
                $6\%$-Spanne. %Keine der beiden Aktien veränderte sich signifikant.
            \item[$C$:] Der Kurs von höchstens einer der beiden Aktien
                verschlechterte sich um mehr als $3\%$. 
            \item[$D$:] Der Kurs von mindestens einer der beiden Aktien
                verschlechterte sich um mehr als $3\%$. 
        \end{itemize}
    \item Welche Bedeutung haben die Ereignisse\\
        $E_1=A\cup C$, $E_2=A\cup D$,
        $E_3=A\cap C$, $E_4=A\cap\overline{C}$, $E_5= \overline{A\cap D}$ ?
\end{enumerate}



\problem{Ereignisse. Fertigungsstraße.}
Eine Fertigungsstraße bestehe  aus einer Maschine vom Typ I, vier Maschinen vom
Typ II und zwei Maschinen vom Typ III. Wir bezeichnen mit $A$, $B_k$ bzw.\
$C_j$ ($k=1,2,3,4$; $j=1,2$) die Ereignisse, dass die Maschine vom Typ I
bzw.~die $k$-te Maschine vom Typ~II bzw.~die $j$-te Maschine vom Typ III intakt
ist. Die Fertigungsstraße sei arbeitsfähig, wenn mindestens eine Maschine von
jedem Maschinentyp intakt ist. Dieses Ereignis werde mit $D$ bezeichnet.

Beschreiben Sie die Ereignisse $D$ und $\overline{D}$ mit Hilfe der Ereignisse
$A$, $B_k$, $C_j$.


\problem{Ereignisse. Kosten.}
Drei Betriebsteile werden auf Einhaltung eines bestimmten Kostenfaktors
überprüft. Das Ereignis $A$ liege vor, wenn mindestens ein Betriebsteil nicht
den geforderten Kostenfaktor einhält, das Ereignis $B$ liege vor, wenn alle
drei Betriebsteile den geforderten Kostenfaktor einhalten.

Was bedeuten dann die Ereignisse $A\cup B$ und $A\cap B$ ?

\problem{Zufallsvariablen. Urbildmengen.}
Gegeben seien ein Grundraum $\Omega= \bR$ und die Zufallsvariable
\begin{align*}
    X &: \Omega \to \bR : \omega \mapsto \omega^2.
\end{align*}
Geben Sie folgende Mengen explizit an:
\begin{align*}
    \left\{ X = 0 \right\}, && \left\{ X \in [1,2) \right\}, && \left\{ X > 2 \right\}, && 
    \left\{ 2X < 4 \right\}, && \left\{ |X-2| > 1 \right\}.
\end{align*}

\problem{Zufallsvariablen. Zwei unterscheidbare Würfel.}
Ein weißer Würfel und ein schwarzer Würfel werden geworfen. 
\begin{enumerate}
    \item Geben Sie einen geeigneten Grundraum $\Omega$ an, der diese 
        Zufallssituation beschreibt.
    \item Die Zufallsvariablen $W$ bzw.\ $S$ geben die Augenzahl des weißen bzw.\ 
        schwarzen Würfels an. Geben Sie die Definitionen von $W$ und $S$ explizit an.
        Wie definieren Sie den Zustandsraum?

    \item Geben Sie folgende Ereignisse explizit an:
        \begin{align*}
            \left\{ W = 6 \right\}, && \left\{ W+S = 4 \right\}, && \left\{ W > S \right\}, && \left\{ \frac{W}{S} > 1 \right\}.
        \end{align*}
\end{enumerate}

\problem{Zufallsvariablen. Indikatorfunktionen.}
Ein Experiment bestehe aus einfachem Würfelwurf und wird durch den Grundraum
$\Omega = \left\{ 1,2,3,4,5,6 \right\}$ beschrieben. Die Zufallsvariable $X:
\Omega \to \bR, \omega \mapsto \omega$ liefert das Ergebnis des Würfelwurfs.
Zeigen Sie
\begin{equation*}
    X = 1_{ [1,6] } + 1_{[2,6]} + \dots + 1_{ [6,6] }.
\end{equation*}

\problem{Zufallsvariablen. Schachmeister.}
Ein Schachmeister besucht einen Schachverein und spielt eine Partie Simultanschach gegen
die fünf anwesenden Clubmitglieder. Dazu werden fünf Schachbretter in einer Reihe aufgebaut.
Auf der einen Seite sitzen vor je einem Brett die Vereinsmitglieder, während auf der anderen
Seite der Schachmeister reihum auf jedem Brett zieht.

Es sei X die Anzahl der Spiele, die der Schachmeister gewinnt. (Ein Schachspiel kann mit
Remis beendet werden, so dass keiner gewinnt.)
\begin{enumerate}
    \item Geben Sie einen Grundraum $\Omega$, die Zufallsvariable $X$ und den Zustandsraum $Z$ an.
    \item Bestimmen Sie die Anzahl der Elemente von $\Omega$.
    \item Wie viele Elementarereignisse besitzt das Ereigns $\{X \ge 4\}$?
\end{enumerate}

\solution
\begin{enumerate}
    \item $\Omega = \{0,1\}^5$ ($1:$ gewonnen, $0:$ nicht gewonnen), $X:\Omega\to Z=\{0,1,2,3,4,5\}$, $X(\mathbf \omega) = \sum_{i=1}^5 \omega_i$ wobei $\mathbf \omega = (\omega_1,\hdots,\omega_5)\in\Omega$
    \item $|\Omega|=2^5=32$
    \item $|\{X\ge4\}| = |\{X=5\} \cup \{X=4\}| = 1 + 4 = 5$
\end{enumerate}
Die Wahl von $\Omega$ ist hierbei nicht eindeutig. Es könnte alternativ auch folgende Lösung gewählt werden:
\begin{enumerate}
    \item $\Omega = \{-1,0,1\}^5$ ($1:$ gewonnen, $0:$ nicht gewonnen, $-1:$ verloren),
          $X:\Omega\to Z=\{0,1,2,3,4,5\}$
    \item $|\Omega|=3^5=243$
    \item $|\{X\ge4\}| = |\{X=5\} \cup \{X=4\}| = 1 + 8 = 9$
\end{enumerate}



\problem{$\sigma$-Algebren. Schachspieler.} Zwei Schachspieler spielen eine
Partie. Das Ereignis $A$ liege vor, falls der erste Spieler gewinnt, das
Ereignis $B$, falls der zweite gewinnt. Welche Ereignisse sind noch
hinzuzufügen, damit eine $\sigma$-Algebra entsteht?

\solution
Für $\Omega\ne\emptyset$ heißt $\cA\subset\cP(\Omega)$ Algebra, falls
\begin{enumerate}[(i)]
  \item $\Omega\in\cA$
  \item $A\in\cA \Rightarrow \overline A\in\cA$
  \item $A_1,A_2\in\cA \Rightarrow A_1\cup A_2\in\cA$
\end{enumerate}
Für eine $\sigma$-Algebra wird statt (iii) allgemeiner gefordert
\begin{enumerate}
  \item[(iii$^\sigma$)] $A_i\in\cA, i\in\N \Rightarrow \bigcup_{i=1}^\infty A_i\in\cA$
\end{enumerate}
Für $|\cA|<\infty$ sind (iii) und (iii$^\sigma$) äquivalent, d.h. jede endliche Algebra ist auch $\sigma$-Algebra.

\emph{Interpretation:} Jede und/oder-Verknüpfung sowie Verneinungen von Ereignissen sind wieder als Ereignis darstellbar.

\begin{itemize}
  \item Das Ereignis C liege bei Unentschieden vor.
  \item Das sichere Ereignis ergibt sich dann offenbar als $\Omega=A\cup B\cup C \in\cA$
        (``Ein Spieler gewinnt oder Unentschieden'' tritt immer ein).
  \item Wegen (ii) ist dann auch das unmögliche Ereignis $\emptyset=\overline\Omega\in\cA$ Element der Algebra
        (es gibt keine anderen Spielausgänge).
  \item Wegen (iii) sind außerdem $A\cup B\in\cA$, $A\cup C\in\cA$ und $B\cup C\in\cA$.
  \item Offenbar gilt $\overline{A\cup B} = C\in\cA$, $\overline{A\cup C} = B\in\cA$ und $\overline{B\cup C} = A\in\cA$
  \item $\cA=\{\emptyset,A,B,C, A\cup B, A\cup C, B\cup C, \Omega\}$ erfüllt alle Bedingungen einer Algebra.
        Auf Grund der Endlichkeit ($|\cA|=8$) ist dies auch eine $\sigma$-Algebra.
\end{itemize}

\problem{$\sigma$-Algebren. Potenzmenge.}
Sei $\Omega$ ein endlicher nichtleerer Grundraum. Zeigen Sie, dass die
Menge aller Teilmengen von $\Omega$, also die Potenzmenge $\cP(\Omega)$,
endlich und eine $\sigma$-Algebra ist.

\problem{$\sigma$-Algebren. Restriktion.}
Sei $\cA$ eine $\sigma$-Algebra von Teilmengen eines Grundraumes $\Omega$ und
$B \in \cA$ mit $B\neq \emptyset$. Zeigen Sie, dass $\cC = \left\{ A\cap B :
A\in\cA \right\}$ eine $\sigma$-Algebra von Teilmengen von $B$ ist.

\problem{Ereignisse. Würfel und Münze I.}
Ein Experiment bestehe aus dem Werfen eines sechsseitigen Würfels und einer Münze.
\begin{enumerate}
    \item Geben Sie einen Grundraum $\Omega$ an, der diese Zufallssituation beschreibt.
    \item Stellen Sie die folgenden Ereignisse sowohl formell als auch durch Aufzählung der Elementareignisse dar.
    \begin{itemize}
      \item[$A$:] Der Würfel zeigt eine 1 und die Münze zeigt Zahl.
      \item[$B$:] Der Würfel zeigt eine gerade Augenzahl.
      \item[$C$:] Der Würfel zeigt eine 6 oder die Münze zeigt kein Wappen.
      \item[$D$:] Entweder $A$ oder $C$ tritt ein.
    \end{itemize}
\end{enumerate}
\solution
\begin{enumerate}
  \item \begin{align*}
           \Omega &= \left\{1,2,3,4,5,6\right\} \times \left\{W,Z\right\} \\ 
                  &= \left\{(w,m): w\in\{1,2,3,4,5,6\},m\in\{W,Z\} \right\} \\
                  &=\{(1,W), (2,W), (3,W), (4,W), (5,W), (6,W), 
                     (1,Z), (2,Z), (3,Z), (4,Z), (5,Z), (6,Z)\}
        \end{align*}
  \item 
    \begin{itemize}
      \item[$A$:] $W_1 := \{(1,W), (1,Z)\}$, $M_Z:=\{(1,Z), (2,Z), \hdots, (6,Z)\}$,\\
            $A = W_1 \cap M_Z = \{(1,Z)\}$
      \item[$B$:] $B = W_2\cup W_4\cup W_6 = \{(2,W), (2,Z), (4,W), (4,Z), (6,W), (6,Z)\}$
      \item[$C$:] $C = W_6 \cup \overline{M_W} = \{ (6,W), (6,Z), (1,Z), (2,Z), (3,Z), (4,Z), (5,Z) \}$
      \item[$D$:] $\overline{A} = \Omega\setminus \{(1,Z)\} $, $\overline{C} = \{(1,W),\hdots,(5,W)\}$ \\
            $D = A\triangle C = (A \cap \overline{C}) \cup (\overline{A} \cap C) = \Omega \setminus ( (A\cap C) \cup (\overline{A} \cap \overline{C})) = \{(2,Z), \hdots, (6,Z), (6,W)\}$  
    \end{itemize}
\end{enumerate}

\problem{Ereignisse. Würfel und Münze II.}
Ein Experiment bestehe aus dem Werfen eines fairen Würfels und einer fairen Münze.
\begin{enumerate}
    \item Geben Sie einen Wahrscheinlichkeitsraum ($\Omega$, $\cA$, $\bP$) an,
          der diese Zufallssituation beschreibt.
    \item Zeigt die Münze Wappen, so wird die doppelte Augenzahl des Würfels
        notiert, bei Zahl nur die einfache. Wie groß ist die
        Wahrscheinlichkeit, dass eine gerade Zahl notiert wird?
\end{enumerate}

\problem{Ereignisse. Zerlegung des Würfels.}
Ein Würfel, dessen Seitenflächen gleichartig gefärbt sind, werde in 1000
kleine Würfel einheitlicher Größe zerlegt.

Wie groß ist die Wahrscheinlichkeit dafür, dass ein zufällig ausgewählter
Würfel auf mindestens einer Seite gefärbt ist?


\problem{Ereignisse. Elementare Wahrscheinlichkeiten.}
Für die Ereignisse $A$ und $B$ seien folgende Wahrscheinlichkeiten bekannt:
$\bP(A)=0.25$, $\bP(B)=0.45$, $\bP(A\cup B)=0.5$. Berechnen Sie die
Wahrscheinlichkeiten:
\begin{enumerate}
    \item $\bP(A\cap\overline{B})$,
    \item $\bP(\overline{A}\cap\overline{B})$ und
    \item $\bP\left((A\cap\overline{B})\cup(\overline{A}\cap B)\right)$.
\end{enumerate}

\solution
\begin{enumerate}
    \item \begin{align*}
            \bP(A \cup B) &= \bP( (A \cap \bar B) \cup B ) = 
            \bP( A \cap \bar B  ) + \bP(B) \\
            \bP(A \cup \bar B) &= \bP(A \cup B) - \bP(B) = 0.05.
        \end{align*}
    \item \begin{align*}
            \bP(\bar A \cap \bar B) &= \bP(\overline{A \cup B}) = 0.5.
        \end{align*}
    \item Die symmetrische Differenz $A \Delta B = (A \setminus B) \cup (B \setminus A)$
        ist eine disjunkte Vereinigung, und daher
        \begin{align*}
            \bP(A \Delta B) &= \bP(A \cap \bar B) + \bP(\bar A \cap B).
        \end{align*}
        Die vorherigen Überlegungen liefern
        \begin{align*}
            \bP( A \cap \bar B) &= \bP(A \setminus B) = \bP(A \cup B) - \bP(B) = 0.05 \\
            \bP( \bar A \cap B) &= \bP(B \setminus A) = \bP(A \cup B) - \bP(A) = 0.25.
        \end{align*}
        Insgesamt gilt also $\bP(A \Delta B) = 0.3$.
\end{enumerate}


\section{Wahrscheinlichkeitsmaße}

\problem{Wahrscheinlichkeitsmaße. Monotonie.} Seien ein Wahrscheinlichkeitsraum \linebreak
$(\Omega, \cA, \bP)$ und die Ereignisse $A,B \in \cA$ gegeben. Zeigen Sie
\begin{equation*}
A \subseteq B \quad  \impl \quad \bP(A) \leq \bP(B).
\end{equation*}

\solution $\bP(B) = \bP(A \cup (B \setminus A))= \bP(A) + \bP(B\setminus A) \geq \bP(A)$. 


\problem{Wahrscheinlichkeitsmaße. Subaditivität.} Seien ein
Wahrscheinlichkeitsraum $(\Omega, \cA, \bP)$ und die Ereignisse $A_1, A_2, \dots
\in \cA$ gegeben. 
\begin{enumerate}
    \item Zeigen Sie, dass für alle  $n\in \bN$ 
        \begin{equation*}
            \bP \left(  \bigcup \limits_{i=1}^n A_i \right) \leq \sum_{i=1}^{n} \bP\left( A_i \right)
        \end{equation*}
        gilt. 

    \item Zeigen Sie, dass sogar gilt 
        \begin{equation*}
            \bP \left(  \bigcup \limits_{i=1}^\infty A_i \right) \leq \sum_{i=1}^{\infty} \bP\left( A_i \right).
        \end{equation*}
\end{enumerate}

\solution
\begin{enumerate}
    \item Betrachte folgende Darstellung der Vereinigung als Summe disjunkter
        Elemente von $\cA$.
        \begin{align*}
            \bigcup_{i=1}^n A_i &= A_1 \cup (\bar A_1 \cap A_2) \cup 
            (\bar A_1 \cap \bar A_2 \cap A_3) \cup \dots \cup
            (\bar A_1 \cap \bar A_2 \cap \dots \cap \bar A_{n-1} \cap A_n).
        \end{align*}

    \item Genau das gleiche Argument funktioniert für $n=\infty$. Das ist 
        die Konsequenz der $\sigma$-Additivität des Wahrscheinlichkeitsmaßes $\cP$. 
\end{enumerate}


\problem{Wahrscheinlichkeitsmaße. Zerlegung der Vereinigung.} Seien ein
Wahrscheinlichkeitsraum $\left( \Omega, \cA, \bP \right)$ und die Ereignisse
$A,B,C\in \cA$ sowie $A_1, A_2, \dots, A_n \in \cA$ gegeben.
Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item \begin{equation*}
            \bP \left( A \cup B \cup C \right) = 
            \bP(A)+\bP(B)+\bP(C) - \bP(A\cap B) - \bP(A\cap C) - \bP(B\cap C) + \bP(A\cap B \cap C). 
        \end{equation*}

    \item
        \begin{align*}
            \bP\left( \bigcup \limits_{i=1}^{n} A_i \right) =&
            \sum_{i=1}^{n} \bP(A_i) - \sum_{i<j} \bP(A_i \cap A_j) + \\
            & \sum_{i<j<k} \bP(A_i \cap A_j \cap A_k) - \dots + 
            (-1)^{n+1} \bP\left( A_1 \cap \dots \cap A_n \right). 
        \end{align*}
\end{enumerate}

\solution 
\begin{enumerate}
    \item Hier genügt eine Überlegung mit Hilfe der Venn-Diagrame. 

    \item Die Menge $A_1 \cup \dots \cup A_n$ ist Vereinigung der Mengen der
        Form $A_{i_1} \cap \dots \cap A_{i_k} \cap \bar A_{i_{k+1}} \cap
        \dots \cap \bar A_{i_{n}}$, $k\geq 1$, wobei $i_1,\dots, i_n$ eine
        Umnumerierung von $1, \dots, n$ ist. Nun kommt solche Menge in der
        ersten Summe als Teilmenge von $A_i$ genau $\binom{k}{1}
        $ vor. In der zweiten Summe wird die Menge $\binom{k}{2}$ mal abgezogen. 
        Nachdem $\sum_{i=0}^{k} (-1)^{i} \binom{k}{i} = 0$, erhalten wir 
        $\sum_{i=1}^{k} (-1)^{i+1} \binom{k}{i}=1$.
\end{enumerate}


\problem{Wahrscheinlichkeitsmaße. Einfache Ungleichung.} Seien ein
Wahrscheinlichkeitsraum $\left( \Omega, \cA, P \right)$ und die Ereignisse $A$
und $B$ gegeben. Beweisen Sie die Ungleichung $$P(A \cap B) \geq P(A) + P(B) -1.$$

\problem{Arithmetik der Wahrscheinlichkeitsmaße.} Seien ein
Wahrscheinlichkeitsraum $\left( \Omega, \fA, P \right)$ und die Ereignisse $A,
B \in \fA$ gegeben. 
\begin{enumerate}
    \item Angenommen $P(\bar A)=\frac{1}{3}$, $P(A\cap B)=\frac{1}{4}$ und
        $P(A \cup B)=\frac{2}{3}$. Berechnen Sie $P(\bar B)$, $P(A \cap \bar B)$
        und $P( B \cap \bar A )$.
        
    \item Angenommen $P(A \cup B) = \frac{1}{2}$, $P(A \cap B) = \frac{1}{4}$
        und $P(A \cap \bar B) = P(B \cap \bar A)$. Berechnen Sie $P(A)$ und
        $P(A \cap \bar B)$. 
\end{enumerate}

\solution
\begin{enumerate}
    \item $B \subset A$.

    \item $P(A) = \frac{3}{8}$ und $P(A \setminus B) = \frac{1}{8}$.
\end{enumerate}



\section{Kombinatorik.}

\problem{Grundformeln der Kombinatorik.} 
Die Kugeln in einer Urne sind mit Zahlen $1,2,\dots, n$ nummeriert. Es werden
nacheinander $m$ Kugeln aus der Urne entnommen und die zugehörigen Zahlen
notiert. Nach jeder Entnahme können die Kugeln wieder in die Urne zurückgelegt
oder beiseite gelegt werden. Es muss auch festgelegt werden, ob die Reihenfolge
der notierten Zahlen eine Rolle spielt. Abhängig von diesen Entscheidungen 
ergeben sich folgende Anzahlen der möglichen Ergebnisse:
\begin{center}
\begin{tabular}{ | r | c | c |}
    \hline
    & geordnet (Variationen) & ungeordnet (Kombinationen) \\
    \hline
    mit Zurücklegen & $n^{m}$ & $\binom{n+m-1}{m}$ \\
    \hline
    ohne Zurücklegen & $\frac{n!}{(n-m)!}$ & $\binom{n}{m}$ \\
    \hline
\end{tabular}
\end{center}
Betrachten Sie folgende Zufallssituationen
\begin{enumerate}
    \item Der Betreiber der Lotto Lotterie ($6$ aus $49$) ermittelt die gewinnende
        Zahlenkombination.
    \item Ein Benutzer gibt ein zufälliges $8$-stelliges Passwort ein.
    \item Es wird zufällig eine injektive (verschiedenwertige) Funktion \[f:
        \left\{ 1,\dots ,m \right\} \to \left\{ 1,\dots ,n \right\}\] gewählt.
    \item Es wird zufällig eine $k$-elementige Teilmenge der Menge $\left\{
        1,2,\dots ,n \right\}$ gewählt.
    \item Es wird zufällig eine Bijektion $f: \left\{ 1,\dots ,n \right\}\to
        \left\{ 1,\dots ,n \right\}$ gewählt. 
    \item $m$ Krapfen (nicht unterscheidbar) werden zufällig an $n$
        Programmierer verteilt. 
    \item Es wird zufällig eine Folge nichtnegativer ganzer Zahlen $\left(
        r_{1},\dots ,r_{n} \right)$ gewählt, so dass die Gleichung $r_{1}+
        \dots + r_{n} = m$ erfüllt ist. 
\end{enumerate}
Geben Sie jeweils eine passende Formel an, die die Anzahl der möglichen
Ergebnisse für diese Zufallssituationen beschreibt. 

\problem{Kombinatorik. Passwörter.} Wir betrachten die Menge
$\cA = \left\{ a, b, \dots, z, 0, \dots, 9 \right\}$ als gegebenen Zeichensatz.
\begin{enumerate}
    \item Wie viele voneinander verschiedene Passwörter der Länge 8 können
        aus $\cA$ gebildet werden?
    \item Betrachten wir nun die Passwörter aus dem obigen Zeichensatz, die an der
        letzen Stelle eine Ziffer aufweisen und sonst aus lauter Buchstaben bestehen.
        Wie viele solche Passwörter gibt es?
    \item Vergleichen Sie die Größenordnungen der Mächtigkeiten der obigen
        Passwortmengen. 
\end{enumerate}

\solution 
\begin{enumerate}
    \item Die Menge $\cA$ enthält $36$ Elemente. Nachdem in einem Passwort die
        Reihenfolge der Zeichen wichtig ist und die Zeichen mehrmals vorkommen
        können, ist die Anzahl der Passwörter gegeben durch die Anzahl der
        Variationen mit Zurücklegen, also durch
        \begin{equation}
            36^{8} = 2.821.109.907.456 \approx 10^{12.45}.
        \end{equation}
    \item Es gibt $26^7$ Passwörter mit Länge $7$, die nur aus Buchstaben bestehen. 
        Wenn wir an jedem solchen Passwort eine Ziffer am Ende hinzufügen, 
        erhalten wir $10\cdot 26^7$ Möglichkeiten. 
        \begin{equation}
            10\cdot 26^7 = 80.318.101.760 \approx 10^{10.9}.
        \end{equation}
    \item Die Mächtigkeiten der obigen Mengen sind also fast zwei Größenordnungen 
        auseinander.
\end{enumerate}



\problem{Kombinatorik. PINs und runs.} 
%In einer Folge $\left( a_1, a_2, a_3, \dots \right)$ wird eine Teilfolge $(a,
%a, \dots, a)$, die aus $n$-facher Wiederholung eines Elements $a$ gebildet
%wird, als ein $n$-run bezeichnet.  Zum Beispiel hat die Folge $(0, 0, 1, 1, 1,
%0)$ acht runs, nämlich einen $2$-run $(0,0)$ und einen $3$-run $(1,1,1)$, sowie
%sechs $1$-runs.
In einem $k$-stelligen PIN $(a_1,a_2,\dots ,a_k)$ wird eine $n$-fache
Wiederholung $(a,a,\dots ,a)$ eines Elementes $a$ mit $n\geq 2$ als ein $n$-run
bezeichnet.  Zum Beispiel hat die Folge $(0, 0, 1, 1, 1, 0)$ vier runs, nämlich
einen $2$-run $(0,0)$, zwei $2$-runs $(1,1)$, sowie einen $3$-run $(1,1,1)$.

Betrachten wir nun die Menge der 4-stelligen PINs, die aus den Ziffern $\left\{
0,1, \dots, 9 \right\}$ gebildet werden können. 
\begin{enumerate}
    \item Wie viele solche PINs gibt es?
    \item Wie viele PINs gibt es, die keine runs enthalten?
    \item Vergleichen Sie die Größenordnungen der Mächtigkeiten der obigen
        PIN-Mengen. 
\end{enumerate}
\textbf{Zusatz:} Führen Sie die obige Berechnung für die $5$-stelligen PINs durch.

\solution
\begin{enumerate}
    \item Es gibt $10^4$ solche PINs.
    \item Um die Anzahl der PINs ohne runs zu erhalten, zählen wir alle
        möglichen PINs mit runs auf. Erste Spalte gibt die Form des PINs,
        zweite die Anzahl solcher PINs und dritte die Anzahl der symmetrischen
        Fälle.
\begin{lstlisting}
----    10          1
---*    10 9        2
--==    10 9        1
--**    10 9 9      2
*--*    10 9 9      1
\end{lstlisting}
        Das sind insgesamt $2710$ Fälle. Es gibt also $7290$ PINs ohne runs.
    \item Wenn wir PINs mit runs ausschliessen, reduziert sich unser pool
        möglicher PINs um ein Viertel. 
\end{enumerate}


\problem{Laplacesche Modelle. Zwei Würfel.}
Wie groß ist die Wahrscheinlichkeit dafür, beim Werfen von zwei Würfeln eine
Augensumme zu erzielen, die größer oder gleich 10 ist?

\solution Wir lösen die Aufgabe einmal unter der Annahme, dass die Würfel
unterschieden werden können und einmal ohne diese Annahme. 
\begin{enumerate}
    \item Wenn wir zwei wohlunterscheidbare Würfel werfen, ist die
        $36$-elementige Ergebnismenge gegeben durch $\Omega = \left\{ (i,j) :
        i,j \in \left\{ 1, \dots, 6 \right\} \right\}$. Die günstigen Fälle
        sind 
        \begin{equation}
            (4,6), (5,5), (5,6), (6,4), (6,5), (6,6).
        \end{equation}
        Nachdem es sich hier um ein Laplace-Modell handelt, ist die
        Wahrscheinlichkeit, dass die Summe größer als $10$ ist, gleich
        $\frac{6}{36}= \frac{1}{6}$.

    \item In Falle der nichtunterscheidbaren Würfel ist das kein
        Laplace-Experiment mehr. Deswegen muss geeignetes
        Wahrscheinlichkeitsmaß $P$ betrachtet werden. 
\end{enumerate}


\problem{Kombinatorik. Würfel. 4 vs.~24.}
Was ist wahrscheinlicher:
\begin{enumerate}
    \item Beim Werfen von vier Würfeln auf wenigstens einem eine Sechs zu
        erzielen, oder
    \item bei 24 Würfen von zwei Würfeln wenigstens einmal zwei Sechsen zu
        erhalten?
\end{enumerate}

\solution
\begin{enumerate}
    \item Das Betrachen des komplementären Ereignisses liefert
        \begin{equation*}
            1 - \frac{5^4}{6^4}.
        \end{equation*}
    \item Ein äquivalentes Model ist, $24$ mal einen $36$-Würfel werfen. Die
        Wahrscheinlichkeit wenigstens ein mal die Zahl $36$ zu erhalten,
        berechnen wir indem wir das komplementäre Ereignis betrachten:
        \begin{equation*} 
            1 - \frac{35^{24}}{36^{24}}.  
        \end{equation*}
    \item Was ist größer? $\frac{5^4}{6^4} = 0.4822530864$ und
        $\frac{35^{24}}{36^{24}} = 0.5085961239$.

%        Berechnung ohne Taschenrechner: 
%        \begin{align*}
%            1 - \frac{5^{4}}{6^{4}} > 1 - \frac{ {35}^{24} }{ {36}^{24}} & \iff 
%            \frac{5^{4}}{6^{4}} < \frac{ {35}^{24} }{ {36}^{24}} \\
%            & \iff \frac{5}{6} < \frac{ {35}^{6} }{ {36}^{6}}.
%        \end{align*}
%        Es gilt aber
%        \begin{equation*}
%            \frac{ {35}^{6} }{ {36}^{6}} < 
%        \end{equation*}
\end{enumerate}




\problem{Kombinatorik. Single choice test.} Bei einem single choice test
werden $n$ Fragen gestellt, wobei jeweils genau eine richtige Antwort aus $m$
Möglichkeiten gewählt werden soll. Der Test wird als bestanden angesehen, wenn
mindestens die Hälfte der Fragen richtig beantwortet wurden.

Wir testen die Prüfmethode indem wir zufällig jeweils eine Antwort bei jeder Frage
ankreuzen. 
\begin{enumerate}
    \item Finden Sie einen geeigneten Wahrscheinlichkeitsraum, der diese
        Zufallssituation \linebreak beschreibt.
    \item Wie hoch ist die Wahrscheinlichkeit, dass die zufällige Antwortwahl zum 
        Bestehen des Tests führt?
    \item Berechnen Sie die obige Wahrscheinlichkeit explizit für $n=25$ und $m=4$.
\end{enumerate}

\solution Laplace-Modell. Anzahl der Möglichen: $| \Omega| = m^n$. 
Anzahl der Günstigen:
\begin{equation*}
    \binom{n}{ \lceil\frac{n}{2}\rceil }(m-1)^{n - \lceil\frac{n}{2}\rceil } +
    \binom{n}{   \lceil\frac{n}{2}\rceil +1 }(m-1)^{n -   \lceil\frac{n}{2}\rceil-1   } + \dots +
    \binom{n}{n} (m-1)^{n-n}. 
\end{equation*}
Hier zählen wir Anzahl der Tests mit $\lceil \frac{n}{2} \rceil, \lceil \frac{n}{2} \rceil +1,\dots $ richtigen
Antworten. Eine andere Möglichkeit ist die Anzahl der Test mit $0,1,2,\dots , \lfloor \frac{n}{2} \rfloor$ falschen
Antworten zu zählen:
\begin{align*}
    1 + n(m-1) + \binom{n}{2}(m-1)^{2} + \cdots + \binom{n}{ \lfloor \frac{n}{2} \rfloor } (m-1)^{\lfloor \frac{n}{2} \rfloor }.
\end{align*}
Die Wahrscheinlichkeit für das Bestehen der Prüfung beträgt also
\begin{align*}
    \sum_{i=\lceil \frac{n}{2} \rceil}^{n} \binom{n}{i} \frac{ \left( m-1 \right)^{n-i}  }{ m^n } = 
    \sum_{i=\lceil \frac{n}{2} \rceil}^{n} \binom{n}{i} \left( \frac{1}{m} \right)^{i} \left( 1-\frac{1}{m} \right)^{n-i}.
\end{align*}
Die letzte Formel kann auch direkt hergeleitet werden. 
Für den Fall $n=25$ und $m=4$, ist $\lceil \frac{n}{2} \rceil = 13$ und die Anzahl 
der günstigen Fälle gleich 
\begin{equation*}
    3794787166756 \approx 10^{12.5}.
\end{equation*}
Nachdem Anzahl der möglichen Fälle ist 
\begin{equation*}
    m^{n} = 4^{25} = 1125899906842624 \approx 10^{15.05},
\end{equation*}
ist die Wahrscheinlichkeit für eine zufällig bestandene Prüfung ungefähr gleich
$0.0033$.


\problem{Kombinatorik. Aufzug.} In einem Aufzug eines $10$-stöckigen Gebäudes
befinden sich $7$ Personen. Wie hoch ist die Wahrscheinlichkeit dafür, dass
alle Personen auf verschiedenen Stockwerken aussteigen?

\solution
\begin{equation*}
    \frac{V^{\text{o.W.}}_{10,7}}{V^{\text{m.W.}}_{10,7}} = \frac{10\cdot \dots 4}{10^{7}} = 0,06048.
\end{equation*}

\problem{Kombinatorik. Karten. $6$ aus $52$.} Aus einem gut gemischten
Spielkartensatz bestehend aus $52$ Karten werden zufällig $6$ Karten gezogen.
Wie hoch ist die Wahrscheinlichkeit dafür, dass sich unter den gezogenen Karten
schwarze und rote Karten befinden?

\solution
Wahrscheinlichkeit dafür, dass nur rote Karten gezogen werden ist 
\begin{equation*}
    p = \frac{\binom{26}6}{\binom{52}{6}}.
\end{equation*}
Daher ist die gesuchte Wahrscheinlichkeit $1-2p$. Man kann die Zufallssituation
auch mit Hilfe der geordneten Ziehungen modellieren. Das Aufzeigen dieser
Tatsache wird sicher zum besseren Verständnis der Materie beitragen.

\problem{Kombinatorik. Karten. $13$ aus $52$.} Aus einem gut gemischten
Spielkartensatz bestehend aus $52$ Karten werden zufällig $13$ Karten gezogen.
Berechnen Sie die Wahrscheinlichkeiten der folgenden Ereignisse:
\begin{enumerate}
    \item Es wurden alle Kartenwerte (d.h.\ A,2,3,4,5,6,7,8,9,10,B,D,K) gezogen.
    \item Es wurden $5$ Piks, $4$ Herzen, $3$ Treffs und ein Karo gezogen.
    \item Es wurden $5$ Karten der Farbe $A$, $4$ Karten der Farbe $B$, $3$
        Karten der Farbe $C$ und eine Karte der Farbe $D$ gezogen. Wir nehmen
        an, dass die Farben paarweise verschieden sind. 
%        Diese Zusammenstellung notieren wir als $\left\langle 5,4,3,1
%        \right\rangle$.
%    \item Es wurde eine Zusammenstellung der Form $\left\langle 5,3,3,2 \right\rangle$ gezogen.
%    \item Es wurde eine Zusammenstellung der Form $\left\langle 4,4,4,1 \right\rangle$ gezogen.
\end{enumerate}

\solution
\begin{enumerate}
    \item \[\frac{4^{13} \cdot 13!}{52 \cdots (52-13)} = \frac{4^{13}}{\binom{52}{13}}.\] 
        Alternativ, wählen wir die erste Karte aus $52$, die zweite aus $48$,
        usw. Das ergibt \[\frac{52\cdot 48 \cdots 4}{\binom{52}{13}}.\] Nachdem
        jeder Term im Nenner durch $4$ teilbar ist, erhalten wir wieder die
        obige Formel. 
    \item Es gibt $\binom{13}{5}$ verschiedene $5$-Pik Mengen. Daher ist die gesuchte
        Wahrscheinlichkeit
        \begin{align*}
            p &= 13\cdots(13-5) 13\cdots (13-4) 13\cdots(13-4) 13 \frac{13!}{5! 4! 3! 1!} / (52\cdots (52-13)) \\
            &= \frac{\binom{13}{5}  \binom{13}{4}   \binom{13}{3}   \binom{13}{1} }{ \binom{52}{13}}.
        \end{align*}
    \item $4! p$. 
\end{enumerate}

\problem{Kombinatorik. $2$ Kartenspieler.} Aus einem gut gemischten Spielkartensatz
bestehend aus $52$ Karten werden je $26$ Karten an $2$ Spieler verteilt. Berechnen
Sie folgende Wahrscheinlichkeiten:
\begin{enumerate}
    \item Der erste Spieler bekommt alle Asse und alle Könige. 
    \item Der erste Spieler bekommt $7$ Piks ($\spadesuit$), $8$ Herzen
        ($\heartsuit$), $5$ Treffs ($\clubsuit$) und $6$ Karos
        ($\diamondsuit$).
    \item Der zweite Spiele bekommt alle Farben ($\clubsuit$ $\spadesuit$
        $\heartsuit$ $\diamondsuit$) von genau $5$ Werten. 
\end{enumerate}
Begründen Sie Ihre Lösung.

\solution
Wir nehmen an, dass die Ordnung in welcher die Karten an die Spieler verteilt
werden, keine Rolle spielt. Die Kartenverteilungen sind alle
gleichwahrscheinlich, es handelt sich also um ein Laplace Modell. Die Anzahl
der möglichen Verteilungen ist dann in allen Fällen gleich $\binom{52}{26}$. 
\begin{enumerate}
    \item Alle Asse und alle Könige ($8$ Karten) können nur auf eine Art an den
        ersten Spieler verteilt werden. Zusätzlich erhält der erste Spieler 
        $26-8=18$ Karten aus den restlichen $52-8=44$ Karten. Anzahl der möglichen
        Verteilungen ist also $\binom{44}{18}$. Die Wahrscheinlichkeit für das 
        beschriebene Ereignis beträgt
        \begin{equation*}
            \frac{\binom{44}{18}}{\binom{52}{26}}.
        \end{equation*}
    \item Nachdem die Reihenfolge der Kartenverteilung keine Rolle spielt,
        verteilen wir zuerst die $7$ Piks an den ersten Spieler. Nachdem es
        insgesammt $13$ verschiedene Piks in dem Spielkartensatz gibt, kann das
        auch $\binom{13}{7}$ verschiedene Arten erfolgen. Anschliessend erhält
        der erste Spieler $8$ Herzen -- dies kann auf $\binom{13}{8}$ Arten
        geschehen, danach $5$ Treffs -- $\binom{13}{5}$ Möglichkeiten, und
        schliesslich $6$ Karos -- $\binom{13}{6}$ Möglichkeiten. Insgesammt
        gibt es $\binom{13}{7} \binom{13}{8} \binom{13}{5} \binom{13}{6}$
        verschiedene Verteilungen, die die geförderte Bedingung erfüllen. Die
        Wahrscheinlichkeit für das beschriebene Ereignis beträgt
        \begin{align*}
            \frac{ \binom{13}{7} \binom{13}{8} \binom{13}{5} \binom{13}{6} }{\binom{52}{26}}.
        \end{align*}
    \item Wir wählen zuerst $5$ Werte aus $13$ -- $\binom{13}{5}$ Möglichkeiten
        -- und verteilen alle $5 \times 4 = 20$ Karten mit diesen Werten an den
        zweiten Spieler. Die restlichen $26 - 20 = 6$ Karten verteilen wir aus
        den übrigen $52-20=32$ Karten. Dabei kommen aber $8\, \binom{28}{2}$
        solche Verteilungen vor, in denen ein der $8$ übriggebliebenen Werte $4$
        mal vertreten ist. Die Wahrscheinlichkeit für das beschriebene Ereignis
        beträgt also
        \begin{align*}
            \frac{\binom{13}{5} \left( \binom{32}{6} - 8 \binom{28}{2} \right)}{\binom{52}{26}}.
        \end{align*}
\end{enumerate}

\problem{Kombinatorik. $4$ Kartenspieler.} Aus einem gut gemischten Spielkartensatz
bestehend aus $52$ Karten werden je $13$ Karten an $4$ Spieler verteilt.
Berechnen Sie folgende Wahrscheinlichkeiten:
\begin{enumerate}
    \item Jeder Spieler bekommt einen Ass. 
%    \item Jeder Spieler bekommt mindestens einen Pik. 
    \item Erster Spieler bekommt genau $7$ Karten einer Farbe.
    \item Erster Spieler bekommt genau $6$ Karten einer Farbe.
\end{enumerate}
%Erster Spieler hat ``zufällig'' einen Ass bei dem zweiten Spieler gesehen. Wie
%hoch ist die Wahrscheinlichkeit dafür, dass der erste Spieler keinen Ass
%bekommen hat?

\solution 
\begin{enumerate}
    \item Wir zählen die Mengen und vernachlässigen dadurch die Reihenfolge der Karten.
        Die Anzahl der möglichen Kartenverteilungen an vier Spieler ist 
        \begin{equation*}
            \binom{52}{13}\binom{39}{13}\binom{26}{13}\binom{13}{13}.
        \end{equation*}
        Die Anzahl der günstigen Verteilungen: Der erste Spieler bekommt einen beliebigen
        Ass und $12$ Karten die keine Asse sind. Der zweite Spieler bekommt einen der drei
        übriggebliebenen Asse und $12$ Karten die keine Asse sind. Diese Überlegung liefert
        \begin{equation*}
            4 \binom{48}{12} 3 \binom{36}{12} 2 \binom{24}{12} 1 \binom{12}{12}.
        \end{equation*}
        Die gesuchte Wahrscheinlichkeit ist demnach
        \begin{equation*}
            \frac{4! 48!  \left( 13! \right)^4}{ 52! \left( 12! \right)^4 }. 
        \end{equation*}
%    \item Lösung mit Inklusions/Exclusionsprinzip
    \item Wahrscheinlichkeit für $7$ Karten einer Farbe:
        \begin{equation*}
            \frac{4 \binom{13}{7} \binom{39}{6}}{ \binom{52}{13}}.
        \end{equation*}
    \item Anzahl der Günstigen wie oben. Anzahl der Möglichen:
        \begin{equation*}
            4 \binom{13}{6} \left[ \binom{39}{7} - 3 \binom{13}{6} \binom{26}{1}  \right].
        \end{equation*}
\end{enumerate}

\problem{Gruppeneinteilung bei Fußball WM.} Bei einer Fußballweltmeisterschaft
werden $32$ Fußballteams zufällig in $8$ Gruppen je $4$ Teams eingeteilt.  Wir
nehmen an, dass alle Gruppeneinteilungen gleichwahrscheinlich sind.  Die
Gruppenbezeichnungen wie "`A"', "`B"', usw.\ werden nach erfolgter Einteilung
vergeben und spielen hier keine Rolle.

%\begin{enumerate}
%    \item 
        Wie viele mögliche Gruppeneinteilungen gibt es?
%    \item $13$ Europäische Mannschaften nehmen an dieser WM teil. Berechnen
%        Sie die Wahrscheinlichkeit dafür, dass es mindestens eine Gruppe gibt
%        zu der mindestens $3$ Europäische Mannschaften eingeteilt wurden.
%    \item Berechnen Sie die Wahrscheinlichkeit dafür, dass die $7$ Führenden
%        der FIFA-Weltrangliste zwei Gruppen zugeteilt werden.
%\end{enumerate}

\solution
    \begin{enumerate}
        \item Die Anzahl der möglichen Gruppeneinteilungen beträgt
            \begin{align*}
                M = \binom{32}{4} \binom{28}{4} \dots \binom{8}{4} \binom{4}{4} / 8!.
            \end{align*}
%        \item Die Anzahl der günstigen Einteilungen beträgt
%            \begin{align*}
%                G_{b} = \left[ \binom{13}{3} \binom{32-13}{1} \right] \binom{28}{4} \dots \binom{8}{4}.
%            \end{align*}
%        \item Die Anzahl der günstigen Einteilungen ist hier
%            \begin{align*}
%                G_{c} = \binom{7}{4} \left[ \binom{3}{3} \binom{25}{1} \right] \binom{24}{4} \dots \binom{8}{4}. 
%            \end{align*}
    \end{enumerate}

%% old version
%\problem{Gruppeneinteilung bei Fußball WM.} Bei der Fußballweltmeisterschaften
%werden $32$ Fußballteams zufällig in $8$ Gruppen je $4$ Teams eingeteilt. Dafür
%wird ein kompliziertes Verfahren angewendet, um sicherzustellen dass
%verschiedene unerwünschte Aufteilungen nicht auftreten können. In dieser
%Aufgabe nehmen wir an, dass alle Gruppeneinteilungen gleichwahrscheinlich sind,
%und zeigen, dass dieses Modell unerwünschte Eigenschaften hat. 
%\begin{enumerate}
%    \item Wie viele mögliche Gruppeneinteilungen gibt es?
%    \item Berechnen Sie die Wahrscheinlichkeit dafür, dass die $7$ Führenden
%        der FIFA-Weltrangliste zwei Gruppen zugeteilt werden.
%    \item Berechnen Sie die Wahrscheinlichkeit dafür, dass es mindestens eine
%        Gruppe gibt zu der mindestens $3$ Europäische Mannschaften eingeteilt
%        wurden.
%\end{enumerate}
%
%\solution
%\begin{enumerate}
%    \item $\frac{\binom{2}{2} \binom{30}{2}}{\binom{32}{4}}$.
%    \item $\binom{32}{4}\binom{28}{4} \cdots \binom{4}{4}$.
%\end{enumerate}

\problem{Kombinatorik. Schachbrett.} Auf ein Schachbrett (entspricht einer 
$8\times 8$-Matrix) werden zufällig $8$ Türme gestellt. 
\begin{enumerate}
    \item Wie groß ist die Wahrscheinlichkeit dafür, dass sich die Türme nicht
        sehen können, d.\,h.\ in jeder Zeile und jeder Spalte jeweils genau ein
        Turm steht?
    \item Wie groß ist die Wahrscheinlichkeit dafür, dass in jeder Zeile jeweils
        genau ein Turm steht?
\end{enumerate}

\solution
\begin{enumerate}
    \item $\frac{8^2 7^2 \cdots 1^2}{ 64 \cdots 57} = \frac{8!}{ \binom{64}{8}}$.
    \item $\frac{8!\, 8^8}{64 \dots 57}$.
\end{enumerate}

\section{Bedingte Wahrscheinlichkeiten}


\problem{Bedingte Wahrscheinlichkeiten. Symmetrie.}
Seien ein Wahrscheinlichkeitsraum $\left( \Omega, \cA, \bP \right)$ und die
Ereignisse $A,B\in \cA$ mit $\bP(A)>0$ und $\bP(B)>0$ gegeben.  Zeigen Sie die
Äquivalenz:
\begin{equation*}
    \bP(A | B ) > \bP(A) \quad \iff \quad \bP(B | A) > \bP(B).
\end{equation*}

\solution
\begin{equation*}
    P(A|B)>P(A) \iff \frac{ P(A \cap B) }{ P(B)} > P(A) \iff P(A \cap B) > P(A)P(B).
\end{equation*}


\problem{Bedingte Wahrscheinlichkeiten. 3 Würfel.} Wir werfen drei ideale Würfel. 
\begin{enumerate}
    \item Wie groß ist die Wahrscheinlichkeit, dass dabei kein Sechser geworfen wurde? 
    \item Wie groß ist die Wahrscheinlichkeit, dass dabei kein Sechser geworfen wurde, wenn
        bekannt ist, dass drei paarweise verschiedene Zahlen geworfen wurden. 
\end{enumerate}

\solution Die Menge aller Tripel aus Zahlen $1,\dots,6$ ist die Ergebnismenge.
Sei $A$ das Ereignis, dass keine Sechser geworfen wurden und $B$ das Ereignis,
dass geworfene Zahlen verschieden sind. Nun ist $P(A) = \frac{5^3}{6^3}$ und
$P(B)= \frac{6\cdot 5\cdot 4}{6^{3}}$. Mit $P(A \cap B) = \frac{5\cdot 4\cdot
3}{6^{3}}$ erhalten wir $P(A|B)=\frac{1}{2}$.


\problem{Bedingte Wahrscheinlichkeiten. 2 Münzen.} Wir werfen zwei ideale Münzen.
\begin{enumerate}
    \item Die erste Münze zeigt Kopf. Berechnen Sie die Wahrscheinlichkeit,
        dass beide Münzen Kopf zeigen.

    \item Eine der Münzen zeigt Kopf. Berechnen Sie die Wahrscheinlichkeit,
        dass beide Münzen Kopf zeigen.
\end{enumerate}

\solution Bezeichne mit $K_1$ bzw.\ $K_2$ das Ereignis, dass die erste
bzw.\ die zweite Münze Kopf zeigt. 
\begin{enumerate}
    \item \begin{equation*}
            P( K_1 \cap K_2 | K_1 ) = \frac{ P( K_1 \cap K_2 \cap K_1) }{ P(K_1)} = \frac{1}{2}.
        \end{equation*}

    \item \begin{equation*}
            P( K_1 \cap K_2 | K_1 \cup K_2 ) = \frac{ P( K_1 \cap K_2) }{ P(K_1 \cup K_2) }
            = \frac{1}{3}. 
        \end{equation*}
\end{enumerate}


\problem{Bedingte Wahrscheinlichkeit ist ein Wahrscheinlichkeitsmaß.}
Seien ein Wahrscheinlichkeitsraum $(\Omega, \fA, P)$ und ein Ereignis $B\in\fA$
mit $P(B)>0$ gegeben. 
\begin{enumerate}
    \item Zeigen Sie, dass die Funktion $Q: \fA \to \bR, A \mapsto P(A|B)$ ein
        Wahrscheinlichkeitsmaß auf $(\Omega, \fA)$ ist.
    \item Sei $\fA_B = \left\{ A \cap B : A\in \fA \right\}$. Zeigen Sie, dass
        die Funktion $R: \fA_B \to \bR, A \mapsto P(A|B)$ ein Wahrscheinlichkeitsmaß
        auf $\left( B, \fA_B \right)$ ist. 
    \item \textbf{Zusatz:} Zeigen Sie, dass $\fA_B$ alle Eigenschaften eines Ereignisfeldes ($\sigma$-Algebra) aufweist.
\end{enumerate}

\solution
\begin{enumerate}
    \item Es sollen die Axiome von Kologorov gelten: (A1) $0 \leq P(A) \leq 1$
        für alle $A\in \fA$, (A2) $P(\Omega)=1$, (A3) $P\left(
        \cup_{i=1}^{\infty} A_i \right) = \sum_{i=1}^{\infty} P(A_i)$ für
        paarweise disjunkte Mengen $(A_i)_{i\in \bN}$. 

        Nun ist $Q(A) = P( A | B) =  \frac{P(A \cap B)}{ P(B)}$.

    \item Ein Ereignisfeld erfüllt folgende Axiome: (1) $\Omega \in \fA$, (2)
        $A \in \fA \impl \bar A \in \fA$, (3) $A_1, A_2, \dots \in \fA \impl
        \cup_{i\in\bN} A_i \in \fA$. 
\end{enumerate}


\problem{Bedingte Wahrscheinlichkeiten. Urne mit Kugeln.} In einer Urne
befinden sich $w$ weiße Kugeln und $s$ schwarze Kugeln. Es wird eine Kugel aus
der Urne gezogen, beiseite gelegt und anschließend eine weitere Kugel gezogen.
Berechnen Sie die Wahrscheinlichkeiten der folgenden Ereignisse:
\begin{enumerate}
    \item Beide Kugeln sind weiß.
    \item Die erste Kugel ist weiß und die zweite Kugel ist schwarz.
\end{enumerate}
Geben Sie die obigen Wahrscheinlichkeiten für $w=5$ und $s=4$ explizit an.

\solution Wir bezeichnen mit $W_1$ das Ereignis, dass eine weiße Kugel als
erste gezogen wird, und mit $W_2$ ein Ereignis, dass eine weiße Kugel als zweite
gezogen wird. Wir sind daran interessiert, die Wahrscheinlichkeit $P(W_1 \cap W_2)$ zu
berechnen. Wir wissen aber $P(W_1 \cap W_2) = P(W_2 | W_1) P(W_1)$ und
$P(W_1) = \frac{w}{w+s}$. Es gilt auch $P(W_2 | W_1) = \frac{w-1}{w-1+s}$.
Insgesamt ist also \begin{equation*}
    P(W_1 \cap W_2) = \frac{w(w-1)}{(w+s)(w-1+s) }.
\end{equation*}

Ähnlich bezeichnen wir mit $S_2$ das Ereignis, dass eine schwarze Kugel als zweite
gezogen wird. Damit ergibt sich
\begin{equation*}
    P(W_1 \cap S_2)= P(S_2 | W_1) P(W_1) = \frac{ w s }{(w+s)(w+s-1)}.
\end{equation*}

Für die oben angegebenen Werte sind die beiden Wahrscheinlichkeiten gleich
$\frac{5}{18}$.

\problem{Urnenmodell. $3$ Farben.} Eine Urne enthält $2$ weiße, $3$ rote und
$5$ schwarze Kugeln. Es wird eine Kugel gezogen, ihre Farbe notiert und wieder
in die Urne gelegt. Die Anzahl der Kugeln der gerade notierten Farbe wird
anschließend verdoppelt falls diese Anzahl ungerade ist. Die Prozedur wird
anschließend wiederholt.
\begin{enumerate}
    \item Wie groß ist die Wahrscheinlichkeit dafür, dass die zweite gezogene
        Kugel weiß ist, unter der Bedingung, dass die erste gezogene Kugel
        nicht weiß ist?

    \item Wie groß ist die Wahrscheinlichkeit dafür, dass die erste gezogene Kugel
        weiß ist, unter der Bedingung, dass die zweite gezogene Kugel rot ist?
\end{enumerate}

\solution
\begin{enumerate}
    \item $P(W_2 \,|\, \bar W_1) = P(W_1) = \frac{1}{5}$.
    \item \begin{align*}
            P(W_1 \,|\, R_2) &= \frac{  P( R_2 \,|\, W_1 ) P( W_1 ) }{ P(R_2) }. \\
            &= \frac{ P(W_1) P( R_2 \,|\, W_1)  }{ P(R_2\,|\, W_1) P(W_1) + P( R_2 \,|\, R_1 ) P(R_1) + P(R_2 \,|\, S_1) P(S_1)  } \\
            &= 0.2878.
        \end{align*}
\end{enumerate}


\problem{Multiplikatives Urnenmodell von P\'olya.} Eine Urne
enthält $2$ schwarze und $2$ weiße Kugeln. Es wird eine Kugel zufällig gezogen,
ihre Farbe notiert und wieder in die Urne gelegt. Die Anzahl der Kugeln der gerade notierten
Farbe wird anschließend verdoppelt und die Prozedur wiederholt. 
\begin{enumerate}
    \item Wie groß ist die Wahrscheinlichkeit, dass die zweite gezogene Kugel weiß ist?
    \item Wie groß ist die Wahrscheinlichkeit, dass die erste gezogene Kugel
        weiß ist, unter der Bedingung, dass die zweite gezogene Kugel weiß ist?
\end{enumerate}

\solution
Mit der üblichen Notation berechnen wir
\begin{align*}
    P(W_2) &= P( W_2 \,|\, W_1 ) P(W_1) + P(W_2 \,|\, S_1) P(S_1) = \frac{1}{2}\\
    P(W_1 \,|\, W_2 ) &= \frac{ P(W_2 \,|\, W_1 ) P( W_1 )  }{ P(W_2) } = \frac{2}{3}. 
\end{align*}

\problem{Bedingte Wahrscheinlichkeiten. Urnenmodell von P\'olya.} Eine Urne
enthält $s$ schwarze und $w$ weiße Kugeln. Es wird eine Kugel zufällig gezogen,
ihre Farbe notiert und die Kugel gemeinsam mit $d$ weiteren Kugeln von der
gleichen Farbe in die Urne gelegt. Die Prozedur wird anschließend wiederholt. 
\begin{enumerate}
    \item Wie groß ist die Wahrscheinlichkeit, dass die zweite gezogene Kugel weiß ist?
    \item Wie groß ist die Wahrscheinlichkeit, dass die erste gezogene Kugel
        weiß ist, unter der Bedingung, dass die zweite gezogene Kugel weiß ist?
\end{enumerate}

\solution
\begin{enumerate}
    \item \begin{align*}
            P(W_2) &= P(W_2 | W_1) P(W_1) + P(W_2 | S_1) P(S_1) \\
            &= \frac{w+d}{w+s+d}\ \frac{w}{w+s} + \frac{w}{w+s+d}\ \frac{s}{w+s} = \frac{w}{w+s}.
        \end{align*}

    \item \begin{align*}
            P(W_1 | W_2 ) &= \frac{ P(W_2 | W_1) P(W_1)  }{ P(W_2) } = \frac{w+d}{w+s+d}.
        \end{align*}
        
\end{enumerate}


\problem{Modifiziertes Urnenmodell von P\'olya.} Eine Urne enthält $s=2$ schwarze und
$w=3$ weiße Kugeln. Es wird eine Kugel zufällig gezogen und ihre Farbe notiert.
Falls eine weiße Kugel gezogen wurde, wird diese beiseite gelegt. Falls eine
schwarze Kugel gezogen wurde, wird diese zusammen mit einer weiteren schwarzen
Kugel in die Urne gelegt. Die Prozedur wird anschließend wiederholt.
\begin{enumerate}
    \item Wie groß ist die Wahrscheinlichkeit, dass die zweite gezogene Kugel
        weiß ist?
    \item Wie groß ist die Wahrscheinlichkeit, dass die erste gezogene Kugel
        weiß ist, unter der Bedingung, dass die zweite gezogene Kugel weiß ist?
\end{enumerate}

\solution
Wir bezeichnen mit $W_i$ bzw.\ mit $S_i$ das Ereignis, dass bei der $i$-ten
Ziehung eine weiße bzw.\ eine schwarze Kugel gezogen wird.
\begin{enumerate}
    \item \begin{align*}
            P(W_2) &= P(W_2 \,|\, W_1)P(W_1) + P(W_2 \,|\, S_1)P(S_1) \\
            &= \frac{w-1}{s+w-1} \frac{w}{s+w} + \frac{w}{s+1+w} \frac{s}{w+s} = \frac{1}{2}. 
        \end{align*}
    \item \begin{align*}
            P( W_1 \,|\, W_2 ) &= \frac{P(W_2 \,|\, W_1) P(W_1)}{P(W_2)} = 
            \frac{1}{P(W_2)} \frac{ w-1 }{w-1+s}\frac{w}{s+w} = \frac{3}{5}. 
        \end{align*}
\end{enumerate}



\problem{Bedingte Wahrscheinlichkeiten. Unfaire Münzen.} 
In einem Wurfexperiment werden $n$ Münzen verwendet. Davon sind $k$ Münzen
symmetrisch und zeigen Kopf mit Wahrscheinlichkeit $1/2$. Die restlichen $n-k$
Münzen zeigen Kopf mit Wahrscheinlichkeit $1/3$. 
\begin{enumerate}
    \item Es wurde zufällig eine Münze gewählt und geworfen: Kopf. Wie groß ist 
        die Wahrscheinlichkeit, dass eine asymmetrische Münze gewählt wurde?
    \item Eine zufällig gewählte Münze wird zweimal geworfen und in beiden 
        Würfen fällt Kopf. Wie groß ist die Wahrscheinlichkeit, dass eine asymmetrische
        Münze gewählt wurde. 
\end{enumerate}

\solution Wir bezeichnen mit $S$ bzw. $A$ das Ereignis, dass eine
symmetrische bzw. asymmetrische Münze gewählt wurde. Es gilt $P(S) = \frac{k}{n}$
und $P(A)=\frac{n-k}{n}$. Sei $K$ das Ereignis, dass mit einer festen zufällig
gewählten Münze Kopf geworfen wird. Wir suchen $P(A | K)$. Es gilt aber nach dem
Satz von Bayes
\begin{equation*}
    P( A | K) = \frac{ P( K | A) P(A) }{ P(K | A) P(A) + P(K | S) P(S) }
    = \frac{\frac{1}{3} \frac{n-k}{n} }{ \frac{n-k}{3 n} + \frac{1}{2} \frac{k}{n}} 
    = \frac{2(n-k)}{2n + k}.
\end{equation*}

Wenn zwei Münzen geworfen werden, müssen wir das Ereignis $K$ durch das Ereignis $K_2$ ersetzen. 
$K_2$ tritt ein wenn zwei Köpfe nacheinander geworfen werden. Dies ergibt
\begin{align*}
    P\left( A | K_{2} \right) = \frac{4(n-k)}{4n + 5k}. 
\end{align*}




\problem{Bedingte Wahrscheinlichkeiten. Urne mit Loch.} In einer Urne befinden
sich $s$ schwarze und $w$ weiße Kugeln. Während die Urne transportiert wird,
gehen $d$ Kugeln verloren. Es wird eine Kugel zufällig aus der Urne gezogen.
\begin{enumerate}
    \item Angenommen $d=1$. Wie groß ist die Wahrscheinlichkeit, dass die
        gezogene Kugel weiß ist?
    \item Nun ist $d<s$ und $d<w$. Wie groß ist die Wahrscheinlichkeit, dass die
        gezogene Kugel weiß ist?
\end{enumerate}

\solution
\begin{enumerate}
    \item Im Fall $d=1$ können wir annehmen, dass eine Kugel entnommen wurde und
        den Satz von der vollständigen Wahrscheinlichkeit verwenden. 
        \begin{equation*}
            P(W_2) = P(W_2 | W_1 ) P(W_2 ) + P(W_2 | S_1) P(S_1).
        \end{equation*}

    \item Alle Arten auf die $i$ weiße und $d-i$ schwarze Kugeln für $i\in
        {0,\dots, d}$ verloren gehen können, sind gleichwahrscheinlich.
        Bezeichnen wir dieses Ereignis als $Z_i$.  Daher haben wir
        \begin{align*}
            P(W) &= \sum_{i=0}^{d} P(W | Z_i ) P(Z_i) \\
            &= \sum_{i=0}^{d} \frac{ w -i}{w+s-i} P(Z_i).
        \end{align*}
        Nun ist 
        \begin{align*}
            P(Z_i) &= \binom{d}{i} P( W_1\cap \dots\cap W_i\cap S_{i+1}\cap \dots\cap S_d ) \\
            &= \binom{d}{i} P(W_1) P(W_2 | W_1) \dots P( S_k | W_1\cap \dots\cap S_{i+1}\cap \dots\cap S_{d-1}) \\
            &= \binom{d}{i} \frac{w}{w+s}\frac{w-1}{w+s-1}\dots \frac{w-i+1}{s+w-i-1}
            \frac{s}{s+w-i}\dots\frac{s-(d-i)+1}{s+w -d+1} \\
            &= \binom{d}{i} \frac{ (w)_{i} (s)_{d-i} }{ (w+s)_{d}}.
        \end{align*}
\end{enumerate}




\problem{Bedingte Wahrscheinlichkeiten. Urnen mit unbekanntem Inhalt.} Es
sind zwei Urnen mit Kugeln gegeben. Eine der Urnen enthält $k_1$ weiße und
$n_1$ schwarze Kugeln und die andere enthält $k_2$ weiße und $n_2$ schwarze
Kugeln. Die Urnen sind äußerlich nicht unterscheidbar.
\begin{enumerate}
    \item Geben Sie einen Ziehungsprozedur an, die die Wahrscheinlichkeit, dass
        zwei weiße Kugeln gezogen werden unter allen Ziehungsprozeduren maximiert.

    \item Seien nun $k_1=3, n_1=7, k_2=2$ und $n_2=8$. Wie groß ist die
        Wahrscheinlichkeit, dass unter Verwendung der von Ihnen gefundenen
        Prozedur zwei weiße Kugeln gezogen werden?
\end{enumerate}

\solution
Nachdem die Urnen äußerlich nicht unterscheidbar sind, können wir eine
Urne nur zufällig und nur mit Wahrscheinlichkeit $1/2$ wählen. Wir wählen 
also eine Urne zufällig und ziehen eine Kugel. Es kann nur auf folgende Weisen
weiterfahren werden:

\begin{enumerate}
    \item Für die zweite Ziehung wählen wir die Urnen zufällig.
    \item Zweite Kugel ziehen wir aus der anderen Urne. 
    \item Zweite Kugel ziehen wir aus derselben Urne.
    \item Zweite Kugel ziehen wir aus derselben Urne falls die erste weiß war.
    \item Zweite Kugel ziehen wir aus anderen Urne falls die erste weiß war.
\end{enumerate}
Bezeichnen wir mit $U_{ij}$, $i,j\in \left\{ 1,2 \right\}$ das Ereignis, dass
die $j$-te Urne bei der $i$-ten Ziehung gewählt wurde. Nun berechnen wir die 
Wahrscheinlichkeit, dass zwei weiße Kugel gezogen werden, also $P(W_1 \cap W_2)$.
\begin{align*}
    P_1 = P(W_1 \cap W_2) = 
\end{align*}


\problem{Bedingte Wahrscheinlichkeiten. Versicherungsunternehmen.} Ein
Versicherungsunternehmen versichert $n$ Autofahrer, davon $f$ Frauen und $m$
Männer. Die Wahrscheinlichkeit, dass innerhalb eines Jahres ein zufällig
gewählter männlicher Autofahrer einen Unfall hat, ist $\alpha$. Entsprechende
Wahrscheinlichkeit für einen weiblichen Autofahrer ist $\beta$. Die Unfälle in
verschiedenen Jahren sind unabhängig. Ein Autofahrer wird zufällig gewählt. 
Berechnen Sie folgende Wahrscheinlichkeiten:
\begin{enumerate}
    \item Wie groß ist die Wahrscheinlichkeit, dass der gewählte Autofahrer 
        innerhalb des nächsten Jahres einen Unfall hat?

    \item Wie groß ist die Wahrscheinlichkeit, dass der gewählte Autofahrer
        Unfälle in zwei aufeinanderfolgenden Jahren hat?

    \item Der gewählte Autofahrer hat im letzten Jahr einen Unfall gehabt. Wie
        groß ist die Wahrscheinlichkeit, dass der Autofahrer männlich ist?
\end{enumerate}

\solution
\begin{enumerate}
    \item $\frac{\alpha m + \beta f}{n}$.
    \item $\frac{\alpha^2 m + \beta^2 f}{n}$.
    \item $\frac{\alpha m}{\alpha m + \beta f}$.
\end{enumerate}



\problem{Bedingte Wahrscheinlichkeiten. Urnenmodell von P\'olya. II.} Eine Urne
enthält $s$ schwarze und $w$ weiße Kugeln. Es wird eine Kugel zufällig gezogen,
ihre Farbe notiert und die Kugel gemeinsam mit $d$ weiteren Kugeln von der
gleichen Farbe in die Urne gelegt. Die Prozedur wird anschließend wiederholt.
\begin{enumerate}
    \item Berechnen Sie die Wahrscheinlichkeit $p_{k,n}$, dass in einer Sequenz von $n$
        Ziehungen $k$ schwarze Kugeln gezogen werden.
    \item Bezeichne mit $S_n$ das Ereignis, dass bei der $n$-ten Ziehung
        schwarze Kugel gezogen wird. Zeigen Sie, dass $P(S_1)=P(S_n)$ für alle
        $n\in\bN$ gilt.
\end{enumerate}

\solution
\begin{enumerate}
    \item Es gibt $\binom{n}{k}$ mögliche Ziehungen von $n$ Kugeln, die $k$
        schwarze Kugeln beinhalten. Alle diese Ziehungen sind
        gleichwahrscheinlich, denn eine Ziehung $(I_1, \dots, I_n)$ kann in
        eine andere durch einfache Vertauschungen der Form $(I_1, \dots,
        I_{k}, I_{k+1}, \dots, I_n) \to \left( I_1, \dots, I_{k+1}, I_{k},
        \dots, I_n \right)$ überführt werden. Diese Transformationen lassen
        die Wahrscheinlichkeiten der Ziehungen unverändert. Man kann noch 
        genauer Argumentieren und zeigen
        \begin{equation*}
            P( I_1, \dots, S_k, W_{k+1}, \dots, I_n) = 
            P( I_1, \dots, W_{k}, S_{k+1}, \dots, I_n).
        \end{equation*}
        Mit Hilfe der Multiplikationsregel erhalten wir
        \begin{align*}
            P( I_1, \dots, S_k, W_{k+1}, \dots, I_n) = \\
            P(I_1)\dots P(S_k | I_{k-1}\cap\dots\cap I_1) 
            P( W_{k+1} | S_k\cap I_{k-1}\dots) \dots P(I_n | I_{n-1}\cap \dots)
        \end{align*}
        Es sind nur die Wahrscheinlichkeiten in der Mitte des obigen Produkts
        interessant, da die übrigen von der Ersetzung $(S_k\cap W_{k+1} ) \to
        (W_k \cap S_{k+1})$ nicht beeinflusst werden. Wir definieren 
        \begin{equation*}
            Q(A) = P(A | I_{k-1} \cap \dots\cap I_1)
        \end{equation*}
        und zeigen
        \begin{equation*}
            Q( S_k ) Q(W_{k+1} | S_k) = Q(W_k) Q( S_{k+1} | W_k)
        \end{equation*}
        indem wir die konkrete Zahlen einsetzen.

        Demzufolge können wir Ziehungen betrachten in denen zuerst $k$
        schwarze Kugeln und dann die $n-k$ weiße Kugeln gezogen werden.
        Wir verwenden die Multiplikationsregel:
        \begin{align*}
            P( S_1 \cap \dots \cap S_k \cap W_{k+1} \cap \dots \cap W_{n} ) = \\
            P(S_1) \dots P(S_k | S_1\cap\dots\cap S_{k-1})\dots
            P(W_{n} | S_1\cap \dots\cap W_{n-1} ) = \\
            \frac{s}{s+w} \frac{s+d}{s+w+d}\dots\frac{s+(k-1)d}{s+w+(k-1)d}
            \frac{w}{s+w+ kd}\dots\frac{w + (n-k-1)d}{ w+s+(n-1)d}
        \end{align*}
        Wir erhalten also insgesamt
        \begin{equation*}
            p_{k,n} = \binom{n}{k} P( S_1 \cap \dots \cap S_k \cap W_{k+1} \cap \dots \cap W_{n} ).
        \end{equation*}

    \item FEHLT!
\end{enumerate}


\problem{Bedingte bedingte Wahrscheinlichkeiten.} 
Seien ein Wahrscheinlichkeitsraum $(\Omega, \fA, P)$ und die Ereignisse
$A,B,C\in\fA$ mit $P(A)>0$, $P(B)>0$ und $P(C)>0$ gegeben. Wir definieren
$P(A|B|C)= Q(A|B)$ für ein Wahrscheinlichkeitsmaß $Q$ mit $Q(D)=P(D|C) \ \forall
D\in\fA$. Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item $P(A|B|C) = P(A| B \cap C) = P(A|C|B)$. 
    \item $P(A|B|C) = P(A| C)$ falls $C \subset B$.
\end{enumerate}


\problem{Elfmeterschießen.}  In einem einfachen Modell des Elfmeterschießens
wird nur eine Mannschaft betrachtet. Die Wahrscheinlichkeit beim ersten Schuss
einen Tor zu erzielen sei $p_{1}=0.8$. Ist in der Tat ein Tor gefallen, wird
die Treffwahrscheinlichkeit beim nächsten Versuch auf $p_2 = \sqrt{p_1}$
erhöht, andernfalls verringert sich diese Wahrscheinlichkeit und beträgt $p_2 =
p_{1}^{2}$. Der Vorgang wird wiederholt, wobei die Treffwahrscheinlichkeit beim
$i$-ten Schuss $p_{i}=\sqrt{p_{i-1}}$ beträgt, falls vorher ein Tor
gefallen ist, und sonst $p_i = p^{2}_{i-1}$.
\begin{enumerate}
    \item Wie groß ist die Wahrscheinlichkeit dafür, dass beim zweiten Versuch
        ein Tor fällt?

    \item Angenommen Sie haben den ersten Schuss nicht gesehen, wissen aber
        dass beim zweiten Versuch ein Tor gefallen ist. Unter dieser Bedingung,
        wie groß ist die Wahrscheinlichkeit, dass beim ersten Schuss ein Tor
        gefallen ist?
\end{enumerate}

\solution
$T_i$ bezeichne Treffer beim $i$-ten Versuch und $F_i = \bar T_i$. 
\begin{enumerate}
    \item $P(T_2) = P( T_2 | T_1) P(T_1) + P(T_2 | F_1) P(F_1) 
        = p_1(\sqrt{p_1}+p_1 -p_1^2) = 0.8435$.
    \item 
        \begin{align*}
            P( T_1 | T_2) &= \frac{ P(T_1) P( T_2 | T_1 )}{P(T_2)} = 0.8482.
        \end{align*}
\end{enumerate}


\problem{Schaltkreise. Qualitätskontrolle.} Bei einer Endkontrolle von
Schaltkreisen sollen nicht voll funktionsfähige Schaltkreise aussortiert
werden. Bekannt sei, dass im Mittel $20\%$ der erzeugten Schaltkreise nicht
voll funktionsfähig sind. Beim Prüfen werden $95\%$ dieser Schaltkreise
aussortiert, aber auch $1\%$ der einwandfreien.
\begin{enumerate}
    \item Mit welcher Wahrscheinlichkeit ist ein nicht aussortierter
        Schaltkreis voll funktionsfähig?
    \item Mit welcher Wahrscheinlichkeit ist ein aussortierter Schaltkreis
        einwandfrei?
\end{enumerate}

\solution 
Definiere die Ereignisse: $A$ ein Schaltkreis wird aussortiert, und $K$ ein Schaltkreis ist
nicht voll funktionsfähig (kaputt). Wir wissen
\begin{align*}
    \bP( A | K ) &= 0.95 \\
    \bP( A | \bar K ) &= 0.01.
\end{align*}
Daraus folgt
\begin{align*}
    \bP( \bar A | \bar K ) &= 0.99 \\
    \bP( \bar A | K ) &= 0.05 \\
    \bP( \bar K ) &= 0.8.
\end{align*}
Wir berechnen mit Hilfe des Satzes von Bayes:
\begin{enumerate}
    \item \begin{equation*}
            \bP\left( \bar K | \bar A \right) = \frac{\bP(\bar A | \bar K)\bP(\bar K)}
            {\bP(\bar A | \bar K)\bP(\bar K) + \bP(\bar A | K ) \bP(K)} = 0.9875311\cdots
        \end{equation*}
    \item \begin{equation*}
            \bP\left( \bar K | A \right) = 0.0404040\cdots
        \end{equation*}
\end{enumerate}


\section{Unabhängigkeit}

\problem{Disjunkt vs.\ unabhängig. Einfache Rechnungen.} 
Seien $(\Omega, \cA, P)$ ein Wahrscheinlichkeitsraum und Ereignisse $A, B, C
\in \cA$ mit $P(A)=\frac{1}{3}$, $P(B)=\frac{1}{4}$ und $P(C)=\frac{1}{6}$
gegeben. Seien darüber hinaus $A$ und $B$ stochastisch unabhängig, sowie $A\cup B$ 
disjunkt von $C$. Berechnen Sie folgende Wahrscheinlichkeiten:
\begin{enumerate}
    \item $P(A \cap B \cap C)$,
    \item $P(A \cup B \cup C)$,
    \item $P( ( \bar A \cup \bar B) \cap \bar C )$.
\end{enumerate}

\solution
\begin{enumerate}
    \item $P(A \cap B \cap C) = 0$.
    \item $P(A \cup B \cup C) = P(C) + 1 - (1 - P(A))(1-P(B)) = 2/3$.
    \item $P( ( \bar A \cup \bar B) \cap \bar C ) = 1 - P(C) - P(A)P(B) = 3/4$.
\end{enumerate}

\problem{Unvereinbar versus unabhängig. Einfache Rechnungen.} 
Gegeben seien  ein Wahrscheinlichkeitsraum $(\Omega, \sAlg{A}, P)$ und zuf\"allige Ereignisse $A, B, C
\in \sAlg{A}$  mit $P(A)=\frac{1}{3}$, $P(B)=\frac{1}{4}$ und $P(C)=\frac{1}{6}$. 
Weiterhin seien die Ereignisse $A$ und $B$ stochastisch unabhängig sowie die Ereignisse $A\cup B$
und $C$ unvereinbar. Berechnen Sie die folgenden Wahrscheinlichkeiten:
\begin{enumerate}
    \item $P(A \cap B \cap C)$,
    \item $P(A \cup B \cup C)$,
    \item $P( ( \bar A \cup \bar B) \cap \bar C )$.
\end{enumerate}

\solution
\begin{enumerate}
    \item $P(A \cap B \cap C) = 0$.
    \item $P(A \cup B \cup C) = P(C) + 1 - (1 - P(A))(1-P(B)) = 2/3$.
    \item $P( ( \bar A \cup \bar B) \cap \bar C ) = 1 - P(C) - P(A)P(B) = 3/4$.
\end{enumerate}

\problem{Unabhängigkeit. Einfache Rechnungen.}
Seien $(\Omega, \cA, P)$ ein Wahrscheinlichkeitsraum und die vollständig
stochastisch unabhängigen Ereignisse $A, B, C \in \cA$ mit $P(A)=\frac{1}{3}$,
$P(B)=\frac{1}{4}$ und $P(C)=\frac{1}{6}$ gegeben.
\begin{enumerate}
    \item Berechnen Sie $P(A \cup B)$.
    \item Berechnen Sie $P( A \cup \bar B \cup C)$.
\end{enumerate}

\solution
\begin{enumerate}
    \item $P(A \cup B) = \frac{1}{2}$. %= \frac{2}{3}$.
    \item $P(A \cup \bar B \cup C) = \frac{31}{36}$.  % \frac{124}{144}$.
\end{enumerate}

\problem{Unabhängigkeit. Disjunkte Ereignisse.} Seien $(\Omega, \cA, \bP)$ ein
Wahrscheinlichkeitsraum und die Ereignisse $A,B \in \cA$ gegeben. Zeigen Sie
folgende Aussagen:
\begin{enumerate}
    \item Für $A\cap B=\emptyset$ sind die Ereignisse $A$ und $B$ genau dann
        unabhängig, wenn $\bP(A)=0$ oder $\bP(B)=0$ gilt.

    \item Sind $A$ und $B$ unabhängig und ist $A \cup B = \Omega$, dann gilt
        $\bP(A)=1$ oder $\bP(B)=1$. 
\end{enumerate}


\problem{Unabhängigkeit. Gleichwahrscheinliche Ereignisse.} Seien $(\Omega,
\cA, \bP)$ ein Wahrscheinlichkeitsraum und $A_1,\dots, A_n \in\cA, n\in\bN$
vollständig stochastisch unabhängige Ereignisse mit $\bP(A_i)=p\in[0,1]\ \forall
i\in \left\{ 1,\dots,n \right\}$ gegeben. Berechnen Sie die
Wahrscheinlichkeiten der folgenden Ereignisse:
\begin{enumerate}
    \item Alle Ereignisse $A_1,\dots, A_n$ treten ein.
    \item Keines der Ereignisse $A_1, \dots, A_n$ tritt ein. 
    \item Genau eines der Ereignisse $A_1, \dots, A_n$ tritt ein. Warum 
        ist diese Wahrscheinlichkeit kleiner als $1$?
\end{enumerate}

\solution
\begin{enumerate}
    \item $\bP(A_1 \cap \dots \cap A_n) = p^n$.
    \item $\bP(\overline{A_1 \cup \dots \cup A_n}) = (1-p)^n$.
    \item \begin{align*}
            \bP( (A_1 \cap \bar A_2 \cap \dots \cap \bar A_n)  \cup\dots\cup (\bar A_1 \cap \dots \cap \bar A_{n-1} \cap A_n) &= \\
            \sum_{i=1}^{n} \bP(\bar A_1 \cap \dots \cap A_i \cap \dots \cap \bar A_n) &= n p (1-p)^{n-1}. 
        \end{align*}
        Dies ist tatsächlich eine Wahrscheinlichkeit, denn
        \begin{align*}
            1 &= (p + 1-p)^{n} = \sum_{i=0}^{n} \binom{n}{i} p^{i}(1-p)^{n-i}, &
            \binom{n}{1} &= n.
        \end{align*}
\end{enumerate}



\problem{Unabhängigkeit. Münzen.} Eine ideale Münze wird $n$ mal geworfen.
Bezeichnen wir als $K_i, i\in \left\{1,\dots,n\right\}$ das Ereignis, dass
beim $i$-ten Wurf Kopf fällt. 
\begin{enumerate}
    \item Finden Sie einen geeigneten Wahrscheinlichkeitsraum, der diese
        Zufallssituation beschreibt.
    \item Zeigen Sie, dass die Ereignisse $K_1,\dots, K_n$ vollständig 
        stochastisch unabhängig sind.
\end{enumerate}


\problem{Unabhängigkeit. Karten.} Aus einem gut gemischten Spielkartensatz
bestehend aus 52 Karten wird zufällig eine Karte gezogen. Bezeichnen wir mit
$A$ das Ereignis, dass ein Ass gezogen wird und mit $K$ das Ereignis, dass eine
Karte mit der Farbe Karo gezogen wird. Zeigen Sie, dass die Ereignisse $A$ und
$K$ unabhängig sind. 


\problem{Unabhängigkeit. Abschätzung mit der Exponentialfunktion.} Seien
$(\Omega, \cA, \bP)$ ein Wahrscheinlichkeitsraum und $A_1, \dots, A_n \in\cA$
vollständig stochastisch unabhängige Ereignisse  gegeben. Zeigen Sie:
\begin{equation*}
    \bP( \overline{A_1 \cup \dots \cup A_n} ) \leq \exp \left( - \sum_{i=1}^{n} \bP(A_i) \right).
\end{equation*}

\solution Benutze $1-x \leq e^{-x}$ und vollständige Induktion.


\problem{Unabhängigkeit. Notstromanlage.} 
Bei einer Notstromanlage werden von einem Dieselmotor drei gleichartige
Generatoren angetrieben. Die Wahrscheinlichkeit dafür, dass innerhalb einer
Woche ein Defekt am Motor bzw.\ am Generator auftritt, sei 0.1 bzw.~0.2. Die
einzelnen Teile des Systems fallen unabhängig voneinander aus.
\begin{enumerate}
\item Berechnen Sie die Wahrscheinlichkeit dafür, dass die Stomversorgung
im betrachteten Zeitraum vollständig ausfällt.
\item Mit welcher Wahrscheinlichkeit sind für den angegebenen Zeitraum
stets wenigstens $2/3$ der maximal möglichen Leistung verfügbar?
\end{enumerate}

\solution
\begin{enumerate}
    \item Sei $\bar M$ ein Ausfall des Motors und $\bar G_1, \bar G_2, \bar G_3$ die Ausfälle 
        der Generatoren. Der Ausfall der Notstromanlage hat die Wahrscheinlichkeit
        \begin{align*}
            P\left( \bar M \cup ( \bar G_1 \cap \bar G_2 \cap \bar G_3)   \right) &= 
            1 - P\left(  M \cap ( G_1 \cup G_2 \cup G_3 )  \right) \\
            &= 1 - P\left( M \right) \left( 1 - P(\bar G_1)P(\bar G_2)P(\bar G_3) \right) \\
            &= 1 - 0.9(1- {0.2}^{3}) = 0.1072.
        \end{align*}
    \item Hier ist die Wahrscheinlichkeit
            \[P\left( M \cap \left(  (G_1\cap G_2 \cap G_3) \cup (\bar G_1 \cap G_2 \cap G_3) \cap (G_1 \cap \bar G_2\cap G_3)
            \cup (G_1 \cap G_2 \cap \bar G_3)\right) \right) \] 
            gleich
            \begin{align*}
            P( M \cap G_1 \cap G_2 \cap G_3) +  & \\
            P( M \cap \bar G_1 \cap G_2 \cap G_3) + & \\
            P( M \cap G_1 \cap \bar G_2 \cap G_3) + & \\
            P( M \cap G_1 \cap G_2 \cap \bar G_3) &
            \end{align*}
            da die entsprechenden Ereignisse disjunkt sind. Wegen der Unabhängigkeit 
            ist diese Wahrscheinlichkeit gleich
            \begin{equation*}
                0.9\, {0.8}^{3} + 3\ 0.9\, {0.8}^2\, 0.2 = 0.8064.
            \end{equation*}
\end{enumerate}


\problem{Unabhängigkeit. Sicherungen.}
Die nachstehenden Abbildungen zeigen Systeme unabhängiger Sicherungen
(symbolisiert durch Kreise). Die Zahlen in den Kreisen geben die
Ausfallswahrscheinlichkeiten der jeweiligen Sicherungen innerhalb eines Jahres
an. Geben Sie die Wahrscheinlichkeiten dafür, dass innerhalb eines Jahres der
Strom von (s) nach (t) fließen kann.

\hspace*{7mm}
\unitlength0.9cm
\begin{picture}(16,5)
\thicklines
\put(0.5,4){(a)}
\put(1,2){s}
\put(6.8,2){t}
\put(1,2.5){\line(1,0){1.5}}
\put(2,1){\line(0,1){3}}
\put(2,1){\line(1,0){0.5}}
\put(2,4){\line(1,0){0.5}}
\multiput(3,1)(0,1.5){3}{\circle{1}\makebox(0,0){0.2}}
\multiput(3.5,1)(0,3){2}{\line(1,0){0.5}}
\put(3.5,2.5){\line(1,0){1.5}}
\put(4,1){\line(0,1){3}}
\put(5.5,2.5){\circle{1}\makebox(0,0){0.1}}
\put(6,2.5){\line(1,0){1}}
\put(8,2){s}
\put(14.8,2){t}
\put(7.5,4){(b)}
\multiput(8,2.5)(6,0){2}{\line(1,0){1}}
\multiput(9,1.5)(5,0){2}{\line(0,1){2}}
\multiput(9,3.5)(2,0){3}{\line(1,0){1}}
\multiput(9,1.5)(3,0){2}{\line(1,0){2}}
\put(10.5,3.5){\circle{1}\makebox(0,0){0.2}}
\put(12.5,3.5){\circle{1}\makebox(0,0){0.1}}
\put(11.5,1.5){\circle{1}\makebox(0,0){0.25}}
\end{picture}

\begin{center}
\unitlength0.8cm
\begin{picture}(17,5)
\thicklines
\put(0.5,4){(c)}
\put(0.1,2.5){s}
\put(16.8,1.5){t}
\put(0,3){\line(1,0){2}}
\multiput(2,2)(5,0){2}{\line(0,1){2}}
\multiput(1,1)(11,0){2}{\line(0,1){2}}
\multiput(2,4)(3,0){2}{\line(1,0){2}}
\multiput(2,2)(2,0){3}{\line(1,0){1}}
\multiput(7,3)(4,0){2}{\line(1,0){1}}
\put(1,1){\line(1,0){6}}
\put(8,1){\line(1,0){4}}
\put(4.5,4){\circle{1}\makebox(0,0){0.2}}
\put(7.5,1){\circle{1}\makebox(0,0){0.1}}
\multiput(3.5,2)(2,0){2}{\circle{1}\makebox(0,0){0.2}}
\multiput(9.5,2)(0,2){2}{\circle{1}\makebox(0,0){0.3}}
\multiput(14.5,1)(0,2){2}{\circle{1}\makebox(0,0){0.1}}
\multiput(8,2)(3,0){2}{\line(0,1){2}}
\multiput(8,2)(2,0){2}{\line(1,0){1}}
\multiput(8,4)(2,0){2}{\line(1,0){1}}
\multiput(12,2)(4,0){2}{\line(1,0){1}}
\multiput(13,1)(2,0){2}{\line(1,0){1}}
\multiput(13,3)(2,0){2}{\line(1,0){1}}
\multiput(13,1)(3,0){2}{\line(0,1){2}}
\end{picture}
\end{center}

\solution
\begin{enumerate}
    \item Analog zum Teil (a) von ``Unabhängigkeit. Notstromanlage.''
    \item Mit $\bar B_1$ und $\bar B_2$ bezeichnen wir die Ausfälle der 
        Sicherungen in dem oberen Teil des Diagramms. Analog bezeichnet
        $\bar C$ den Ausfall von dem unteren Teil. Die gesuchte Wahrscheinlichkeit
        ist
        \begin{align*}
            P( C \cup (B_1 \cap B_2 )) &= 1 - P( \bar C \cap ( \bar B_1 \cup \bar B_2)) \\
            &= 1 - P(\bar C)\left( 1- P(B_1)P(B_2) \right) \\
            &= 0.93.
        \end{align*}
    \item Wir lösen diese Problem indem wir folgenden Satz anwenden. 

        Sei $(A_{ij})$ mit $i=1, \cdots, m_{j}$, $j=1,\dots ,n$ ein
        endliches vollständig stochastisch unabhängiges Mengensystem. Dann
        sind die Mengen $\bigcup_{i=1}^{m_1} A_{i 1}$,\ldots,
        $\bigcup_{i=1}^{m_n} A_{i n}$ ebenfalls vollständig stochastisch
        unabhängig. 
        
        Beweis: Wir berechnen 
        \begin{align*}
            P\left( \bigcup_{i} A_{i 1} \cap \dots \cap \bigcup_{i} A_{i n} \right) =& 
            1 - P \left(  \bigcap_{i} \bar A_{i 1} \cup \dots \cup \bigcap_{i} \bar A_{i n} \right) \\
            =& 1 - P(\bigcap_{i} A_{i1}) + \dots + P(\bigcap_{i} A_{i n}) + \dots \\ 
            & - (-1)^{n+1}
            P\left(   \bigcap_{i} \bar A_{i 1} \cap \dots \cap \bigcap_{i} \bar A_{i n} \right) \\
            =& \left( 1 - P( \bigcap_{i} \bar A_{i 1} ) \right)\dots \left( 1-P( \bigcap_{i} \bar A_{i n}  ) \right) \\
            =& P( \bigcup_{i} A_{i 1} ) \dots P( \bigcup_{i} A_{i n} ).
        \end{align*}
        Dies ist eine Anwendung der Formel aus dem Problem ``Wahrscheinlichkeitsmaße. Zerlegung der Vereinigung.''

        Wir bezeichnen mit $\bar A$, $\bar B$ und $\bar D$ die Ausfälle der Teilsysteme in der Reihenfolge
        von links nach rechts. Es gilt
        \begin{align*}
            P(A) &= 1 - P(\bar A_3)(1- P(A_1)P(A_2)) = 0.928 \\
            P(B) &= 1 - P(\bar B_1 ) P(\bar B_2) = 0.91    \\
            P(D) &= 0.99 \\
            P(A\cap B) &= 0.84448 \\
            P( D \cap ( C \cup (A \cap B))) &= P( D )( 1- P(\overline{ A \cap B }) P( \bar C )) = 0.97460352.
        \end{align*}
\end{enumerate}

\problem{Unabhängigkeit. $3$-teiliges Gerät.}
Ein Gerät bestehe aus $3$ nacheinander angeordneten Teilsystemen, die
unabhängig voneinander mit den Wahrscheinlichkeiten $0.3$, $0.4$ bzw.~$0.6$
ausfallen können. Das Gerät sei nur funktionsfähig, wenn alle Teilsysteme
funktionieren.
\begin{enumerate}
\item Mit welcher Wahrscheinlichkeit ist das Gerät funktionsfähig?

\item Angenommen, das erste Teilsystem fällt niemals aus und dem dritten
    Teilsystem werden zwei weitere Reservesysteme (mit derselben
    Ausfallwahrscheinlichkeit $0.6$) parallel geschaltet. Mit welcher
    Wahrscheinlichkeit ist das Gerät funktionsfähig?
\end{enumerate}

\solution 
\begin{enumerate}
    \item Wir bezeichnen jeweils mit $\bar A$, $\bar B$, bzw.\ $\bar C$ die Ausfallereignisse
        von den Geräten. Die Wahrscheinlichkeit, dass das System funktionsfähig ist, berechnet 
        sich zu
        \begin{align*}
            P( A \cap B \cap C ) &= 0.168.
        \end{align*}

    \item Nun ist $P(\bar A) = 0$ und statt $C$ haben wir drei unabhängige Kopien $C_1,C_2,C_3$.
        Wir berechnen die Wahrscheinlichkeit
        \begin{align*}
            P( B \cap (C_1 \cup C_2 \cup C_3)  ).
        \end{align*}
        Dafür brauchen wir folgenden Satz. Sind die Ereignisse $A$, $B$, $C$ vollständig stochastisch
        unabhängig, so sind die Ereignisse $A$ und $B\cup C$ ebenfalls unabhängig. Beweis:
        \begin{align*}
            P(A \cap ( B \cup C)) &= P( (A\cap B) \cup (A\cup C) ) \\
            &= P( A\cap B ) + P(A\cap C) - P(A \cap B\cap C) \\
            &= P(A) \left(   P(B) + P(C) - P(B\cap C)   \right) \\
            &= P(A) P(B\cup C). 
        \end{align*}
        Somit ist $C_1$ unabhängig von $C_2 \cup C_3$ und 
        \begin{align*}
            P\left( B \cap ( C_1 \cup C_2 \cup C_3 ) \right) &= P(B)( 1 - P(\bar C_1)P(\bar C_2)P(\bar C_3)   ) \\
            &= 0.6\left( 1- {0.6}^{3} \right) = 0.4704. 
        \end{align*}
\end{enumerate}


\problem{Unabhängigkeit. Fertigungsprozess.}
Bei einem kontinuierlichen Fertigungsprozess treten nacheinander die
Arbeits\-gän\-ge Drehen, Fräsen und Schleifen auf. Zur Sicherung eines
gleichmäßigen Erzeugnisdurchlaufs werden dabei $3$ Drehmaschinen, $2$ Fräsmaschinen
und eine Schleifmaschine eingesetzt. Die benutzten Maschinen seien voll
ausgelastet und fallen innerhalb einer Schicht unabhängig voneinander mit
folgenden Wahrscheinlichkeiten aus:
\begin{center}
\begin{tabular}{c|c}
Maschine& Ausfallswahrscheinlichkeit \\ \hline
Drehmaschine & 0.3 \\
Fräsmaschine & 0.2 \\
Schleifmaschine & 0.1
\end{tabular}
\end{center}
\begin{enumerate}
\item Berechnen Sie die Wahrscheinlichkeit dafür, dass innerhalb einer Schicht
    durch Ausfälle der betrachteten Maschinen der Erzeugnisdurchlauf gestoppt
    wird.
\item Geben Sie die Wahrscheinlichkeit dafür an, dass innerhalb einer Schicht
    durch Maschinenausfälle der Erzeugnisdurchlauf verlangsamt wird, ohne dass
    es zu einem Stopp bei den betrachteten Arbeitsgängen kommt.
\end{enumerate}

\solution
\begin{enumerate}
    \item Wir bezeichnen die Ausfälle der Drehmaschinen mit $\bar D_1, \bar
        D_2, \bar D_3$, der Fräsmaschinen mit $\bar F_1, \bar F_2$ und der
        Schleifmaschine mit $\bar S_1$. Der Erzeugungsdurchlauf wird gestoppt
        mit Wahrscheinlichkeit
        \begin{align*}
            \bP \left( (\bar D_1 \cap \bar D_2 \cap \bar D_3 ) \cup ( \bar F_1 \cap \bar F_2) \cup \bar S_1 \right) &=  \\
            1 - ( 1 - P(\bar D_1)P(\bar D_2)P(\bar D_3))(1- P(\bar F_1)P(\bar F_2))P(\bar S_1) &= \\
            &= 0.159328
        \end{align*}
    \item Die Ereignisse ``Stopp'', ``Verlangsamung'', ``Volle Kapazität'' sind disjunkt. 
        Das System läuft mit voller Kapazität mit Wahrscheinlichkeit
        \begin{align*}
            P( D_1 \cap D_2 \cap D_3 \cap F_1 \cap F_2 \cap S_1 ) = 0.197568.
        \end{align*}
        Daher ist die Wahrscheinlichkeit für den verlangsamten Betrieb
        \begin{equation*}
            1 - 0.159328 - 0.197568 = 0.643104.
        \end{equation*}
\end{enumerate} 

\problem{Ausfälle der Notstromgeneratoren.}
Der Betrieb eines Rechenzentrums wird von einer Kette von Notstromgeneratoren
$G_1,\dots ,G_n$, $n\geq 1$ gesichert. Jedem Generator $G_i$ wird eine fixe
maximale Arbeitsdauer $d_{i}>0$ (in Stunden) zugeordnet, über die die
Stromversorgung mit $G_i$ gewährleistet werden kann. Beim Einschalten, und nur
dann, kann $G_i$ mit Wahrscheinlichkeit $p_{i}>0$ ausfallen. Die Ausfälle der
Generatoren sind unabhängig voneinander.

Betrachten Sie eine Kette bestehend aus einem Dieselmotor $G_1$,
sowie zwei Batterieanlagen $G_2$ und $G_3$. Die Arbeitsdauer sind
$\left( d_1, d_2, d_3 \right) = \left( 24, 2, 1 \right)$ und die
Ausfallswahrscheinlichkeiten betragen $\left( p_{1}, p_{2}, p_{3}
\right) = \left( 0.1, 0.02, 0.03 \right)$.

\begin{enumerate}
    \item Berechnen Sie die Wahrscheinlichkeit dafür, dass die Kette
        $\left( G_1, G_2, G_3 \right)$ das Rechenzentrum $23$ Stunden lang
        mit Strom versorgen kann.

    \item Berechnen Sie die Wahrscheinlichkeit dafür, dass $\left( G_1, G_2,
        G_3 \right)$ das Rechenzentrum $25$ Stunden lang mit Strom versorgen
        kann.
\end{enumerate}

\solution
Bezeichnen wir mit $A_1, A_2, A_3$ die Ausfallsereignisse der Generatoren
und mit $\tau$ die Ausfallszeit der Kette. Folgende Wahrscheinlichkeiten
sind zu berechnen:
\begin{enumerate}
    \item $P\left( \tau>23 \right) = P( \bar A_1 ) = 0.99$.
\end{enumerate}

\problem{Komplexes Gerät.} Ein Gerät besteht aus den Baugruppen $B_1,\dots
,B_4$. Es funktioniert, wenn wenigstens eine der Baugruppen $B_1$ oder $B_2$
und wenn außerdem beide Baugruppen $B_3$ und $B_4$ funktionieren. Die
Baugruppen können unabhängig voneinander ausfallen. Die
Ausfallwahrscheinlichkeiten bei $10$-stündiger Betriebsdauer betragen für $B_1$
und $B_2$ jeweils $0.5$, für $B_3$ und $B_4$ jeweils $0.2$. 
\begin{enumerate}
    \item Mit welcher Wahrscheinlichkeit funktioniert das Gerät nach
        $10$-stündigem Betrieb noch?
    \item Mit welcher Wahrscheinlichkeit funktionieren nach dieser Zeit noch
        alle vier Bauelemente?
\end{enumerate}


\section{Elementare Maßtheorie}

\problem{Maßtheorie. Borel-messbare Abbildungen.} Sei ein messbarer Raum
$(\Omega, \fA)$ mit $\Omega=\left\{ \omega_1, \omega_2, \omega_3, \omega_4
\right\}$ und der $\sigma$-Algebra $\fA = \left\{ \emptyset, \Omega, \left\{
\omega_1, \omega_2 \right\}, \left\{ \omega_3, \omega_4 \right\} \right\}$
gegeben. Wir betrachten Abbildungen von $\Omega$ nach $\bR$, wobei $\bR$ mit
der Borel'schen $\sigma$-Algebra $\cB(\bR)$ ausgestattet ist.
\begin{enumerate}
    \item Zeigen Sie, dass die Abbildung $f: \Omega \to \bR, \omega \mapsto 5$
        messbar ist.
    \item Zeigen Sie, dass die Abbildung $g: \Omega \to \bR$ mit $g(\omega_i) = i$
        für $i\in \left\{ 1,2,3,4 \right\}$ nicht messbar ist.
    \item Beschreiben Sie nun alle messbaren Abbildungen $h: \Omega \to \bR$. 
\end{enumerate}



\problem{Maßtheorie. Dirac-Maß.} Sei ein messbarer Raum $(\bR, \cB(\bR))$
gegeben. Für ein festes $x\in\bR$ definieren wir die Mengenfunktion $\delta_x
: \cB(\bR) \to \R$ als
\begin{equation*}
    \delta_{x} (A) =
        \begin{cases}
            1 & \textrm{ falls } x\in A \\
            0 & \textrm{ sonst. }
        \end{cases}
\end{equation*}
\begin{enumerate}
    \item Zeigen Sie, dass $\delta_x$ für alle $x\in\bR$ ein Maß ist.

    \item Betrachten wir ein $n$-Tupel $(x_1, \dots, x_n)$ mit $x_i>0$ für
        alle $i\in \left\{ 1, \dots, n \right\}$ und eine Mengenfunktion
        $\delta: \cB(\bR) \to \bR, A \mapsto \sum_{i=1}^{n} \delta_{x_i} (A)$.
        Zeigen Sie, dass $\delta$ ein Maß auf $\left( \bR, \cB(\bR)
        \right)$ ist.

    \item Sei nun $(c_1, \dots, c_n)$ ein $n$-Tupel positiver reeller Zahlen
        mit $\sum_{i=1}^{n} c_i = 1$. Zeigen Sie, dass die Mengenfunktion
        $\delta$ ein Wahrscheinlichkeitsmaß auf $\left( \bR, \cB(\bR) \right)$
        ist. 
\end{enumerate}




\problem{Maßtheorie. Bildmaß.}
Seien als Ma{\ss}raum ein Wahrscheinlichkeitsraum $\left( \Omega, \fA, P \right)$ und eine
Zufallsgröße $X: \Omega \to \bR$ gegeben, welche als messbare Abbildung zwischen den messbaren R\"aumen $\left( \Omega,\fA\right)$ und  $\left( \bR,\cB(\bR) \right)$ definiert ist.
Dabei bezeichnet $\cB(\bR)$ die Borel'sche $\sigma$-Algebra auf den reellen Zahlen. Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item Die Mengenfunktion $P_{X}: \cB(\bR) \to [0,1], A \mapsto P( X \in A
        )$ ist ein Wahrscheinlichkeitsmaß auf $(\bR, \cB(\bR))$. 

    \item Ist die Ergebnismenge $\Omega = \left\{ \omega_{1},\dots ,\omega_{n} \right\}$ endlich,
        so gibt es $n$-Tupel reeller Zahlen $(x_1,\dots ,x_n)$ und $(p_1,\dots ,p_n)$ mit 
        $p_i>0 \ \forall i\in \left\{ 1,\dots ,n \right\}$ und $\sum_{i=1}^{n} p_i = 1$, sodass 
        \begin{equation*}
            P_X = \sum_{i=1}^{n} p_i \delta_{x_i}.
        \end{equation*}
        Dabei ist $\delta_x : \cB(\bR) \to \R$ für ein festes $x\in\bR$ ein
        Diracmaß mit $$\delta_x(B)=\left\{\begin{array}{lll} 1 & \mbox{falls} & x \in B\\ 0 & \mbox{falls} & x \notin B \end{array}\right., \quad B \in \cB(\bR).$$
        Sind die Tupel
        $(x_1,\dots ,x_n)$ und $(p_1,\dots ,p_n)$ eindeutig durch $X$ bestimmt?
        Welche Bedeutung haben sie?
\end{enumerate}

\solution
\begin{enumerate}
    \item $P_X$ ist wohldefiniert, normiert und $\sigma$-additiv: $P_X \left( \bigcup_{k\geq 0} A_i \right) = P\left( X \in \bigcup_{k\geq 0} A_i \right)= P\left( \bigcup_{k\geq 0} \left\{ X\in A_i \right\} \right) = \sum_{k\geq 0}^{} P_X\left( X \in A_i \right)$.
\end{enumerate}

\problem{Maßtheorie. Ereignisfelder und Information.} Die Ergebnismenge
$\Omega = \left\{ (i,j) : i,j \in \left\{ 1,\dots ,6 \right\} \right\}$
gemeinsam mit dem Wahrscheinlichkeitsmaß $P( (i,j) ) = \frac{1}{36}$ beschreiben
einen Wurfexperiment, in dem zwei ideale wohlunterscheidbare Würfel geworfen
werden. 
Für ein Ergebnis $\omega = (i,j) \in \Omega$ definieren wir die Funktionen
\begin{align*}
    X_0(\omega) &= 0 \\
    X_1(\omega) &= i+j, \\
    X_2(\omega) &= i, \\
    X_3(\omega) &= 
    \begin{cases}
        1 & \text{ falls } i+j=6 \\
        0 & \text{ sonst.}
    \end{cases}
\end{align*}
\begin{enumerate}
    \item Finden Sie die kleinsten Ereignisfelder $\fA_{i}$, $i\in \left\{
        0,1,2,3 \right\}$, sodass $X_i$ Zufallsgrößen auf dem Wahrscheinlichkeitsraum
        $(\Omega, \fA_i, P)$ sind. 
    \item Beschreiben Sie den Zusammenhang zwischen den Ereignisfeldern $\fA_i$
        und der Information, die die Zufallsgrößen $X_i$ über die Ergebnisse
        $\omega \in \Omega$ liefern.
\end{enumerate}

\solution 
\begin{align*}
    \fA_0 &= \left\{ \emptyset, \Omega \right\} \\
    \fA_1 &= \sigma \left\{ \left\{ 11 \right\}, \left\{ 21,12 \right\}, \left\{ 13,22,31
    \right\},\dots , \left\{ 16,25,34,\dots ,61 \right\} \right\} \\
    \fA_2 &= \sigma \left\{ \left\{ 11,12,13,14,15,16 \right\},\dots ,\left\{ 61,62,\dots ,66
    \right\} \right\} \\
    \fA_3 &= \sigma \left\{ \left\{ 15,24,\dots ,51 \right\}, 
    \overline{ \left\{ 15,24,\dots ,51 \right\}  } \right\}.
\end{align*}


\section{Diskrete Zufallsvariablen}

\problem{Zufallsvariablen. Bernoulli-Verteilung.} Eine Zufallsvariable $X$ mit den
Werten in der Menge $\left\{ 0,1 \right\} \subset \bR$ und der
Wahrscheinlichkeitsfunktion $\bP(X=1)= p$, $\bP(X=0)=1-p$ mit $p\in(0,1)$ heißt
Bernoulli-verteilt mit dem Parameter $p$. 
\begin{enumerate}
    \item Geben Sie die Verteilungsfunktion $F: \bR\to [0,1], x\mapsto F(x) =
        \bP(X\leq x)$ von $X$ an.

    \item Berechnen Sie den Erwartungswert $\bE [X]$ und die Varianz $\Var(X)$ von $X$. 

    \item Berechnen Sie den Erwartungswert $\bE [Y]$ und die Varianz $\Var (Y)$ der
        Zufallsgröße \[Y = \exp\left( X \right).\]

    \item Berechnen Sie die $k$-ten Momente $m_k=\bE [X^k]$ und die $k$-ten
        zentralen Momente $\mu_k=\bE[ (X-\bE X)^k]$ von $X$ für alle $k\in
        \bN$. 
\end{enumerate}

\solution Verteilungsfunktion ist $F: x\mapsto (1-p)1_{[0,\infty)}(x) + p 1_{[1,
+\infty)}(x)$. $\bE X = p$. $\Var X = p(1-p)$. $\bE Y = 1-p + p e$. $\Var Y =
p(1-p)(1-e)^2$. $m_k = \bE X^{k} = \bE X$, da $X = X^k$. $\mu_k = \bE \left[ X
-\bE X \right]^{k} = p(1-p)^{k} + (-p)^{k}(1-p)$.


\problem{Zufallsvariablen. Funktionen der Augenzahl.} In einem Wurfexperiment
werden zwei ideale, wohlunterscheidbare Würfel geworfen. Die Zufallsgrößen
$X_i$ geben die geworfene Augenzahl des $i$-ten Würfels an.
\begin{enumerate}
    \item Berechnen Sie die Wahrscheinlichkeiten $\bP(X_1-X_2 > 2)$, $\bP(X_1/X_2 > 1)$
        und \linebreak $\bP(|X_1-X_2| \leq 1)$. 
    \item Berechnen Sie die Verteilungsfunktion $F$ der Zufallsgröße $Y = X_1 + X_2$. 
\end{enumerate}

\solution $\bP(X_1-X_2 > 2)=\frac{6}{36}$, $\bP(X_1/X_2>1)=\frac{15}{36}$, 
$\bP(|X_1-X_2| \leq 1) =\frac{6+5+5}{36}$. 


\problem{Zufallsvariablen. Konstruktion über Wahrscheinlichkeitsfunktion.}
Es sei $X$ eine diskrete Zufallsgröße mit dem Wertebereich $Z=\{0,1,2\}$ und
der Wahrscheinlichkeitsfunktion $p: Z \to [0,1], x \mapsto c \cdot (x+1)$.
\begin{enumerate}
    \item Bestimmen Sie den Wert der Konstanten $c$.
    \item Ermitteln Sie die folgenden Wahrscheinlichkeiten: $\bP(X<2)$, $\bP(X
        \leq 2)$ und $\bP(0<X<2)$. 
    \item Berechnen Sie den Erwartungswert und die Varianz von $X$.
    \item Bestimmen Sie den kleinsten Wert von $x$, für den $\bP(X\leq x)>0.5$
        gilt.
\end{enumerate}

\solution $c=\frac{1}{6}$. $\bP(X<2)=\frac{1}{2}$. $\bP(X\leq 2)=1$.
$\bP(0<X<1)=\frac{1}{3}$. $\bE X=\frac{4}{3}$. $\Var X = \frac{5}{9}$.
$\min_{x} \left\{ \bP(X\leq x) \geq 0.5 \right\}=2$.


\problem{Zufallsvariablen. Ausfälle der Bauelemente.}
In einem vorgegebenen Zeitintervall $[0,T]$ könnnen drei Baugruppen $A_1$,
$A_2$, bzw.\ $A_3$ eines Gerätesystems mit Wahrscheinlichkeiten $0.95$, $0.8$
bzw.\ $0.1$ ausfallen. Die Baugruppen arbeiten unabhängig voneinander. 

Bestimmen Sie die Wahrscheinlichkeitsfunktion, den Erwartungswert sowie die
Varianz für die Anzahl der ausgefallenen Baugruppen zum betrachteten
Zeitintervall.

\solution
Seien $X_1 \sim \text{Ber}(0.95)$, $X_2 \sim \text{Ber}(0.8)$, $X_3 \sim
\text{Ber}(0.1)$ und $Y=X_1+X_2+X_3$. $\bE Y = 1.85$. $\Var Y = 0.2975$.
Berechnung der Varianz direkt oder über den Satz von Bienaym\'e.


\problem{Zufallsvariablen. Diskrete Gleichverteilung.} Eine Zufallsvariable heißt
diskret gleichverteilt auf $\left\{ 1,\dots ,n \right\}$ falls
$\bP(X=i)=\frac{1}{n}$ für alle $i\in\left\{ 1,\dots ,n \right\}$.
\begin{enumerate}
    \item Geben Sie die Verteilungsfunktion $F: \bR\to [0,1], x\mapsto F(x) =
        \bP(X \leq x)$ von $X$ an.

    \item Berechnen Sie den Erwartungswert und die Varianz von $X$. 

    \item Berechnen Sie den Erwartungswert und die Varianz der Zufallsgröße
        $Y = \log\left( X \right)$.

%    \item Berechnen Sie das $k$-te Moment und das $k$-te zentrale Moment von 
%        $X$ für alle $k\in \bN$. 
\end{enumerate}

\solution
$\bE X = \frac{n+1}{2}$, $\sum_{k=1}^{n} k^2 = \frac{n(n+1)(2n+1)}{6}$,
$\bE X^2 = \frac{(n+1)(2n+1)}{6}$, $\Var X = \bE X^2 - \left( \bE X \right)^2$.
$\bE \log X = \frac{1}{n} \log n!$.
$\Var \left( \log X \right) = \frac{1}{n} \sum_{i=1}^{n} (\log i)^{2} - \left( \frac{1}{n} \log n! \right)^{2}$. 

%Die Berechnung der $k$-ten Momente führt zu der Formel von Faulhaber: \url{https://en.wikipedia.org/wiki/Faulhaber\%27s\_formula}


\problem{Erwartungswert. Einfache Eigenschaften.} Sei $X$ eine diskrete
Zufallsvariable. Beweisen Sie folgende Aussagen.
\begin{enumerate}
    \item $\bE \left( X - \bE X \right)^2 = \bE X^2 - \left( \bE X \right)^2$, falls alle diese
        Erwartungswerte existieren.
    \item $\left( \bE X \right)^2 \leq \bE X^{2}$, falls beide Erwartungswerte existieren. 
    \item Angenommen $X$ hat Werte in $\bN$. Dann gilt
        \begin{equation*}
            \bE X = \sum_{n=0}^{\infty} \bP(X > n). 
        \end{equation*}
\end{enumerate}

\solution 
\begin{align*}
    \bE X &= \sum_{n\geq 0} n\, \bP\left( X=n \right) \\
    &= \bP(X=1)+\bP(X=2)+\bP(X=2)+\dots+ \underbrace{\bP(X=n)+\dots+\bP(X=n)}_{n \text{ times}} + \dots \\
    &= \sum_{n\geq 0}^{} \bP(X>n).
\end{align*}


\problem{Geometrische Verteilung. Konstruktion.} Eine Münze nicht
notwendigerweise faire Münze wird wiederholt geworfen, bis Kopf fällt.
Bezeichnen wir mit $X_0$ die Anzahl der dafür notwendigen Würfe.
\begin{enumerate}
    \item Geben Sie die Wahrscheinlichkeitsfunktion der Zufallsvariable $X_0$ 
        explizit an.
    \item Berechnen Sie den Erwartungswert und die Varianz von $X$.
\end{enumerate}


\problem{Geometrische Verteilung. Erwartungswerte.} Sei $X$ geometrisch
verteilt mit dem Parameter $p\in (0,1)$. Zeigen Sie folgende Aussagen: 
\begin{enumerate}
    \item \begin{equation*}
            \bfE \left[ \frac{1}{1+X} \right] = \log \left( (1-p)^{\frac{p}{p-1}} \right).
        \end{equation*}

    \item Für alle $n\in \left\{ 2,3, \dots \right\}$ gilt
        \begin{equation*}
            \bfE \left[ X(X-1)\cdot \dots \cdot (X - n+1) \right] = \frac{n!\ p^n}{(1-p)^n}.
        \end{equation*}
\end{enumerate}

\solution
\begin{enumerate}
    \item \begin{align*}
            \log (1+z) &= z - \frac{z^2}{2} + \frac{z^3}{3} - \cdots \\
            E \left( \frac{1}{1+X} \right) &= \frac{p}{1-p} \sum_{k\geq 0} \frac{(1-p)^{k+1}}{k+1}
            = \frac{p}{p-1} \log \left( 1-p \right) = \log (1-p)^{\frac{p}{p-1}}. 
        \end{align*}

\end{enumerate}

\problem{Geometrische Verteilung. Gedächtnislosigkeit.} Sei $X$ geometrisch
verteilt mit dem Parameter $p$.
\begin{enumerate}
    \item Zeigen Sie, dass 
        \begin{equation*}
            P \left( X > i+j \,|\, X \geq i \right) = P\left( X > j \right) 
        \end{equation*}
        für alle $i,j>0$ und $i,j\in\bN$ gilt.

    \item Finden Sie eine Interpretation für die obige Beziehung.
\end{enumerate}

\solution 
\begin{align*}
    \bP\left( X > i+j \vb X \geq i \right) &= \frac{\bP(X > i+j)}{\bP(X \geq i)} \\
    &= \frac{  (1-p)^{i} \sum_{k=i+1}^{\infty} p(1-p)^{k} }{ (1-p)^{i} \sum_{l=0}^{\infty} p(1-p)^{l} } \\
    &= \sum_{k > j}^{} p(1-p)^k.
\end{align*}


\problem{Geometrische Verteilung. Gedächtnislosigkeit und Erwartungswert.} Sei 
$X$ geometrisch verteilt mit dem Parameter $p\in (0,1)$ und der 
Wahrscheinlichkeitsfunktion
\begin{equation*}
    P(X = k) = p(1-p)^{k}, \ k=0,1,\dots
\end{equation*}
\begin{enumerate}
    \item Zeigen Sie, dass
        \begin{equation*}
            P( X > i+j \,|\, X \geq i) = P(X > j)
        \end{equation*}
        für alle $i,j>0$ gilt.
    \item Zeigen Sie, dass 
        \begin{align*}
            \E X = \frac{1-p}{p}
        \end{align*}
        gilt.
    \item Berechnen Sie den Erwartungswert der Zufallsvariable $Y=2\,X -1$.
\end{enumerate}


\problem{Binomialverteilung. Maximum der Wahrscheinlichkeitsfunktion.} 
Sei $X \sim \bfB(n,p)$ binomialverteilt mit den Parametern $n$ und $p$. Zeigen
Sie, dass das Maximum der Wahrscheinlichkeitsfunktion von $X$ an der Stelle
\begin{equation*}
    \operatorname{argmax}_{k\in \left\{ 0,\dots ,n \right\}} P(X=k) = \lfloor (n+1)p \rfloor.
\end{equation*}
angenommen wird. Dabei bezeichnet $\lfloor x \rfloor$ für ein $x\in\bR$ die
größte ganze Zahl, die nicht größer als $x$ ist. 

\solution 
Wir berechnen
\begin{align*}
    P(X=k)/P(X=k-1) &= \frac{n+1-k}{k} \frac{p}{1-p}.
\end{align*}
Dieser Quotient ist größer als $1$ genau dann, wenn 
\begin{equation*}
    (n+1)p > k. 
\end{equation*}


\problem{Binomialverteilung. Gerade Werte.} 
Sei $X \sim \bfB(n,p)$ binomialverteilt mit den Parametern $n$ und $p$. Zeigen
Sie, dass die folgende Formel gilt:
\begin{equation*}
    P\left( X \text{ ist eine gerade Zahl} \right) = \frac{1}{2}\left( 1 + (1-2p)^n \right). 
\end{equation*}

\solution 
Sei $p_n$ die Wahrscheinlichkeit, dass eine
$\bfB(n,p)$-verteilte Zufallsgröße gerade ist. $p_n$ erfüllt folgende rekursive
Relation
\begin{align*}
    p_n &= p_{n-1}(1-2p) + p, &  p_0 &= 1.
\end{align*}
Nun ist es einfach zu überprüfen, dass $\frac{1}{2}(1+ (1-2p)^n)$ die Lösung
dieser Rekursion ist. 


\problem{Poisson-Verteilung. Eigenschaften.} Sei $X$
Poisson-verteilt mit der Wahrscheinlichkeitsfunktion $P(X = k) =
\frac{\lambda^k}{k!} e^{-\lambda}$, $k\in \left\{ 0,1,\dots \right\}$ und dem
Parameter $\lambda>0$. Beweisen Sie folgende Aussagen: 
\begin{enumerate}
    \item Die Varianz von $X$ ist 
        \begin{equation*}
            \bfD^2 X = \lambda.
        \end{equation*}
    \item  Das Maximum der Wahrscheinlichkeitsfunktion von $X$ wird an der Stelle
        \begin{equation*}
            \operatorname{argmax}_{k\in \bN} P(X= k) = \lfloor \lambda \rfloor 
        \end{equation*}
        angenommen.

    \item Für $n\in \left\{ 2,3,\dots  \right\}$ gilt 
        $\bfE \left[ X(X-1)\cdot\dots\cdot (X-n+1) \right] = \lambda^n$.
\end{enumerate}

\solution


\problem{Binomialverteilung. Additionstheorem.} 
Sei $b(k, n, p)=P(X=k)$ die Wahrscheinlichkeitsfunktion einer binomialverteilten
Zufallsgröße $X\sim\bfB(n,p)$.
\begin{enumerate}
    \item Zeigen Sie, dass für $n_1>0$ und $n_2>0$ die Gleichung
        \begin{equation*}
            \sum_{i=0}^{k} b(i, n_1, p)\, b( k-i, n_2, p) = b( k, n_1+n_2, p ).
        \end{equation*}
        gilt. 

    \item Geben Sie die wahrscheinlichkeitstheoretische Interpretation der
        obigen Aussage.
\end{enumerate}


\problem{Poisson-Verteilung. Additionstheorem.} Seien $X_1\sim
\text{Poiss}(\lambda_1)$ und $X_2\sim \text{Poiss}(\lambda_2)$ unabhängige
Poisson-verteilte Zufallsvariablen mit den Parametern $\lambda_1>0$ und
$\lambda_2>0$. Zeigen Sie, dass die Zufallsvariable
\begin{equation*}
    Y = X_1 + X_2
\end{equation*}
ebenfalls Poisson-verteilt mit dem Parameter $\lambda=\lambda_1 + \lambda_2$ ist. 

\solution 
\begin{align*}
    P\left( X+Y=n \right) &= \sum_{i=0}^{n} P(X=i) P(Y=n-i) \\
    &= \sum_{i=0}^{n} e^{-\lambda_1} \frac{\lambda_{1}^{i}}{i!}\, e^{-\lambda_2} \frac{ \lambda_2^{n-i}}{(n-i)!} \\
    &= \frac{e^{-\lambda_1 -\lambda_2}}{n!} \sum_{i=0}^{n} \binom{n}{i} \lambda_1^{i} \lambda_2^{n-i} \\
    &= e^{- (\lambda_1 + \lambda_2)} \frac{ (\lambda_1 + \lambda_2)^{n}}{n!}.
\end{align*}


\problem{Binomialverteilung. Verteilungsfunktion.} Seien $X\sim\bfB(n,p)$ binomialverteilt
und $B(k, n, p) = P( X <k)$ die Verteilungsfunktion von $X$. Zeigen Sie:
\begin{equation*}
    B(k+1, n, p) = (n-k) \binom{n}{k} \int_{0}^{1-p} t^{n-k-1} (1-t)^k dt.
\end{equation*}

\solution
\begin{enumerate}
    \item Die Formel kann mit Hilfe der partiellen Integration hergeleitet werden.
        \begin{align*}
            B(k+1, n, p) &= (n-k) \binom{n}{k} \int_{0}^{1-p} t^{n-k+1} (1-t)^{k} dt \\
            &= (n-k)\binom{n}{k} \left[  \frac{1}{n-k}(1-p)^{n-k} p^{k} + 
            \frac{k}{n-k} \int_{0}^{1-p} t^{n-k} (1-t)^{k-1} dt \right] \\
            &= \binom{n}{k} (1-p)^{n-k} p^{k} + 
            (n-(k-1)) \binom{n}{k-1} \int_{0}^{1-p} t^{n-(k-1)-1} (1-t)^{k-1} dt \\
            &= \binom{n}{k} (1-p)^{n-k} p^{k} + B(k, n, p). \\
            B(1,n,p) &= n \binom{n}{0} \int_{0}^{1-p} t^{n-1} dt = (1-p)^n.
        \end{align*}
        Damit erhalten wir
        \begin{equation*}
            B(k+1, n,p) = \sum_{i=0}^{k} \binom{n}{k} p^{i}(1-p)^{n-i}.
        \end{equation*}
\end{enumerate}

\problem{Geometrische Verteilung. Zusammenhang mit der Gleichverteilung.}
Die Zufallsgrößen $X$ und $Y$ sind stochastisch unabhängig und geometrisch verteilt mit dem Parameter $p\in(0,1)$.
\begin{enumerate}
    \item Berechnen Sie die Wahrscheinlichkeiten der Ereignisse $X=Y$ und
        $X\geq 2Y$.

    \item Zeigen Sie, dass für $k,n\in\bN$ und $n>k$
        \begin{equation*}
            P \left( X = k \,|\, X+Y = n \right) = \frac{1}{n+1}
        \end{equation*}
        gilt. Mit anderen Worten: Die bedingte Verteilung von $X$ bei gegebenem Wert von 
        $X+Y$ ist die Gleichverteilung.
\end{enumerate}

\solution
\begin{enumerate}
    \item \begin{align*}
            P\left( X = Y \right) &= \sum_{k=0}^{\infty} P\left( X=k \wedge Y=k \right) = \frac{p}{1+p} \\
            P\left( X \geq 2y \right) &= \left( 1-p \right)^{2y} \\
            P\left( X \geq 2Y \right) &= \frac{p}{1- (1-p)^{3}}.
        \end{align*}

    \item Für $X_1 \sim \text{Poiss}(p_1)$ und $X_2\sim\text{Poiss}(p_2)$ erhalten wir
        \begin{align*}
            P\left( X_1 = k \,|\, X_1+X_2 = n \right) &= 
            \frac{(1-p_1)^{k} (1-p_2)^{n-k}  }{ \sum_{i=0}^{n} (1-p_1)^i (1-p_2)^{n-i} }. 
        \end{align*}
\end{enumerate}


\problem{Negative Binomialverteilung. Konstruktion.} $X$ ist negativ
binomialverteilt mit den Parametern $n\in\bN$ und $p\in \left( 0,1 \right)$, wenn
für $k\in\bN$
\begin{equation*}
    P(X = k) = \binom{k+n-1}{n-1} p^{n} (1-p)^{k} 
\end{equation*}
gilt. Seien $Z_1,Z_2\dots$ stochastisch unabhängige und geometrisch verteilte Zufallsgrößen mit 
$Z_i \sim \text{Geom}(p)$. Zeigen Sie folgende Aussagen: 
\begin{enumerate}
    \item Die Zufallsgröße $Z_1+Z_2$ ist negativ binomialverteilt mit den
        Parametern $(2,p)$. 
    \item Für $k, n \in\bN$ gilt
        \begin{equation*}
            \binom{k+n}{n} = \sum_{j=0}^{k} \binom{j+n-1}{n-1}.
        \end{equation*}
    \item Die Zufallsgröße $Z_1+\dots+Z_n$ ist negativ binomialverteilt mit den
        Parametern $(n,p)$. 
\end{enumerate}

\solution
\begin{enumerate}
    \item \begin{equation*}
            P\left( Z_1+Z_2 = k \right) = (k+1)p^{2}(1-p)^k.
        \end{equation*}
    \item Beweisen wir zuerst
        \begin{align*}
            \binom{n}{k} &= \binom{n-1}{k-1} + \binom{n-1}{k} \\
            \binom{n+k}{k} &= \binom{n+k}{n}
        \end{align*}
        und den Rest mit Induktion.
    \item \begin{align*}
            P\left( \sum_{i=1}^{n+1} Z_i = k \right) &= 
            \sum_{j=0}^{k} P\left( \sum_{i=1}^{n} Z_i = j \right) P\left( Z_{n+1} =k-j \right) \\
            &= \sum_{j=0}^{k} \binom{j+n-1}{n-1} p^{n}(1-p)^{j} (1-p)^{k-j}p \\
            &= \sum_{j=0}^{k} \binom{j+n-1}{n-1} p^{n+1}(1-p)^{k} = \binom{k+n}{n} p^{n+1}(1-p)^{k}. 
        \end{align*}
\end{enumerate}


\problem{Diskrete Gleichverteilung. Ordnungsstatistiken.} Die Zufallsvariablen
$X_1,\dots ,X_m$ seien stochastisch unabhängig und diskret gleichverteilt auf der Menge
$\left\{ 1,\dots ,n \right\}$, mit $n>1$. Wir bezeichnen mit $X_{(1)} = \min
(X_1,\dots ,X_m)$ das Minimum und mit $X_{(m)} = \max \left( X_1,\dots ,X_n
\right)$ das Maximum der Zufallsvariablen $X_1,\dots ,X_m$.
%\begin{enumerate}
%    \item 
Finden Sie die Wahrscheinlichkeitsfunktion der Zufallsvariablen
        $X_{(1)}$ und $X_{(n)}$.
%    \item Finden Sie die Wahrscheinlichkeitsfunktion der Zufallsvariable $X_{(n)}-X_{(1)}$.
%\end{enumerate}

\solution
\begin{align*}
    P\left( X_{(1)} \leq k \right) &= 1 - \left( \frac{n-k}{n} \right)^m \\
    P\left( X_{(m)} \leq k \right) &= \left( \frac{k}{n} \right)^{m}. 
\end{align*}


\problem{Diskrete Gleichverteilung ist nicht teilbar.} Sei $U$ diskret
gleichverteilt auf der Menge $\left\{ 1,\dots ,n \right\}$ mit $n \in \bN$.
Zeigen Sie folgende Aussage: Es gibt unabhängige identisch verteilte Zufallsvariablen $X$ und $Y$
mit $X+Y = U$ genau dann, wenn $n=1$. 


\problem{Geometrische Verteilung. Minima und Maxima.} Seien $Y_1,\dots ,Y_m$
stochastisch unabhängig und geometrisch verteilt mit dem Parameter $p\in(0,1)$.
Wir bezeichnen mit $Y_{(1)} = \min (Y_1,\dots ,Y_m)$ das Minimum und mit
$Y_{(m)} = \max \left( Y_1,\dots ,Y_m \right)$ das Maximum der Zufallsvariablen
$Y_1,\dots ,Y_m$.  Finden Sie die Verteilungsfunktionen und Wahrscheinlichkeitsfunktionen der
Zufallsvariablen $Y_{(1)}$ und $Y_{(m)}$.

\solution
Die Verteilungsfunktion der geometrischen Verteilung an den Stellen $k\in\N$ ist gegeben durch
\begin{equation*}
  F(k) = P(Y_l \le k) = \sum_{i=0}^{k} p(1-p)^i = p\frac{1-(1-p)^{k+1}}{1-(1-p)} = 1-(1-p)^{k+1}.
\end{equation*}
Nun ergibt sich unter Ausnutzung der Unabhängigkeit
\begin{align*}
    F_{(1)}(k)
    &= P\left(\bigcup_{i=1}^m \{Y_i \le k\}\right)
    = 1 - \prod_{i=1}^m P\left(\overline{\{Y_i \le k\}}\right)
    = 1 - \prod_{i=1}^m \left[1-F(k)\right]
    = 1 - \left( 1-p \right)^{m(k+1)}, \\
    F_{(m)}(k) 
    &= P\left(\bigcap_{i=1}^m \{Y_i \le k\}\right)
    = \prod_{i=1}^m F(k)
    = \left[1-(1-p)^{k+1}\right]^m, \\
    P\left( Y_{(1)} = k \right)
    &= F_{(1)}(k) - F_{(1)}(k-1) 
    %= \left[1-(1-p)^{(k+1)m}\right] - \left[1-(1-p)^{km}\right] 
    = (1-p)^{km}\left[1-(1-p)^m\right], \\
    P\left( Y_{(m)} = k \right)
    &=  F_{(m)}(k) - F_{(m)}(k-1)
    = \left[ 1-(1-p)^{(k+1)} \right]^m - \left[ 1-(1-p)^{k} \right]^m. \\
\end{align*}


\problem{Geometrische Verteilung. Additionstheorem.} Seien $X$ und $Y$ unabhängig und geometrisch
verteilt mit den Parametern $p_1, p_2 \in (0,1)$ und $p_1\neq p_2$. Zeigen Sie folgende Aussage: 
Die Wahrscheinlichkeitsfunktion der Summe $X+Y$ ist gegeben durch
\begin{equation*}
    P(X+Y = n ) = \frac{p_1 p_2}{p_1 - p_2} \left( (1-p_2)^{n+1}- (1-p_1)^{n+1} \right).  
\end{equation*}

\solution 
\begin{align*}
    P( X+Y = n) &= \sum_{i=0}^{n} P(X=i)P(Y=n-i) \\
    &= p_1 p_2 (1-p_2)^n \sum_{i=0}^{n} \left( \frac{1-p_1}{1-p_2} \right)^{i} \\
    &=  p_1 p_2 (1-p_2)^n \frac{1 - \left( \frac{1-p_1}{1-p_2} \right)^{n+1}}{ 1- \frac{1-p_1}{1-p_2}} \\
    &= \frac{p_1 p_2}{ p_1 -p_2} (1-p_2)^{n+1}\left( 1 - \left( \frac{1-p_1}{1-p_2} \right)^{n+1} \right).
\end{align*}


\problem{Bernoulliverteilung. Momente. }
Sei $X$ Bernoulliverteilt mit dem Parameter $p \in \left( 0,1 \right)$. Es gilt also
$X: \Omega \to \left\{ 0,1 \right\}$ und $P(X=1)=p$. 
\begin{enumerate}
    \item Die Momente von $X$ sind gegeben durch
        \begin{equation*}
            \E X^n = p, \quad n\in\bN.
        \end{equation*}
    \item Die momentenerzeugende Funktion von $X$ ist 
        \begin{equation*}
            \psi(u) = \E \exp \left( u X \right) = p\exp (u)+1-p.
        \end{equation*} 
\end{enumerate}
\solution Aus $X = X^n$ für alle $n\in \bN$, erhalten wir  
\begin{align*}
    \E X = \E X^n = 1\, p + 0\, (1-p) = p.
\end{align*}
Auf direktem Wege erhalten wir auch
\begin{equation*}
    \psi(u) = \E \exp (uX) = \exp\left( u\, 1 \right)p + \exp(0)(1-p).
\end{equation*}



\problem{Binomialverteilung als Summe von unabhängigen Bernoulli Zufallsvariablen.}
Seien $X_1, \cdots, X_n$, $n\in\bN$, i.i.d.\ und Bernoulliverteilt mit dem Parameter $p\in (0,1)$. 
Dann ist
\begin{equation*}
    Y=\sum_{i=1}^{n} X_i \sim \text{Bin}(n,p)
\end{equation*}
binomialverteilt mit Parametern $(n,p)$. D.h., $P(Y = k) = \binom{n}{k}  p^{k}(1-p)^{n-k}$, 
für $k\in \left\{ 0, \cdots, n \right\}$.

\problem{Binomialverteilung. Momente und momentenerzeugende Funktion.}
Sei $X$ binomialverteilt mit den Parametern $(n,p)$, $n\in\N$ und $p\in(0,1)$. Dann gilt
\begin{enumerate}
    \item \begin{equation*}
            \E X = np. 
        \end{equation*}
    \item Die momentenerzeugende Funktion von $X$ ist gegeben durch
        \begin{equation*}
            \psi(u) = \E \exp( uX) = (1 -p + p e^u)^{n}.
        \end{equation*}
\end{enumerate}
\solution
Nachdem $X$ die Summe unabhängiger Bernoulliverteilter Zufallsvariablen $B_1,
\cdots, B_n$ ist, können wir die Linearität des Erwartungswertes benutzen, und
erhalten
\begin{equation*}
    \E X = \E B_1 + \cdots \E B_n = np. 
\end{equation*}
Zufallsvariablen $A$ und $B$ sind genau dann unabhängig, wenn 
\begin{equation*}
    \E f(A)g(B) = \E f(A)\, \E g(B)
\end{equation*}
für alle beschränkte messbare Funktionen gilt.\footnote{Beachten Sie, dass $\E
AB = \E A\, \E B$ im Allgemeinen nicht ausreichend für Unabhängigkeit ist!}
Wir können also die zuvor benutzte Methode wieder einsetzen. 
\begin{equation*}
    \E \exp (u X) = \E \prod_{i=1}^{n} e^{u B_i} 
    = \prod_{i=1}^{n} \E e^{u B_i} 
    = \left[ e^{u}p + 1 -p \right]^{n}.
\end{equation*}
Dieses Ergebnis lässt sich auch mit Hilfe der Wahrscheinlichkeitsfunktion der 
Binomialverteilung herleiten. 
\begin{align*}
    \E \exp (u X) &= \sum_{k=0}^{n} e^{u k} \binom{n}{k} p^{k}(1-p)^{n-k} \\
    &= \sum_{k=0}^{n} \binom{n}{k} (p e^{u})^{k}(1-p)^{n-k} \\
    &= \left[ e^u p + 1-p \right]^n.
\end{align*}

\problem{Poisson-Verteilung. Momente. } 
Sei $X$ Poisson-verteilt mit dem Parameter $\lambda>0$. Das bedeutet, $X:\Omega\to\bN$
hat die Wahrscheinlichkeitsfunktion 
\begin{equation*}
    P(X = k) = \frac{\lambda^{k}}{k!} e^{-\lambda}.
\end{equation*}
Es gilt
\begin{enumerate}
    \item 
        \begin{equation*}
            \E X = \Var X = \lambda.
        \end{equation*}
    \item Die momentenerzeugende Funktion von $X$ ist gegeben durch
        \begin{equation*}
            \psi(u) = \E \exp(uX) = \exp \left( \lambda(e^u - 1) \right).
        \end{equation*}
\end{enumerate}

\problem{Zusammengesetzte Poisson-Verteilung.}
Sei $N$ ein Poisson-verteilte Zufallsvariable mit dem Parameter $\lambda>0$ und
eine Folge von i.i.d.\ Zufallsvariablen $(Z_i)_{i\in\bN}$ mit existierenden
momentenerzeungenden Funktion. Wir definieren $X = \sum_{i=1}^{N} Z_i$. $X$ ist
also eine Summe mit zufälliger Anzahl der Summanden. Wir setzen bei $X=0$ auf
dem Ereignis $\left\{ N=0 \right\}$. Zeigen Sie:
\begin{enumerate}
    \item Die Momentenerzeugende Funktion von $X$ ist gegeben durch
        \begin{equation*}
            \psi_X(u) = \exp \left( \lambda \left( 
            \E \left( \exp uZ_1 -1 \right)
            \right) \right).
        \end{equation*}
    \item Der Erwartungswert von $X$ ist gegeben durch
        \begin{align*}
            E X &= \lambda \E Z_1 
        \end{align*}
        falls $\E Z_1$ existiert. 
\end{enumerate}

\solution Wir benutzen die Eigenschaften der bedingten Erwartung und erhalten
\begin{align*}
    \E \left[ \E \left[ \exp u \sum_{i=0}^{N} Z_i | N \right] \right] &= 
    \E \left( \E \exp uZ_1 \right)^N \\
    &= \sum_{k=0}^{\infty} \left( \E \exp u Z_1 \right)^k \frac{\lambda^k}{k!} e^{-\lambda} \\
    &= \exp \left( -\lambda \E u Z_1  - \lambda\right)  \\
    &= \exp \left[  \lambda\left( \E e^{uZ_1} -1 \right)  \right].
\end{align*}

\problem{Benfordsches Gesetz.}
Gegeben sei eine diskret verteilte Zufallsvariable $X$ mit den Werten in der Menge
$\left\{ 1,2,3,\dots ,9 \right\}$ und der Wahrscheinlichkeitsfunktion
\begin{align*}
    p(d) = P(X = d) &= \log_{10} \left( \frac{d+1}{d} \right),\  d \in \left\{ 1,2,\cdots,9 \right\}.
\end{align*}
\begin{enumerate}
    \item Zeigen Sie, dass die Funktion $p$ tatsächlich eine Wahrscheinlichkeitsfunktion ist.
    \item Berechnen Sie den Erwartungswert von $X$.
    \item Berechnen Sie die Verteilungsfunktion von $X$. 
\end{enumerate}
Es sei an folgende Rechenregeln für Logarithmen erinnert:
\begin{align*}
    \log_{10} (a) + \log_{10} (b) &= \log_{10} \left( ab \right), &
    c \log_{10} \left( b \right) &= \log_{10}\left( b^{c} \right),
\end{align*}
für alle $a,b>0$ und $c\in\R$.

\solution
\begin{enumerate}
    \item Offenbar gilt $1<\frac{d+1}{d}<10$ für alle $d\in{1,2,\hdots,9}$. Wegen der strengen Monotonie des Logarithmus ergibt dies
          $0=\log(1)<\log\left(\frac{d+1}{d}\right)<\log(10)=1$.
          Also hat die Funktion $p$ Werte in dem Einheitsintervall $\left[ 0,1 \right]$ und erfüllt
        \begin{equation*}
            \sum_{d=1}^{9} p(d) = \sum_{d=1}^9 \log_{10} \left(\frac{d+1}{d} \right) = \log_{10} \left(\prod_{d=1}^9 \frac{d+1}{d} \right) = \log_{10} \left(\frac{10}{1}\right)=1.
        \end{equation*}
    \item \begin{align*}
            \E X &= \sum_{d=1}^{9} d\, p(d) 
            = \sum_{d=1}^{9} d\, \log_{10} \left( \frac{d+1}{d} \right)
            = \log_{10} \prod_{d=1}^{9} \left( \frac{d+1}{d} \right)^{d} \\
            &= \log_{10} \left( 10 \cdot{} \frac{10}{2} \dots \frac{10}{9} \right) 
            = 9 - \log_{10} 9! = 3.4402.
        \end{align*}
    \item Die Verteilungsfunktion von $X$ ausgewertet an $k\in \left\{ 1,\dots ,9 \right\}$ ist
        \begin{align*}
            F(k) = P(X \le k) = \sum_{d=1}^{k} \log_{10} \left( \frac{d+1}{d} \right) = \log_{10} (k+1).
        \end{align*}
        Für $x\in\R$ gilt daher
        \begin{align*}
            F(x) = P(X \le x) = \begin{cases}
                0,           & \text{falls }       x < 1, \\
                \log_{10} 2, & \text{falls } 1 \le x < 2, \\
                \log_{10} 3, & \text{falls } 2 \le x < 3, \\
                \dots        & \dots \\
                \log_{10} 9, & \text{falls } 8 \le x < 9, \\
                1,           & \text{falls }  9 \le x.
            \end{cases}
        \end{align*}
\end{enumerate}

\problem{Rademacher-verteilte Zufallsgrößen.} Sei $X$ Rademacher-verteilt mit der
Wahrscheinlichkeitsfunktion
\begin{align*}
    P(X=-1) &= \frac{1}{2}, & P(X=1)& = \frac{1}{2}.
\end{align*}
\begin{enumerate}
    \item Berechnen Sie den Erwartungswert und die Varianz von $X$. 

    \item Zeigen Sie, dass alle ungeraden Momente und alle ungeraden zentralen
        Momente von $X$ verschwinden. 

    \item Berechnen Sie die charakteristische Funktion von $X$. 

    \item Seien $X_1$ und $X_2$ stochastisch unabhängig und rademacherverteilt.
        Berechnen Sie die Wahrscheinlichkeitsfunktion der Zufallsgröße
        \begin{equation*}
            Y = X_1 + X_2.
        \end{equation*}
\end{enumerate}

\solution
\begin{enumerate}
    \item $\bfE X= 0$, $\bfD^2 X = \bfE X^2 = 1$.
    \item $k\in 2\bN+1$ impliziert $X^k= X$, daher auch $m_{k} = \bfE X^k = \bfE
        X=0$. Die zentralen Momente $\mu_k$ und Momente $m_{k}$ sind gleich. 
    \item $\phi_{X}(t) = \bfE e^{i t X} = \frac{1}{2} \left(  e^{it} + e^{-it} \right)$.
    \item \begin{equation*}
            P(Y=k) =
            \begin{cases}
                1/4 & k=2 \\
                1/2 & k=0 \\
                1/4 & k=-2.
            \end{cases}
        \end{equation*}
\end{enumerate}


\section{Ungleichungen der Wahrscheinlichkeitstheorie}

\problem{Tschebyscheff'sche Ungleichung. 100 Würfel.} Ein idealer Würfel
wird $100$ mal geworfen. Sei $X$ die Summe der geworfenen Augenzahlen. Finden
Sie eine obere Schranke für die Wahrscheinlichkeit $\bP( | X - 350 | > 50)$. 

\solution Bezeichne mit $X_i$, $i\in \left\{ 1,\dots ,100 \right\}$ die geworfenen
Augenzahlen und $X= X_1 +\dots + X_{100}$. Dann ist
\begin{align*}
    \bE X_i &= \frac{7}{2}, & \Var X_i &= \bE X_i^{2} - \left( \bE X_i \right)^{2} = \frac{91}{6} - \frac{49}{4}, \\
    \bE X &= 350, & \Var X &= \frac{875}{3}.
\end{align*}
Daraus erhalten wir
\begin{equation*}
    \bP \left( | X - \bE X | > 50 \right) = \bP \left( | X - 350  | > 50 \right) < \frac{\Var X}{ {50}^2 } = \frac{875}{ 3 \cdot {50}^{2}} \approx 0.1167.
\end{equation*}

\problem{Markow'sche Ungleichung. Diskreter Fall.} Sei $X$ eine Zufallsvariable
mit den Werten in $\left\{ 0,1,2,\dots \right\}$ und $a> 0$. Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item $\bP \left( X \geq a \right) \leq \frac{\bE X}{a}$.

    \item Sei $h: \bR \to [0,\infty)$ eine wachsende Funktion. Dann gilt
        \begin{equation*}
            \bP \left( X \geq a \right) \leq \frac{\bE h(X)}{ h(a)}. 
        \end{equation*}

    \item Zeigen Sie, dass sich die Tschebyscheff'sche Ungleichung aus der
        obigen Aussage folgern lässt. 
\end{enumerate}

\solution
\begin{enumerate}
    \item \begin{align*}
            \bP( X > a ) &= \sum_{n > a} \bP \left( X = n \right) \\
            &< \frac{1}{a} \sum_{n>a} n \bP( X = n ) \\
            &\leq \frac{1}{a} \sum_{n \geq 0} n \bP(X = n) = \frac{\bE X}{ a}.
        \end{align*}
    \item Folgt aus $\frac{h(n)}{h(n)} \leq \frac{h(n)}{h(a)}$ für alle $n>a$.
    \item Setze $h(x) = x^{2}$ und $\tilde X = | X - \bE X |$.
\end{enumerate}


\section{Diskrete Zufallsvariablen - Textaufgaben} 

\problem{Busfahrerstreik.} Auf dem Weg zur Stochastikprüfung erfahren Sie von
einem Busfahrerstreik. Sie kommen an der Bushaltestelle kurz vor 11:00 Uhr an,
die Prüfung beginnt um 14:00 Uhr und die Dauer der Busfahrt inklusive des
Fußweges zum Prüfungsort beziffern Sie erfahrungsgemäß auf eine Stunde. Gemäß
Fahrplan stehen Ihnen drei Busse mit Abfahrtszeiten 11:00 Uhr, 12:00 Uhr und 13:00 Uhr zur
Verfügung. Laut Aussagen anderer Wartender ist mit einer mittleren
Verspätungsdauer von einer Stunde zu rechnen. Sie nehmen an, dass die
Verspätungsdauern der Busse $B_i$  f\"ur $i=1,2,3$ unabhängig voneinander und exponentialverteilt mit $B_i \sim \ExpDist(\lambda)$ und
$\lambda>0$ sind. 
\begin{enumerate}
    \item Bestimmen Sie den Parameter $\lambda$ und berechnen Sie die erwartete
        Ankunftszeit am Prüfungsort unter der Annahme, dass Sie den Bus mit der
        planmäßigen Abfahrtszeit 11:00 Uhr benutzen.
    \item Berechnen Sie die Wahrscheinlichkeit dafür, dass Sie nicht rechtzeitig 
        zur Prüfung kommen können.
    \item Berechnen Sie die Wahrscheinlichkeit dafür, dass Sie mindestens eine halbe Stunde
        vor der Prüfung am Prüfungsort ankommen.
\end{enumerate}

\solution
\begin{enumerate}
    \item $\E B_i = 1$ impliziert $\lambda=1$. Die erwartete Ankunftszeit bei
        Fahrt mit dem ersten Bus ist $\E \left[ 11 + 1 + B_1  \right] = 12 +
        \E B_1 = 13$ Uhr.
    \item Bezeichne mit $B_1$, $B_2$, und $B_3$ die Verspätungsdauer der Busse.
        Laut Angabe sind $B_i$ exponentialverteilt mit  $B_i \sim
        \ExpDist(1)$. Die Wahrscheinlichkeit dafür, dass der Prüfungsort nicht
        rechtzeitig erreicht werden kann ist
        \begin{align*}
            P\left( B_1 \geq 2 \ \wedge \ B_2 \geq 1 \ \wedge B_3 \geq 0 \right) &= 
            P(B_1 \geq 2) P(B_2 \geq 1) \\
            &= \exp\left( -2 \right) \exp( -1) \approx 0.04978.
        \end{align*}

    \item Die Wahrscheinlichkeit, dass wir mindestens halbe Stunde vor der
        Prüfung ankommen, ist
        \begin{align*}
            P( B_1 \leq 1.5 \ \vee \ B_2 \leq 0.5  ) &= 1 - P(B_1 > 1.5 )P(B_2 > 0.5) 
            \approx 0.8646. 
        \end{align*}
\end{enumerate}


\problem{Hochwasserereignisse.} Historiker haben festgestellt, dass im
Großraum Halle im Mittel zwei extreme Hochwasserereignisse pro Jahrhundert zu
verzeichnen waren. Wir setzen voraus, dass dafür die Annahmen des
Poisson-Prozesses (Stationarität, Homogenität und Ordinarität) gelten mögen.
\begin{enumerate}
    \item Wie können diese drei Annahmen für die vorliegende Problematik interpretiert werden?
%        Was bedeuten diese drei Annahmen für die vorliegende Problematik?
        
    \item Wie wahrscheinlich ist es, dass in einem Jahrhundert mehr als drei
        extreme Hochwasserereignisse im Großraum Halle auftreten?
    \item Wie wahrscheinlich ist es, dass nach dem extremen Hochwasser 2013
        innerhalb von 20 Jahren ein weiteres extremes Hochwasser in dieser
        Region auftreten wird?  
\end{enumerate}

\solution
\begin{enumerate}
        \setcounter{enumi}{1}
    \item Rechnungseinheit Jahre: $\mu=\frac{2}{100}=0.02, \quad X_t \sim \text{Poiss}(\mu t).$ 
        Sei $\pi_{\lambda}$ die Wahrscheinlichkeitsfunktion der Poissonverteilung mit dem Parameter $\lambda$. Dann ist
        $t=100$, $\lambda=\mu t=2$, $P(X_{100}>3)=1-\pi_2(0)-\pi_2(1)-\pi_2(2)-\pi_2(3)= 0.1429$.
    \item Zeitraum bis zum nächsten Ereignis: $T \sim \text{Exp}(\mu)$.
        $P(T \le 20)=1-\exp(-20 \mu)=1-\exp(-0.4)=0.3297.$
\end{enumerate}

% Bernd's solution
%\begin{enumerate}
%        \setcounter{enumi}{1}
%    \item Rechnungseinheit Jahre: $\mu=\frac{2}{100}=0.02, \quad X_t \sim \pi_{\mu t}.$
%        
%        $t=100,\;\lambda=\mu t=2, \quad P(X_{100}>3)=1-\pi_2(0)-\pi_2(1)-\pi_2(2)-\pi_2(3)=\underline{0.1429}.$
%
%    \item Zeitraum bis zum nächsten Ereignis: $\,T \sim \mathbf{Ex}(\mu)$.
%
%        $P(T \le 20)=1-\exp(-20 \mu)=1-\exp(-0.4)=\underline{0.3297}.$
%\end{enumerate}



\problem{Gasmelder.}  Im Hauptgebäude der TU Chemnitz werden Gasmelder
bestimmten Typs für die Früherkennung gefährlicher Methankonzentrationen
eingesetzt. Es wird angenommen, dass im $3.$ Stock im Mittel alle $25$ Jahre
ein gefährlicher Methanaustritt stattfindet, der durch den entsprechenden
Gasmelder signalisiert wird. Unabhängig davon geht der besagte Melder im Mittel
alle $5$ Jahre kaputt und signalisiert dabei einen Falschalarm. Der defekte Melder
wird aus Sicherheitsgründen immer sofort ausgetauscht.
\begin{enumerate}
    \item Wir setzen im Folgenden voraus, dass beide Ereignisfolgen mit Hilfe
        des Poisson-Prozesses modelliert werden können. Diskutieren Sie die
        zugehörigen Annahmen der Stationarität, Homogenität und Ordinarität.

    \item Wie wahrscheinlich ist es, dass es in den nächsten $10$ Jahren zu mehr als
        einem Methanaustritt kommen wird?

    \item Wie wahrscheinlich ist es, dass nach dem Austausch des defekten Melders Ende Mai der neue Melder innerhalb eines halben Jahres kaputt geht?
\end{enumerate}

\solution
\begin{enumerate}
    \item 
    \item Wir rechnen alles in der Einheit Stunden.\\$M$ zähle Methanvorfälle und $G$ zähle Gasmelderausfälle.  Es
        gilt $M_t \sim \pi(\mu t)$ und $G_t \sim \pi(\gamma t)$. Aus $\E
        M_{25}=1=25\, \mu$ und $\E G_5 = 1 = 5\, \gamma$ erhalten wir
        $\mu=0.04$ und $\gamma=0.2$. Daher gilt
        \begin{align*}
            P( M_{10} > 1) &= 1 - P(M_{10}=0) - P(M_{10}=1)\\
           &= 1-\pi_{0.4}(0)-\pi_{0.4}(1)=0.06155.
        \end{align*}
    \item Die Zeiten zwischen den Ausfällen der Gasmelder sind
        exponentialverteilt mit dem Parameter $\gamma$. Es gilt also
        \begin{align*}
            P( T \leq 0.5 ) = 1 - \exp(-\gamma\, 0.5) = 0.0951. 
        \end{align*}
\end{enumerate}

\problem{Vulkanausbr\"uche.} Das Vulkangebiet um Indonesien l\"asst im Mittel einen Vulkanausbruch
in f\"unf Jahren erwarten. Wir setzen voraus, dass dafür die Annahmen des
Poisson-Prozesses (Stationarität, Homogenität und Ordinarität) gelten mögen.
\begin{enumerate}
    \item Wie können diese drei Annahmen für die vorliegende Problematik interpretiert werden?

    \item Wie wahrscheinlich ist es dann, dass es in einem Jahrhundert zu gar keinem Vulkanausbruch in diesem
    Gebiet kommt?
    \item Wie wahrscheinlich ist es andererseits, dass es nach einem Vulkanausbruch bereits innerhalb eines Jahres zu einem
    weiteren Vulkanausbruch kommt?
\end{enumerate}

\solution
\begin{enumerate}
        \setcounter{enumi}{1}
    \item Rechnungseinheit Jahre: $\mu=\frac{1}{5}=0.2, \quad X_t \sim \text{Poiss}(\mu t).$
        Sei $\pi_{\lambda}$ die Wahrscheinlichkeitsfunktion der Poissonverteilung mit dem Parameter $\lambda$. Dann ist
        $t=100$, $\lambda=\mu t=20$, $P(X_{100}=0)=\pi_{20}(0)= \exp(-20)=2.0611 \times 10^{-9}$.
    \item Zeitraum bis zum nächsten Ereignis: $T \sim \ExpDist(\mu)$.
        $P(T \le 1)=1-\exp(-\mu)=1-\exp(-0.2)=0.1813.$
\end{enumerate}

\problem{Weinfehler. Verteilungsfunktion.} Aus einem sehr großen Lieferposten
von Wein werden $100$ Flaschen zufällig herausgegriffen und auf Weinfehler
getestet.  Der Wein sei ungenießbar, wenn der Anteil an $2,4,6$-Trichloranisol
(TCA) über $5$ ng/l liegt -- das zeichnet sich durch einen korkigen Geruch und
Geschmack aus.  Der durchschnittliche Anteil unbrauchbarer Weinflaschen betrage
$3\%$. 
\begin{enumerate}
    \item Wie groß ist die Wahrscheinlichkeit dafür, dass unter $100$ zufällig 
        ausgewählten Weinflaschen genau $3$ ungenießbare sind? 
    \item Berechnen Sie die Wahrscheinlichkeit dafür, dass unter diesen $100$
        Weinflaschen höchstens drei ungenießbare enthalten sind. 
\end{enumerate}

\solution 
\begin{enumerate}
    \item $X\sim \text{Bin}(n, p)$ mit $n=100$ und $p=0.03$. 
        \begin{align*}
            P(X=3) &= \binom{n}{3} p^{3}(1-p)^{n-3} \\
            &= \frac{100!}{3!\, 97!}\, {0.03}^{3}\left( 1-0.03 \right)^{97} \\
            &= \frac{98\cdot 99\cdot 100}{ 6}\, {0.03}^{3}\, {0.97}^{97} = 0.2275.
        \end{align*}
    \item \begin{align*}
            P(X\leq 3) &= P(X=0)+\dots+P(X=3) \\
            &= \sum_{k=0}^{3} \binom{n}{k}\, {0.03}^{k}\, {0.97}^{n-k} = 0.6472.
        \end{align*}
\end{enumerate}
Numerische Auswertung solcher Wahrscheinlichkeiten erfolgt mithilfe  der Grenzverteilungsätze.


\problem{Grenzverteilungsätze. Weinfehler.} Aus einem sehr großen Lieferposten
Wein werden $500$ Flaschen zufällig herausgegriffen und auf Weinfehler
getestet. Der Wein sei ungenießbar, wenn der Anteil an $2,4,6$-Trichloranisol
%(TCA) über $5$ ng/l liegt -- das zeichnet sich durch einen korkigen Geruch und
%Geschmack aus. Der durchschnittliche Anteil unbrauchbarer Weinflaschen betrage
(TCA) über $5$ ng/l liegt. Dies wirkt sich in Form eines korkigen Geruchs und
Geschmacks aus. Der durchschnittliche Anteil unbrauchbarer Weinflaschen betrage
$3\,\%$. 
\begin{enumerate}
    \item Wie groß ist die Wahrscheinlichkeit dafür, dass unter $500$ zufällig 
        ausgewählten Weinflaschen höchstens $20$ ungenießbare enthalten sind? 
    \item Es wird eine Lieferung für einen wichtigen Kunden vorbereitet. Es
        muss sichergestellt werden, dass mit mindestens $99\,\%$
        Wahrscheinlichkeit $500$ Flaschen in der Lieferung keine Weinfehler
        aufweisen. Ein Mitarbeiter schlägt vor, $525$ Flaschen auszuliefern.
        Ist die obige Bedingung in diesem Fall erfüllt?
        
%        Wie viele Weinflaschen müssen ausgeliefert werden, sodass mit
%        mindestens $99\%$ Wahrscheinlichkeit $500$ Flaschen in der Lieferung
%        keine Weinfehler aufweisen? 
\end{enumerate}

\solution 
\begin{enumerate}
    \item Mit $n=500$ und $p=0.03$, erhalten wir $np = 15$ und $np(1-p) = 14.55 > 9$.
        Sei $X \sim \text{Bin}(500,0.03)$.
        \begin{align*}
            P\left( X \leq 20 \right) &= 
                P \left( \frac{ X - np}{ \sqrt{np(1-p)} } \leq \frac{20 - 15}{ \sqrt{14.55}} \right) \\
                &\approx \Phi\left( 1.310806 \right) = 0.9050384. 
        \end{align*}

    \item Die Anzahl der trinkbaren Flaschen werde wir mit $Y \sim
        \text{Bin}(m, 0.97)$ bezeichnet. 
        \begin{align*}
            P\left( Y \geq 500 \right) & \geq 0.99 \\
            P\left( Y<500 \right) & \leq 0.01 \\
            \Phi\left( \frac{500 - m\, 0.97}{\sqrt{m}\sqrt{0.03\cdot 0.97}} \right) & \leq 0.01. \\
        \end{align*}
        $m=525$ liefert
        \begin{align*}
            \frac{500 - m 0.97}{\sqrt{m}\sqrt{0.03\cdot 0.97}} &= -2.3665 \\
            \Phi(-2.3665) &= 1 - \Phi( 2.3665 ) \leq 0.01.
        \end{align*}
\end{enumerate}



\problem{Reifenpannen.} Auf einer bestimmten Rallye-Strecke von $10\, 000$ km
sei pro Pkw im Mittel mit einer Reifenpanne zu rechnen. 
\begin{enumerate}
    \item Ermitteln Sie die Wahrscheinlichkeit dafür, dass ein Pkw bereits auf
        den ersten $7\, 000~\text{km}$ eine Reifenpanne hat. 
    \item Eine Mannschaft sei mit drei Pkw an der Rallye beteiligt. Berechnen
        Sie die Wahrscheinlichkeit dafür, dass alle Pkw ohne Reifenpanne die
        Rallye überstehen. 
\end{enumerate}

\solution
\begin{enumerate}
    \item Wir nehmen an, dass die Anzahl und Auftreten der Reifenpannen mit
        Hilfe der Poisson-Verteilung modelliert werden kann, da die
        Voraussetzungen der Stationarität, Homogenität und Ordinarität erfüllt
        sind.  Wir bezeichnen mit $X_t$ die Anzahl der Reifenpannen bis zum
        Zeitpunkt $t$.  Wir wissen $\bE X_{10^4} = 1$. Es gilt $\bE X_{t} = \mu
        t$ und daher $\mu = 10^{-4}$. Die gesuchte Wahrscheinlichkeit ist
        \begin{align*}
            P(X_{7000} \geq 1) &= \sum_{i=1}^{\infty} e^{-7000 \mu} \frac{ \left( 7000 \mu \right)^{i} }{i!} \\
            &= 1 - e^{-7000 \mu} \frac{ (7000 \mu)^{0} }{0!} \\ 
            &= 1 - e^{-\frac{7}{10}} \approx 0.5034.
        \end{align*}

    \item Die Reifenpannen in den einzelnen Fahrzeugen modellieren wir mit
        $X_{1,t}, X_{2,t}, X_{3,t} \sim \text{Poiss}(\mu t)$. Dann ist
        $Y_t=X_{1,t}+X_{2,t}+X_{3,t}$ und $Y \sim \text{Poiss}(3\mu t)$ die
        Anzahl der Reifenpannen in allen drei Fahrzeugen bis zum Zeitpunkt $t$.
        \begin{align*}
            P(Y_{10000} = 0) &= e^{ - 3 \mu t } = e^{-3} \approx 0.0497.
        \end{align*}
\end{enumerate}


\problem{Beschichtungsfehler.} Die Anzahl der Beschichtungsfehler einer
speziellen Magnetbandsorte betrage pro $1\, 000$ m im Mittel $1.6$. Wie groß
ist die Wahrscheinlichkeit dafür, dass ein $500$ m langes Bandstück keine
Fehler aufweist? Welches Model verwenden Sie um diese Zufallssituation zu
modellieren? 


\problem{Übertragungskanal.} Die Fehlerrate eines Übertragungskanals hängt
von der Entfernung $D$ (im m) zwischen Quelle und Senke ab. Die
Wahrscheinlichkeit, dass ein Bit falsch übertragen wird, beträgt $p =
0.001\cdot{} D$ für $D\leq 500\,\text{m}$ und $p=0.5$ für $D>500\,\text{m}$.
Das heißt, bei Entfernungen von über $500\,\text{m}$ wird nur noch Rauschen an
der Senke empfangen.

Es wird eine $512$ Bit Nachricht zur Übertragung vorbereitet.  
\begin{enumerate}
    \item Die Entfernung sei $D=5\,$m. Berechnen Sie unter Benutzung eines
        geeigneten Grenzverteilungssatzes die Wahrscheinlichkeit dafür, dass
        mehr als $2$ Bits von der obigen Nachricht falsch übertragen werden. 

    \item Zur Verminderung der Fehlerrate wird jedes Bit der Nachricht $3$ mal
        hintereinander übertragen. Ein Decodieralgorithmus an der Senke zerlegt
        die Nachricht in $3$ Bit lange Teile und gibt für jeden Teil $1$ aus,
        falls $2$ oder $3$ Bits gleich $1$ sind und sonst $0$. Berechnen Sie
        bei $D=5\,m$ die Wahrscheinlichkeit dafür, dass mehr als $2$ Bits
        von der ursprünglichen $512$ Bits langen Nachricht falsch übertragen
        werden.
\end{enumerate}

\solution
\begin{enumerate}
    \item Wir approximierten die Binomialverteilung mit Hilfe des Poissonschen
        Grenzwertsatzes. Bei $p=0.005$, $n=512$, $np=2.56$ und $1500p = 7.5$
        ist die Faustregel $np \leq 10$ und $1500p\leq n$ erfüllt. Sei $X\sim
        \text{Bin(512, 0.005)}$ die Anzahl der falsch übertragenen Bits. Nun
        setzen wir $\lambda = np = 2.56$. Der Grenzversatz von Poisson liefert
        \begin{equation*}
            P(X=k) \approx e^{-\lambda } \frac{\lambda^{k}}{k!}
        \end{equation*}
        und
        \begin{align*}
            P\left( X>2 \right) &= 1 - P(X \leq 2) \\
            &= 1 - P(X=0)-P(X=1)-P(X=2) \approx 0.528517.
        \end{align*}
    \item Die Wahrscheinlichkeit für die falsche Übertragung eines Bits beträgt
        in diesem Verfahren
        \begin{equation*}
            p_3 = \binom{3}{3} p^{3}\left( 1-p \right)^{0} + \binom{3}{2} p^{2} \left( 1-p \right)^{3-2} = 7.475\, 10^{-5}.
        \end{equation*}
        Wir erhalten also $n p_3 = 0.038272$. Der Grenzvertsatz von Poisson liefert
        \begin{align*}
            P\left( X>2 \right) &= 1 - P(X \leq 2) \\
            &= 1 - P(X=0)-P(X=1)-P(X=2) \approx 9.079004\, 10^{-6}.
        \end{align*}
\end{enumerate}

\problem{Übertragungskanal mit stochastischer Fehlerkorrektur.} Die Fehlerwahrscheinlichkeit
bei der Übertragung eines Bits ist $p=0.1$. Es wird eine $512$ Bit Nachricht zur Übertragung
vorbereitet.
\begin{enumerate}
    \item Berechnen Sie unter Benutzung eines geeigneten Grenzverteilungssatzes
        die Wahrscheinlichkeit dafür, dass mehr als $35$ Bits der obigen
        Nachricht falsch übertragen werden.

    \item Zur Verminderung der Fehlerrate wird jedes Bit der Nachricht viermal
        hintereinander übertragen. Ein Decodieralgorithmus beim Empfänger zerlegt
        die Nachricht in $4$ Bit lange Teile und gibt für jeden Teil $1$ aus,
        falls $3$ oder $4$ Bits gleich $1$ sind und $0$ falls $3$ oder $4$ Bits
        gleich $0$ sind. Falls zwei Einser und zwei Nuller empfangen wurden,
        wird das Ergebnis eines unabhängigen Bernoulliexperiments $X\sim
        \text{Ber}(0.5)$ ausgegeben. 
        
        Berechnen Sie die Fehlerwahrscheinlichkeit bei der Übertragung eines
        Bits für den modifizierten Algorithmus. 
\end{enumerate}

\solution
\begin{enumerate}
    \item Sei $n=512$, $k=35$ und $X \sim \text{Bin}(n,
        p)$. $\E X = np= 51.2$, $\sigma^{2}_{X} = 46.08$. Die Wahrscheinlichkeit,
        dass mehr als $k$ Bits falsch übertragen werden, ist 
        \begin{align*}
            P\left( X > k \right) &= 1 - P\left( X \leq k \right) \\
            &= 1- P\left( \frac{X - np}{\sqrt{np(1-p)}} \leq \frac{k -np}{ \sqrt{np(1-p)}} \right) \\
            &\approx 1 - \Phi\left( \frac{k -np}{ \sqrt{np(1-p)}}  \right) =  0.9914949.
        \end{align*}
        Die Faustregel rechtfertigt dabei die Normalapproximation. 
    \item Die Wahrscheinlichkeit für die falsche Übertragung eines Bits bei dem alternativen 
        Algorithmus ist 
        \begin{align*}
            \binom{4}{4} p^{4} + \binom{4}{3} p^{3}(1-p) + \binom{4}{2} p^2 (1-p)^{2} \frac{1}{2} = 0.028.
        \end{align*}
        % p^4 + 4*(p^3)*(1-p) + 6*(p^2)*( (1-p)^2 )*0.5 
\end{enumerate}


\problem{Übertragungskanal. II} Die Fehlerrate eines Übertragungskanals hängt
von der Entfernung $D$ (im m) zwischen Quelle und Senke ab. Die
Wahrscheinlichkeit, dass ein Bit falsch übertragen wird, beträgt $p =
0.001\cdot{} D$ für $D\leq 500\,\text{m}$ und $p=0.5$ für $D>500\,\text{m}$.
Das heißt, bei Entfernungen von über $500\,\text{m}$ wird nur noch Rauschen an
der Senke empfangen.

Es wird eine $512$ Bit Nachricht zur Übertragung vorbereitet.  
\begin{enumerate}
    \item Die Entfernung sei $D=100\,$m. Berechnen Sie unter Benutzung eines
        geeigneten Grenzverteilungssatzes die Wahrscheinlichkeit dafür, dass
        mehr als $35$ Bits von der obigen Nachricht falsch übertragen werden. 

    \item Zur Verminderung der Fehlerrate wird jedes Bit der Nachricht $3$ mal
        hintereinander übertragen. Ein Decodieralgorithmus an der Senke zerlegt
        die Nachricht in $3$ Bit lange Teile und gibt für jeden Teil $1$ aus,
        falls $2$ oder $3$ Bits gleich $1$ sind und sonst $0$. Berechnen Sie
        bei $D=100\,m$ die Wahrscheinlichkeit dafür, dass mehr als $35$ Bits
        von der ursprünglichen $512$ Bits langen Nachricht falsch übertragen
        werden.

%    \item Wie groß darf die Entfernung $D$ höchstens betragen, wenn mit
%        Wahrscheinlichkeit $0.95$ mindestens $900$ Bits richtig übertragen
%        werden sollen.  Benutzen Sie dazu einen geeigneten Grenzwertsatz und
%        berechnen Sie $D$ indem Sie eine quadratische Gleichung lösen.
\end{enumerate}

\solution
\begin{enumerate}
    \item Sei $p = D\cdot{} 0.001 = 0.1$, $n=512$, $l=35$ und $X \sim \bfB(n,
        p)$. $\bfE X = np= 51.2$, $\bfD^2 X = 46.08$. Die Wahrscheinlichkeit,
        dass mehr als $l$ Bits falsch übertragen werden, ist 
        \begin{align*}
            P\left( X > l \right) &= 1 - P\left( X \leq l \right) \\
            &= 1- P\left( \frac{X - np}{\sqrt{np(1-p)}} \leq \frac{l -np}{ \sqrt{np(1-p)}} \right) \\
            &\approx 1 - \Phi\left( \frac{l -np}{ \sqrt{np(1-p)}}  \right) =  0.9914949.
        \end{align*}
        Die Faustregel rechtfertigt dabei die Normalapproximation. 
    \item Die Wahrscheinlichkeit für die falsche Übertragung eines Bits beträgt
        in diesem Verfahren
        \begin{equation*}
            p_3 = \binom{3}{3} p^{3}\left( 1-p \right)^{0} + \binom{3}{2} p^{2} \left( 1-p \right)^{3-2} = 
            0.028.
        \end{equation*}
        Wir erhalten also $n p_3 = 2.8$ und $n p_3 (1-p_3) = 13.93459$ und
        \begin{equation*}
            P\left( \tilde X > l \right) =  1.550534 \cdot{} 10^{-8}. 
        \end{equation*}
\end{enumerate} 
R code:
\begin{lstlisting}
    f <- function(n,p,l) { 1 - pnorm( (l- n*p)/sqrt( n*p*(1-p))) }
    n <- 512; p <- 0.1; l <- 35; f(n,p,l)
    # 0.9914949
    n <- 512; p <- 0.028; l <- 35; f(n,p,l)
    # 1.550534e-08
\end{lstlisting}


\problem{Bitcoin. Einfaches Bubble-Modell.} In einem einfachen Marktmodel für
den Euro-Wert einer Bitcoin wird davon ausgegangen, dass es innerhalb eines
Jahres mit Wahrscheinlichkeit $p=0.95$ zu einem Crash kommen wird. Der
Crash-Zeitpunkt $\tau$ ist dabei exponentialverteilt mit dem Parameter
$\lambda$. Gleichzeitig nehmen wir an, dass der Euro-Wert $b(t)$ einer Bitcoin
(1 BTC) durch die Funktion
\begin{equation*}
    b(t) =
    \begin{cases}
        500 \exp( t ) & t \leq \tau \\
        0 & \tau < t
    \end{cases}
\end{equation*}
gegeben ist. $t$ bezeichnet dabei die Zeit in Jahren. 
\begin{enumerate}
    \item Berechnen Sie den Wert des Parameters $\lambda$.
    \item Berechnen Sie den erwarteten Euro-Wert von $1$ BTC zum Crash-Zeitpunkt.
\end{enumerate}

\solution
\begin{enumerate}
    \item Die Dichte und die Wahrscheinlichkeitsfunktion der Exponentialverteilung sind gegeben 
        durch
        \begin{align*}
            p(t) &= \lambda \exp\left( -\lambda t \right), & P(\tau \leq t) = 1- \exp( - \lambda t). 
        \end{align*}
        Aus $P( \tau \leq 1 ) = 0.95$ erhalten wir $\lambda = - \log\left( 0.05 \right) \approx 2.9957$.
    \item Erwarteter Preis zum Crash-Zeitpunkt ist 
        \begin{align*}
            \E b(\tau) &= 500 \frac{\lambda}{\lambda-1} \approx 750.5346.
        \end{align*}
\end{enumerate}


\section{Stetige Zufallsvariablen}


\problem{Exponentialverteilung. Minima.}
Seien $X_1, \dots, X_n$ stochastisch unabhängig und exponentialverteilt mit
$X_i\sim \mathbf{Ex}(\lambda_i)$ und $\lambda_i>0$. Zeigen Sie, dass die
Zufallsgröße 
\begin{equation*}
    X_{(1)} = \min \left\{ X_1, \dots, X_n \right\} 
\end{equation*}
exponentialverteilt ist und dabei $X_{(1)} \sim \mathbf{Ex}(\lambda_1+\dots+\lambda_n)$ gilt.


\problem{Exponentialverteilung. Dichte des Maximums.} Seien $X_1, \dots, X_n$
stochastisch unabhängig und exponentialverteilt mit $X_i\sim
\text{Exp}(\lambda)$ und $\lambda>0$. Berechnen Sie die Dichtefunktion der
Zufallsvariable $X_{(n)} = \max \left\{ X_1,\dots ,X_n \right\}$.

\solution Die Verteilungsfunktion von $X_{(n)}$ ist 
\begin{align*}
    P\left( X_{(n)} \leq t \right) &= P\left( X_1 \leq t,\dots ,X_n \leq t \right) \\
    &= P\left( X_1 \leq t \right)^{n} \\
    &= \left( 1- e^{-\lambda t} \right)^{n}. 
\end{align*}
Die Dichte von $X_{(n)}$ ist demnach
\begin{equation*}
    f(x) = n \left( 1- e^{-\lambda x} \right)^{n-1} \lambda e^{-\lambda x}. 
\end{equation*}


\problem{Momente der Exponentialverteilung.} 
Sei $X$ exponentialverteilt mit dem Parameter $\lambda>0$. $X$ hat also die Dichte
\begin{equation*}
    p(x) = \lambda e^{-\lambda x } 1_{\R_{\geq 0}}(x)
\end{equation*}
\begin{enumerate}
    \item Die Verteilungsfunktion von $X$ ist gegeben durch
        \begin{equation*}
            P( X \leq x) = \left[ 1 - e^{-\lambda x} \right] 1_{\R_{\geq 0}}(x).
        \end{equation*}
    \item Die Momente der Exponentialverteilung sind für $n\in\bN$ durch
        \begin{equation*}
            \E X^n = \frac{n!}{\lambda^{n}}
        \end{equation*}
        gegeben. 
    \item Der Erwartungswert und die Varianz der Exponentialverteilung sind
        \begin{align*}
            \E X    & = \frac{1}{\lambda} & 
            \Var X  & = \frac{1}{\lambda^2}.
        \end{align*}
\end{enumerate}
\solution 
Wir berechnen
\begin{align*}
    P(X \leq x) & = 
    \int_{-\infty}^{x} \lambda e^{-\lambda x } 1_{\R_{\geq 0}}(x) dx \\
    &= \int_{0}^{x} \lambda e^{-\lambda x } dx \\
    &= \left[  - e^{-\lambda x}  \right] |_{0}^{x} \\
    &= \left[ 1 - e^{-\lambda x} \right] 1_{\R_{\geq 0}}(x).
\end{align*}
Die Momente erhalten wir mit Hilfe der partiellen Integration. 
\begin{align*}
    \E X^n &= \int_{0}^{\infty} x^n \lambda e^{- \lambda x} dx \\
    &= \int_{0}^{\infty} x^n\left( -e^{-\lambda x} \right)' dx \\
    &= x^n \left( - e^{-\lambda x} \right)|_{0}^{\infty} 
    - \int_{0}^{\infty} n x^{n-1} \left( -e^{-\lambda x} \right) dx  \\
    &= \frac{n}{\lambda} \int_{0}^{\infty} x^{n-1}\, \lambda e^{-\lambda x} dx \\
    & = \frac{n}{\lambda} \E X^{n-1}.
\end{align*}
Mit
\begin{equation*}
    \E X^{0} = \E 1 = 1
\end{equation*}
erhalten wir $\E X^n = \frac{n!}{\lambda^n}$ und daraus 
\begin{align*}
    E X &= \frac{1}{\lambda} &   \Var X &= \E X^2 - \left( \E X \right)^2 = 
    \frac{1}{\lambda^2}. 
\end{align*}


\problem{Gedächtnislosigkeit der Exponentialverteilung.}
Sei $X$ exponentialverteilt mit dem Parameter $\lambda>0$. Dann gilt für
$x,h>0$ 
\begin{equation*}
    P\left( X > x+h \vb X > x \right) = P( X > h).
\end{equation*}
Warum wird diese Eigenschaft von $X$ Gedächtnislosigkeit genannt?

\problem{Momentenerzeugende Funktion der Exponentialverteilung. }
Sei $X$ exponentialverteilt mit dem Parameter $\lambda>0$. 
\begin{enumerate}
    \item Die momentenerzeugende Funktion von $X$ ist 
        \begin{equation*}
            \psi (u) = \E \exp \left( u X \right) = - \frac{\lambda}{u - \lambda}
        \end{equation*}
        für $|u| < \lambda$. 
    \item Berechnen Sie mit Hilfe von $\psi(u)$ den Erwartungswert und die Varianz von $X$. 
\end{enumerate}
\solution 
Wir berechnen
\begin{align*}
    \psi(u) = \E \exp \left( u X \right) &= 
    \int_{\R}^{} \exp \left( ux \right) \lambda e^{-\lambda x} 1_{\R_{\geq 0}}(x) dx \\
    &= \lambda \int_{0}^{\infty} e^{(u-\lambda)x} dx \\
    &= \lambda \left( \frac{1}{u-\lambda} e^{(u-\lambda)x} \right) |_{0}^{\infty}\\
    &= -\frac{\lambda}{u- \lambda}. 
\end{align*}
Um die Momente zu erhalten, brauchen wir nur noch die momentenerzeugende Funktion 
zu differenzieren. 
\begin{align*}
    \E X = \frac{\partial}{ \partial u} \psi (u) |_{u=0} &= \frac{1}{\lambda} \\
    \E X^2 = \frac{\partial^2}{\partial u^2} \psi (u) |_{u=0} &=
    \frac{\partial}{\partial u} \frac{\lambda}{(u-\lambda)^2} = \frac{2}{\lambda^2}. 
\end{align*}

\problem{Minimum von exponentialverteilten Zufallsvariablen.}
Seien $X_1, \dots, X_n$ i.i.d.\ und exponentialverteilt mit $X\sim \text{Exp}(\lambda_i)$
und $\lambda_i>0$. Dann ist die Zufallsvariable 
\begin{equation*}
    Y = \min \left\{ X_1, \cdots, X_n \right\} 
\end{equation*}
wieder exponentialverteilt und es gilt $Y \sim
\text{Exp}(\lambda_1+\cdots+\lambda_n)$.

\problem{Gleichverteilung und Exponentialverteilung.} 
Sei $X$ stetig gleichverteilt auf dem Intervall $[0,1]$. Zeigen Sie, dass die
Zufallsvariable $Y = -\log X$ exponentialverteilt ist.

\solution $Y$ ist exponentialverteilt mit $\lambda=1$.

\problem{Erlang-$2$-Verteilung.} 
Sei $X$ stetig verteilt mit der Dichte \[f(x) = \lambda^{2} x e^{-\lambda x}\,
1_{(0,+\infty)}(x)\] und dem positiven Parameter $\lambda>0$.  Berechnen Sie
den Erwartungswert der Zufallsvariable $Y = \frac{1}{X}$.

\solution
$\E \frac{1}{X} = \int_{0}^{\infty} \frac{1}{x} \lambda^{2} x e^{-\lambda x} dx = \lambda$.

\problem{Stetige Gleichverteilung. Momente und Momentenerzeugende Funktion.}
Sei $X$ stetig gleichverteilt auf dem Intervall $\left( a,b \right)\subset \R$.  
Dies kann als $X \sim U(a,b)$ notiert werden. 
\begin{enumerate}
    \item Das $n$-te Moment $\E X^{n}$, für $n\in\bN$, ist 
        \begin{equation*}
            \E X^n = \frac{1}{n+1}\left( b^{n}a^0 + \cdots + b^{0}a^{n} \right).
        \end{equation*}
    \item Der Erwartungswert und die Varianz von $X$ sind gegeben durch
        \begin{align*}
            \E X    & = \frac{1}{2} \left( b+a \right) & 
            \Var X  & = \frac{1}{12} \left( b-a \right)^{2}.  
        \end{align*}
    \item Die momentenerzeugende Funktion von $X$ ist 
        \begin{equation*}
            \psi(u) = \E \exp \left( u X \right) = \frac{1}{u(b-a)}\left( e^{ub} - e^{ua} \right).
        \end{equation*}
\end{enumerate}

\solution Berechnen wir zunächst die $n$-ten Momente
\begin{align*}
    \E X^n &= \int_{\R}^{} x^n \frac{1}{b-a} 1_{(a,b)}(x) dx \\
    &= \frac{1}{b-a} \int_{a}^{b} x^n dx \\
    &= \frac{1}{b-a} \left( \frac{x^{n+1}}{n+1} \right) |_{a}^{b} \\
    &= \frac{1}{b-a}\frac{1}{n+1} \left( b^{n+1}-a^{n+1} \right) \\
    &= \frac{a^n b^{0} +\cdots+ a^0 b^{n}}{n+1}.
\end{align*}
Wir brauchen nur noch $n=1,2$ einzusetzen.
\begin{align*}
    \E X &= \frac{a+b}{2} & \E X^2 &= \frac{b^2 + ab + a^2}{3} & 
    \Var X = \frac{(b-a)^2}{12}.
\end{align*}
Die momentenerzeugende Funktion kann folgendermassen berechnet werden. 
\begin{align*}
    \psi(u) = \E \exp\left( uX \right) &= 
    \int_{\R} \exp \left( ux \right) \frac{1}{b-a} 1_{(a,b)}(x) dx \\
    &= \frac{1}{b-a} \int_{a}^{b} \exp (ux) dx \\
    &= \frac{e^{ub}-e^{ua}}{u(b-a)}.
\end{align*}
Beachten Sie, dass diese Funktion an Null beliebig oft differenzierbar ist. 


\problem{Stetige Gleichverteilung. Momente.}
Sei $X$ stetig gleichverteilt auf dem Intervall $\left[ a,b \right]\subset \R$.
Zeigen Sie folgende Aussagen: 
\begin{enumerate}
    \item Für $k=1,2,\dots$ ist das $k$-te Moment $\bE X^{k}$ gegeben durch
        \begin{equation*}
            \bE X^k = \frac{1}{k+1}\left( b^{k}a^0 + \cdots + b^{0}a^{k} \right).
        \end{equation*}
    \item Der Erwartungswert und die Varianz von $X$ sind gegeben durch
        \begin{align*}
            \bE X    & = \frac{1}{2} \left( b+a \right) & 
            \Var X  & = \frac{1}{12} \left( b-a \right)^{2}.  
        \end{align*}
\end{enumerate}

\solution Berechnen wir zunächst die $n$-ten Momente
\begin{align*}
    \bE X^n &= \int_{\R}^{} x^n \frac{1}{b-a} 1_{(a,b)}(x) dx \\
    &= \frac{1}{b-a} \int_{a}^{b} x^n dx \\
    &= \frac{1}{b-a} \left( \frac{x^{n+1}}{n+1} \right) |_{a}^{b} \\
    &= \frac{1}{b-a}\frac{1}{n+1} \left( b^{n+1}-a^{n+1} \right) \\
    &= \frac{a^n b^{0} +\cdots+ a^0 b^{n}}{n+1}.
\end{align*}
Wir brauchen nur noch $n=1,2$ einzusetzen.
\begin{align*}
    \bE X &= \frac{a+b}{2} & \bE X^2 &= \frac{b^2 + ab + a^2}{3} & 
    \Var X = \frac{(b-a)^2}{12}.
\end{align*}


\problem{Stetige Gleichverteilung ist nicht teilbar.} Sei $U$ stetig
gleichverteilt auf dem Intervall $\left[ 0,1 \right]$. Zeigen Sie, dass
es keine unabhängigen identisch verteilten Zufallsvariablen $X$ und $Y$
gibt, so dass $X+Y=U$ gilt.


\problem{Cauchy-Verteilung. Gegenbeispiel.}
Sei $X$ Cauchy-verteilt mit der Dichtefunktion
\begin{equation*}
    f(x) = \frac{1}{\pi (1+x^2)}.
\end{equation*}
Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item Die Funktion $f$ ist wirklich eine Dichtefunktion.
    \item Der Erwartungswert $\bfE X$ existiert nicht.
    \item Die Momente $\bfE X^k$ existieren für alle $k=1,2,\dots$ nicht.
\end{enumerate}

\solution
\begin{enumerate}
    \item $\int \frac{dx}{a^2+x^2} = \frac{1}{a} \arctan \frac{x}{a}$.
    \item $\bfE X$ ist nicht definiert im Sinne von Lebesque, weil 
        $\bfE X^{+}=+\infty$ und $\bfE X^{-}=+\infty$ gilt. 
    \item Gleiches folgt für $\bfE X^{n}$ mit einer einfachen Abschätzung.
\end{enumerate}


\problem{Cauchy-Verteilung. Ein Gegenbeispiel.}
Sei $X$ Cauchyverteilt mit der Dichte
\begin{equation*}
    p(x) = \frac{1}{\pi (1+x^2)}.
\end{equation*}
Zeigen Sie:
\begin{enumerate}
    \item $p$ ist wirklich eine Dichte.
    \item Der Erwartungswert $\E X$ existiert nicht.
    \item $\E X^n$ existiert nicht, für alle positiven $n\in \bN$. 
    \item Die momentenerzeugende Funktion von $X$ existiert für alle $t \neq 0$ nicht. 
\end{enumerate}

\problem{Cauchy-Verteilung ist hartneckig.} Seien $X_1,\dots ,X_n$
unabhängig und Cauchy-Verteilt. Zeigen Sie, dass $X_1$ die gleiche
Verteilung wie $\frac{X_1 +\dots + X_n}{n}$ hat. 




\problem{L\'evy-Verteilung. Dichtefunktion.} Die Dichtefunktion der
L\'evy-Verteilung mit den Parametern $(\mu,\sigma)\in \bR\times\bR$, $\sigma>0$ ist
\begin{equation*}
    f(x) = 
    \begin{cases}
    \sqrt{ \frac{\sigma}{2\pi}} (x-\mu)^{-\frac{3}{2}} \exp\left( -\frac{\sigma}{2(x-\mu)} \right), & x\geq 0 \\
    0, & x<0.
    \end{cases}
\end{equation*}
Sei $X\sim \bfN (0, \frac{1}{\sigma})$ normalverteilt mit $\sigma>0$.  Zeigen
Sie, dass die Zufallsgröße $\frac{1}{X^2}$ L\'evy-verteilt mit den
Parametern $(0,\sigma)$ ist.

\solution 
Für ein $y>0$ gilt
\begin{align*}
    P \left( \frac{1}{X^{2}} \leq y \right) &= P \left( 1 \leq y X^{2}   \right) \\
    &= P \left( \sqrt{\frac{1}{y}} \leq | X | \right) \\
    &= 2 \left( 1 - P \left( X < \frac{1}{\sqrt{y}} \right) \right) \\
    &= 2 \left( 1 - \Phi\left( \sqrt{ \frac{\sigma}{y}} \right) \right).
\end{align*}
Die Dichte von $\frac{1}{X^2}$ erhalten wir durchs Differenzieren:
\begin{align*}
    \frac{\partial}{\partial y} P \left( \frac{1}{X^2} \leq y \right) &=
    -2 \phi \left( \sqrt{\frac{\sigma}{y}} \right) \sqrt{\sigma} 
    \left( -\frac{1}{2} y^{-\frac{3}{2}} \right) \\
    &= \sqrt{\frac{\sigma}{2 \pi}} \exp \left( - \frac{\sigma}{2 y} \right) y^{-\frac{3}{2}}.
\end{align*}
Daher ist $\frac{1}{X^2} \sim \text{L\'evy}(\sigma)$.


\problem{Mellin-Transformation.} Sei $X$ eine positive Zufallsgröße.
Die Mellin-Transformierte $T_{X}(\theta)$ von $X$ ist definiert durch
\begin{equation*}
    T_{X}(\theta) = \bfE X^{\theta}
\end{equation*}
für alle $\theta\in\bR$ für die der obige Erwartungswert existiert. Zeigen Sie
folgende Aussagen:
\begin{enumerate}
    \item Es gilt
        \begin{equation*}
            T_{X}(\theta) = \varphi_{ \log X} (\theta/i)
        \end{equation*}
        wenn alle obigen Ausdrucke wohldefiniert sind.
    \item Sind $X$ und $Y$ positive und stochastisch unabhängige Zufallsgrößen, so gilt
        \begin{equation*}
            T_{XY}(\theta) = T_{X}(\theta) \ T_{Y}(\theta). 
        \end{equation*}
    \item Es gilt
        \begin{equation*}
            T_{b X^{a}}(\theta) = b^{\theta} T_{X} (a\theta)
        \end{equation*}
        für alle $b>0$ und alle $a$ für die $T_{X}(a\theta)$ existiert. 
\end{enumerate}


\problem{Normalverteilung. Momentenerzeugende Funktion und Momente.}
Sei $X \sim \cN \left( \mu, \sigma^{2} \right)$ mit $\mu\in\R$ und $\sigma>0$. 
Die Dichte von $X$ ist also
\begin{equation*}
    f(x) = \frac{1}{\sigma \sqrt{2\pi}}
    \exp \left( -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2\right).
\end{equation*}
Dann gilt
\begin{enumerate}
    \item Die momentenerzeugende Funktion von $X$ ist gegeben durch 
        \begin{equation*}
            \psi(u) = \exp \left( \frac{1}{2} \sigma u^2 + \mu u \right).
        \end{equation*}
    \item 
        \begin{align*}
            \E X &= \mu, & \Var X &= \sigma^2.
        \end{align*}
    \item Die Zufallsvariable $Z = \frac{X-\mu}{\sigma}$ ist standardnormalverteilt.
\end{enumerate}
\solution Wir berechnen die momentenerzeugende Funktion der
Normalverteilung indem wir das Integral $\E \exp (u X)$ auf ein Integral der
Dichte einer Normalverteilung zurückführen. 
\begin{align*}
    \psi(u) = \E \exp (u X)
    &= \int_{\R}^{} e^{ux } \frac{1}{\sigma \sqrt{2\pi}} 
    e^{ -\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2} dx \\
    &= \int_{\R}^{} \frac{1}{\sigma \sqrt{2\pi}} 
    \exp \left( ux - \frac{1}{2} \frac{(x-\mu)^2}{\sigma^2} \right) dx
\end{align*}
Nun gilt für den Ausdruck im Exponenten
\begin{align*}
    ux - \frac{1}{2} \frac{(x-\mu)^2}{\sigma^2} &= 
    -\frac{1}{2\sigma^2} \left( -2 u\sigma^2 x + x^2 - 2\mu x + \mu^2 \right) \\
    &= -\frac{1}{2\sigma^2} \left( x^2 - 2( \sigma^2 u +\mu  )x + \mu^2  \right)  \\
    &= -\frac{1}{2\sigma^2} \left( \left( x - (\sigma^2 u + \mu) \right)^2 
    - (\sigma^2 u + \mu)^2 + \mu^2  \right).
\end{align*}
Eingesetzt in die obige Formel für $\psi(u)$ ergibt das 
\begin{align*}
    \psi(u) &=  \exp \left( \frac{ (\sigma^2 u + \mu)^2 - \mu^2 }{2 \sigma^2} \right)
    \int_{\R}^{} \frac{1}{\sigma \sqrt{2\pi}} 
    \exp \left( - \frac{1}{2} \frac{(x- \sigma^2 u + \mu)^2}{\sigma^2} \right) dx \\
    &= \exp \left( \frac{1}{2} \sigma^2 u^2 + u \mu \right).
\end{align*}
Der Erwartungswert und die Varianz der Normalverteilung lassen sich nun durchs
Differenzieren berechnen. 
\begin{align*}
    \partial_u \psi(u) &= \psi(u) \left( \sigma^2 u + \mu \right) &\impl && \E X &= \mu \\
    \partial_{uu} \psi(u) &= \psi(u) \left( \sigma^2 u + \mu \right)^2 + \psi(u) \sigma^2
    & \impl && \E X^2 &= \mu^2 + \sigma^2
\end{align*}
Daraus erhalten wir 
\begin{align*}
    \Var X = \sigma^2.
\end{align*}
Wenden wir uns nun $Z = \frac{X - \mu}{\sigma}$ zu. Wir zeigen $Z \sim \cN(0,1)$ 
indem wir die momentenerzeugende Funktion von $Z$ berechnen.
\begin{align*}
    \psi_{Z}(u) &= \E \exp \left( u Z \right) \\
    &= \int_{\R} e^{u \frac{x-\mu}{\sigma}} \frac{1}{\sigma \sqrt{2\pi}}
    \exp \left( -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right) dx \\
    &= \int_{\R} \frac{1}{\sigma\sqrt{2\pi}} 
    \exp \left(  u \frac{x-\mu}{\sigma} -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right) dx \\
    &= \int_{\R} \frac{1}{\sigma\sqrt{2\pi}} 
    \exp \left( -\frac{1}{2\sigma^2} \left( 
    -2 u \sigma (x-\mu) + x^2 - 2 \mu x + \mu^2 
    \right) \right) dx \\
    &= \int_{\R} \frac{1}{\sigma\sqrt{2\pi}} 
    \exp \left( -\frac{1}{2\sigma^2} \left( 
    (x - (u\sigma+\mu))^2 - u^2 \sigma^2 \right) \right) dx \\
    &= \exp \left( \frac{u^2}{2} \right).
\end{align*}
$\psi_{Z}$ ist die momentenerzeugende Funktion der Normalverteilung mit 
Parametern $(0,1)$. Was zu zeigen war. 


\problem{Normalverteilung. Momente.} 
Sei $Z$ standardisiert normalverteilt und $n\in\bN$. Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item Für die geraden Momente von $Z$ gilt die Formel
        \begin{equation*}
            \bfE\left( Z^{2n} \right) = \frac{(2n)!}{2^n \cdot n!} =: \left( 2n-1 \right)!!.
        \end{equation*}
    \item Alle ungeraden Momente verschwinden, d.h.~es gilt  $\; \bfE \,(Z^{2n+1})=0.$
    \item Für $\sigma>0$ und $\mu\in\bR$ gilt
        \begin{equation*}
            \bfE \left( \sigma Z + \mu \right)^n = 
            \sum_{k=0}^{\lfloor\frac{n}{2}\rfloor} \binom{n}{2k} \,\sigma^{2k} \left( 2k-1 \right)!!\, \mu^{n-2k}.
        \end{equation*}
        Dies ist die Formel für die Momente $m_{n}$ einer normalverteilten
        Zufallsgröße \linebreak $X \sim \cN \left( \mu, \sigma^2 \right)$. 
\end{enumerate}

\solution
\begin{enumerate}
    \item 
        Die momentenerzeugende Funktion der Standardnormalverteilung ist 
        \begin{equation*}
            \psi(u) = \exp \left( \frac{u^2}{2} \right).
        \end{equation*}
        Diese Funktion lässt sich leicht ableiten. 
        \begin{align*}
            \psi^{(1)} &= \psi u \\
            \psi^{(2)} &= \psi^{(1)} u + \psi \\
            \psi^{(3)} &= \psi^{(2)} u + 2 \psi^{(1)} \\
            \psi^{(4)} &= \psi^{(3)} u + 3 \psi^{(2)} \\
            \psi^{(2n)} &= \psi^{(2n-1)} u + (2n-1) \psi^{(2n-2)}.
        \end{align*}
        Wenn wir $u=0$ einsetzen erhalten wir die Relation
        \begin{equation*}
            \left. \psi^{(2n)} \right|_{u=0}^{} =
            \psi_{0}^{(2n)} = (2n-1)\psi^{2n-2}_{0}
        \end{equation*}
        und daraus
        \begin{equation*}
            \psi_{0}^{(2n)} = (2n-1)(2n-3) \cdots 1 = \frac{ 2n!}{ (2n)(2n-2)\cdots 2  } = \frac{2n!}{ 2^n n!}.
        \end{equation*}
    \item Das Produkt der Dichte der Standardnormalverteilung und einer ungeraden Funktion 
        $x^{2n+1}$ ist eine ungerade Funktion. Daher muss das Integral dieser Funktion über $\bR$
        verschwinden. 
    \item Die Formel wird mit Hilfe des binomischen Lehrsatzes hergeleitet. 
\end{enumerate}


\problem{Summen von normalverteilten Zufallsvariablen. }
Sind die Zufallsvariablen $X_1,\ldots,X_n$ unabhängig und normalverteilt
mit $X_i \sim \mathcal N (\nu_i, \sigma_i^2)$, so ist die Summe
wieder normalverteilt mit 
\begin{equation*}
    \sum_{i=1}^{n} X_i \sim 
    \mathcal N \left( \sum_{i=1}^{n} \nu_i, \sum_{i=1}^{n} \sigma_i^2 \right). 
\end{equation*} %\cite{czadoschmidt} %(A 1.31)

\solution
Wir berechnen die momentenerzeugende Funktion der Zufallsvariable $\sum_{i=1}^{n} X_i$
und verwenden dabei die Unabhängigkeit von $X_1, \cdots, X_n$.
\begin{align*}
    \E \exp \left( u \sum_{i=1}^{n} X_i \right) 
    &= \prod_{i=1}^{n} \E \exp \left( u X_i \right) \\
    &= \prod_{i=1}^n \exp \left( \frac{\sigma_i^2 u^2 }{2} + \nu_i u \right) \\
    &= \exp \left( \frac{ u^2 \sum_{i=1}^{n} \sigma_i^2 }{2} + u \sum_{i=1}^{n} \nu_i \right).
\end{align*}
Die momentenerzeugende Funktion, wenn sie in einer Umgebung von $0$ existiert, 
charakterisiert eine Verteilung eindeutig. Die oben berechnete momentenerzeugende Funktion
gehört zu einer Normalverteilung mit dem Erwartungswert $\sum_{i=1}^{n} \nu_i$ und
der Varianz $\sum_{i=1}^{n} \sigma^2_i$. 


\problem{Dichte der multivariaten Normalverteilung. } Zeigen Sie, dass 
$X \sim \mathcal N_p\left( \mu, \Sigma \right) $ folgende Dichte hat, falls 
$\textrm{Rang}(\Sigma)=p$: 
\begin{equation*}
    p(x) = \frac{1}{ \textrm{det}(\Sigma)^{1/2} (2\pi)^{p/2}} \exp\left( - \frac{1}{2} (x  - \mu)^\top \Sigma^{-1} (x - \mu) \right).
\end{equation*} %\cite{czadoschmidt} %(A 1.37)


\problem{Gaussche Zufallsvariablen und Unabhängigteit. } Seien $X$ und $Y$
standardnormalverteilt und unabhängig. Dann sind die Zufallsvariablen $U =
\frac{X-Y}{\sqrt{2}}$ und $V=\frac{X+Y}{\sqrt{2}}$ ebenfalls standardnormalverteilt 
und unabhängig. 

\solution Betrachte die Transformation der Zufallsvariable $(X,Y)^\top$
mit der Matrix
\begin{equation*}
    \begin{pmatrix}
        1 & -1 \\ 1 & 1 
    \end{pmatrix}
\end{equation*}
normiert mit $\frac{1}{\sqrt{2}}$. 


\problem{Log-Normalverteilung. Momente und Dichte.}
Sei $X \sim \cN(\mu,\sigma^2)$ normalverteilt, dann ist die Zufallsvariable 
$Y = e^{X}$ log-normalverteilt, was als $Y \sim \cL\cN (\mu, \sigma^2)$ notiert 
werden kann. Beweisen Sie folgende Eigenschaften der Log-Normalverteilung.
\begin{enumerate}
    \item Die Dichte von $Y$ ist durch
        \begin{equation*}
            f(t) = \frac{1}{\sigma t\sqrt{2\pi}} 
            \exp \left( -\frac{1}{2} \left( \frac{\log t -\mu}{\sigma} \right)^2  \right)
            1_{\R_{\geq 0}}(x) 
        \end{equation*} 
        gegeben.
    \item Die Momente von $Y$ sind 
        \begin{equation*}
            \E Y^n = \exp \left( \mu n + \frac{n^2\sigma^2}{2} \right).
        \end{equation*}
    \item Die momentenerzeugende Funktion $\psi_Y$ von $Y$ existiert für alle $t\neq 0$ nicht. 
\end{enumerate}

\solution
Sei $\phi_X$ die Dichte und $\Phi_X$ die Verteilungsfunktion von $X$. Wir bemerken zunächst
\begin{equation*}
    P \left( e^{X} \leq t \right) = P \left( X \leq \log t \right) = 
    \int_{- \infty}^{\log t} \phi_X (x) dx. 
\end{equation*}
für $t>0$. Dies ist die Verteilungsfunktion von $Y$. Die Dichte von $Y$
erhalten wir durchs Differenzieren.
\begin{align*}
    \frac{\partial}{\partial t} P \left( e^X \leq t \right) &= 
    \frac{\partial}{\partial t} \int_{-\infty}^{\log t} \phi(x) dx \\
    &= \frac{\partial}{\partial t} \Phi(\log t) \\
    &= \phi(\log t) t^{-1}. 
\end{align*}
Die Momente der Log-Normalverteilung erhalten wir, indem wir die Beziehung $Y=e^{X}$ benutzen. 
\begin{align*}
    \E Y^n = \E e^{nX} &= \int_{\R}^{} e^{nx} \frac{1}{\sigma \sqrt{2\pi} } 
    \exp \left( -\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^2 \right) dx \\
    &= \int_{\R} \frac{1}{ \sigma \sqrt{2\pi}} \exp \left( 
    -\frac{1}{2\sigma^2} \left( x^2 - 2 x \mu + \mu^2 - 2n \sigma^2 x \right)
    \right) dx \\
    &= \int_{ \R}^{} \frac{1}{ \sigma \sqrt{2\pi} } 
    \exp \left( -\frac{1}{2\sigma^2} \left( x - (\mu + n \sigma^2) \right)^2
    + \mu n + \frac{n^2 \sigma^2}{ 2}
    \right) dx.
\end{align*}
Daraus ergibt sich
\begin{equation*}
    \E Y^n = \exp \left[ \mu n + \frac{n^2 \sigma^2}{2} \right]. 
\end{equation*}
Insbesondere gilt $\E Y = \exp \left[ \mu + \frac{\sigma^2}{2} \right]$.

Die momentenerzeugende Funktion kann mit der gleichen Methode untersucht werden. 
Nehmen wir zunächst an, dass $X$ standardnormalverteilt ist. Dann gilt
\begin{align*}
    \psi_Y = \E \exp \left( uY \right) &= \E \exp \left( u e^{X} \right) \\
    &= \int_{\R} e^{u e^{x}} \frac{1}{\sqrt{ 2 \pi}} \exp \left(  - \frac{x^2}{2}  \right) dx \\
    &= \int_{\R} \frac{1}{\sqrt{2 \pi}} \exp \left(u e^{x} - \frac{x^2}{2} \right)dx \\
    & \geq \int_{\R}^{} \frac{1}{\sqrt{2\pi}} 
    \exp \left( u\left(1 + x + \frac{x^2}{2} + \frac{x^3}{6}\right) - \frac{x^2}{2} \right) dx.
\end{align*}
Der Ausdruck im Exponenten divergiert für $x\to \infty$ und $u>0$. Das Integral muss 
also auch divergent sein, und die momentenerzeugende Funktion existiert nur für $u=0$. 
Die oben bewiesene Aussage gilt auch allgemein für $X \sim \cN (\mu, \sigma^2)$, denn
\begin{equation*}
    \E e^{uY} = \E \exp \left( e^{u (\sigma X + \mu)} \right) = 
    \int_{\R} \exp \left( u e^{\sigma x + \mu} \right) \frac{1}{\sqrt{2\pi}}
    \exp \left( -\frac{x^2}{2} \right) dx. 
\end{equation*}


\problem{Log-Normalverteilung. Dichtefunktion.} Ist $X\sim\bfN(\mu,
\sigma^2)$ normalverteilt, dann ist die Zufallsgröße $Y=e^{X}$
log-normalverteilt mit Parametern $\mu$ und $\sigma^{2}$. Zeigen Sie, dass die
Dichtefunktion von $Y$ durch
\begin{equation*}
    f(x) = 
    \begin{cases}
        \frac{1}{\sigma x \sqrt{2\pi}} \exp \left( -\frac{1}{2} \left( \frac{\log x -\mu}{\sigma} \right)^2  \right), & x\geq 0 \\
        0, & x<0
    \end{cases}
\end{equation*} 
gegeben ist.


\problem{Log-Normalverteilung. Momente.} Ist $X\sim\bfN(\mu, \sigma^2)$
normalverteilt, dann ist die Zufallsgröße $Y=e^{X}$ log-normalverteilt mit
Parametern $\mu$ und $\sigma^{2}$. Zeigen Sie folgende Aussagen: 
\begin{enumerate}
    \item Für die Mellin-Transformierte $T_{Y}$ von $Y$ gilt
        \begin{equation*}
            T_{Y}(k) =  \bfE \, Y^{k}, \ k=1,2,\dots.
        \end{equation*}

    \item Die Momente $m_k$ von $Y$ sind gegeben durch
        \begin{equation*}
            m_{k} = \bfE \, Y^{k} = \exp \left( \mu k + \frac{ \sigma^2 k^{2}}{2} \right).
        \end{equation*}
\end{enumerate}


%\problem{Normalverteilung. Abweichungen vom Mittelwert.}
%Wir betrachten die normalverteilte Zufallsgr\"o{\ss}e $X\sim\bfN(\mu, \sigma^2)$. 
%\begin{enumerate}
%    \item Berechnen Sie die Wahrscheinlichkeiten $P(X - \mu \geq n \sigma)$ für
%        $n\in \left\{ 1,2,6 \right\}$.
%    \item Berechnen Sie die Wahrscheinlichkeiten $P( | X-\mu| \geq n \sigma)$ für 
%        $n\in \left\{ 1,2,6 \right\}$.
%    \item \"Andern sich die Ergebnisse in (a) und (b), wenn $\ge$ durch $>$ ersetzt w\"urde? 
%\end{enumerate}

\problem{Normalverteilung. Abweichungen vom Mittelwert.}
Wir betrachten die normalverteilte Zufallsvariable $X\sim\cN(\mu, \sigma^2)$. 
\begin{enumerate}
    \item Berechnen Sie die Wahrscheinlichkeiten $P(X - \mu \geq n \sigma)$ für
        $n\in \left\{ 1,2,6 \right\}$.
    \item Berechnen Sie die Wahrscheinlichkeiten $P( | X-\mu| \geq n \sigma)$ für 
        $n\in \left\{ 1,2,6 \right\}$.
    \item \"Andern sich die Ergebnisse in (a) und (b), wenn $\ge$ durch $>$ ersetzt w\"urde? 
\end{enumerate}

\solution $P( |X-\mu| \geq n\sigma ) = 2 - 2P(Z \leq n)$.
Standarisierung und folgende Tabelle liefert die Lösung.
\begin{lstlisting}
In [1]: from scipy.stats import norm
In [5]: [ norm.cdf(i) for i in range(7) ]
Out[5]: 
[0.5,
 0.84134474606854293,
 0.97724986805182079,
 0.9986501019683699,
 0.99996832875816688,
 0.99999971334842808,
 0.9999999990134123]
\end{lstlisting}


\problem{Verteilung der Stichprobenvarianz. }
Seien $X_1, \ldots, X_n$ i.i.d., normalverteilt und $\Var\left( X_1 \right)=\sigma^2$.
Für das zweite zentrierte empirische  Moment 
$\hat \sigma^2 \left( X \right)= \frac{1}{n} \sum_{i=1}^{n} \left( X_i - \bar X \right)^2 $
gilt, dass
\begin{equation*}
    \frac{n \hat \sigma^2 \left( X \right)}{\sigma^2 } = 
        \sum_{i=1}^{n} \left( \frac{X_i - \bar X}{ \sigma} \right)^2 \sim \chi^2_{n-1}.
\end{equation*}



\problem{Symmetrische Zufallsgrößen.} Eine Zufallsgröße $X$ nennen wir
symmetrisch, wenn die Verteilungen von $X$ und $-X$ gleich sind. 
Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item Hat $X$ eine Dichtefunktion $f$, so ist $X$ symmetrisch genau
        dann, wenn $f(x)= f(-x)$ für alle $x\in\bR$ gilt. 
    \item Die charakteristische Funktion $\varphi_{X}(t)$ einer symmetrischen
        Zufallsgröße $X$ ist reellwertig. 
    \item Seien $X^{+} = \max\left\{ X,0 \right\}$ und $X^{-} = \max\left\{ -X,0 \right\}$.
        $X$ ist symmetrisch genau dann, wenn die Verteilungen von $X^{+}$ und $X^{-}$ 
        gleich sind.
\end{enumerate}

\solution
\begin{enumerate}
    \item 

    \item Das folgt mit Hilfe der Euler'schen Formel.

    \item 
\end{enumerate}


\problem{Rayleigh-Verteilung. }  Seien $X$ und $Y$ unabhängig und 
$\mathcal N (0, \sigma^2)$-verteilt. Dann ist
\begin{equation*}
    Z= \sqrt{X^2 + Y^2} 
\end{equation*}
Rayleigh-verteilt. Zeigen Sie:
\begin{enumerate}
    \item $Z$ hat die Dichte
        \begin{equation*}
            \frac{z}{\sigma^2}e^{-\frac{z^2}{2\sigma^2}}1_{\R>0}(z).
        \end{equation*}
    \item Es gilt $\E\left( Z \right) = \sigma \sqrt{\pi/2}$ und $\Var\left( Z
        \right) = \sigma^2 \left( 2 - \frac{2}{\pi} \right)$.
\end{enumerate}



\problem{Beta-verteilung.} Sei $X$ eine Beta$(2,2)$-verteilte Zufallsvariable.
\begin{enumerate}
    \item Geben Sie die Dichte und Verteilungsfunktion von $X$ explicit an. 
    \item Geben Sie die Dichte und Verteilungsfuntion der Zufallsvariable
        $Y = \frac{1}{X} - 1$ an. 
    \item Berechnen Sie $\E Y$.  
\end{enumerate}


\problem{Gamma-Verteilung. Momente.}
Gegeben sei eine Gamma-verteilte Zufallsvariable $X$ mit der Dichte
\begin{equation*}
    p_{a,\lambda}(x) = \frac{\lambda^a}{\Gamma(a)}x^{a-1}e^{-\lambda x} 1_{\R_{\geq 0}}(x)
\end{equation*}
und Parametern $a>0$ und $\lambda>0$.
Zeigen Sie für ein positives $n\in\mathbb N$
\begin{equation*}
    E X^n = \frac{a\cdot\ldots\cdot (n+a-1)}{\lambda^n}.
\end{equation*}
Dabei können Sie die Identität 
$\Gamma(a+1)=a \Gamma(a)$, $a\in\R \setminus \left\{ -1,-2,\ldots \right\}$, verwenden.
[Klausur WS11]

\solution
\begin{eqnarray*}
    E X^n &=& \int_{0}^{\infty} x^n \frac{\lambda^a}{\Gamma(a)}x^{a-1}e^{-\lambda x} dx \\
    &=& \frac{a \cdot\ldots\cdot \left( a+n-1 \right)}{\lambda^n} 
    \int_{0}^{\infty} \frac{\lambda^{a+n}}{\Gamma(n+a)} x^{a+n-1} e^{-\lambda x} dx \\
    &=& \frac{a\cdot \ldots \cdot \left( a+n-1 \right)}{\lambda^n} \\
    &=& \frac{\Gamma\left( a+n \right)}{ \lambda^n \Gamma\left( a \right)}.
\end{eqnarray*}


\problem{Momentenerzeugende Funktion einer Gamma-Verteilung. }
Sei $X \sim \textrm{Gamma}(a, \lambda)$. Zeigen Sie, dass für $s<\lambda$
\begin{equation*}
    \Psi_X(s) = E \left( e^{sX} \right) = \frac{\lambda^a}{ (\lambda-s)^a}
\end{equation*}
gilt. Bestimmen Sie damit den Erwartungswert und die Varianz von $X$. %A 1.12

\problem{Transformationen der Gamma-Verteilung. } Seien $X \sim
\textrm{Gamma}(\alpha_1, \beta)$ und $Y \sim \textrm{Gamma}(\alpha_2, \beta)$
zwei unabhängige Zufallsvariablen. Beweisen Sie:
\begin{enumerate}
    \item Ist $c>0$ so gilt
        \begin{equation*}
            c X \sim \text{Gamma}(\alpha, \frac{\beta}{c}).
        \end{equation*}
    \item \begin{equation*}
            X + Y \sim \text{Gamma}(\alpha_1 + \alpha_2, \beta).
        \end{equation*}
    \item \begin{equation*}
            \frac{X}{X+Y} \sim \text{Beta}(\alpha_1, \alpha_2).
        \end{equation*}
    \item $\frac{X}{X+Y}$ und $X+Y$ sind unabhängig. 
\end{enumerate}
Für die Beweise der letzten zwei Aussagen können Sie den Transformationssatz
verwenden. 
% \cite{czadoschmidt}

\problem{Gamma-Verteilung und Exponentialverteilung. }
Seien
\begin{equation*}
    E_1, E_2 \sim \text{Exp}(\lambda)
\end{equation*}
exponentialverteilt mit dem Parameter
$\lambda>0$ und
\begin{equation*}
    G \sim \text{Gamma}(a,\lambda)
\end{equation*}
Gamma-verteilt mit den Parametern $a>0$ und $\lambda>0$. $E_1, E_2, G$ seien
unabhängig. Zeigen Sie die folgenden Aussagen mit Hilfe der Faltungsformel.
\begin{enumerate}
    \item $E_1 + E_2$ ist Gamma-verteilt mit den Parametern $(2,\lambda)$. 
    \item $E_1 + G$ ist Gamma-verteilt mit den Parametern $(a+1, \lambda)$. 
\end{enumerate}
\solution
Sei 
\begin{equation*}
    p(x) = \lambda e^{- \lambda x} 1_{\R_{\geq 0}}(x)
\end{equation*}
die Dichte der Exponentialverteilung und 
\begin{equation*}
    g(x) = \frac{\lambda^a}{\Gamma(a)} x^{a-1} e^{-\lambda x} 1_{\R_{\geq 0}}(x)
\end{equation*}
die Dichte der Gamma-Verteilung. Zunächst stellen wir
\begin{equation*}
    \text{Exp}(\lambda) = \text{Gamma}(1, \lambda)
\end{equation*}
fest. Mit Hilfe der Faltungsformel
\begin{equation*}
    f_{E_1 + G}(z) = \int_{\R}^{} p(z-y) g(y) dy
\end{equation*}
können wir die Dichte der Zufallsvariable $E_1 + G$ berechnen.
\begin{align*}
    f_{E_1 + G} (z) &= \int_{\R}^{} p(z-y) g(z) dy \\
    &= \int_{\R}^{} \lambda e^{-\lambda (z-y)} 1_{\R_{\geq 0}} (z-y)
    \frac{\lambda^a}{\Gamma(a)} y^{a-1} e^{- \lambda y } 1_{\R_{\geq 0}}(y) dy \\
    &= \frac{\lambda^{a+1}}{\Gamma(a)} e^{-\lambda z} \int_{0}^{z} y^{a-1} dy \\
    &= \frac{\lambda^{a+1}}{\Gamma(a)} e^{-\lambda z} \frac{z^a}{a} 1_{\R_{\geq 0}} (z) \\
    &=  \frac{\lambda^{a+1}}{\Gamma(a+1)} z^{a} e^{-\lambda z} 1_{\R_{\geq 0}} (z).
\end{align*}
$f_{E_1 + G}$ ist also die Dichte der Verteilung $\text{Gamma}(a+1, \lambda)$. 


\problem{Mittelwertvergleich bei Gamma-Verteilungen. }
Seien $X_1,\ldots,X_n$ i.i.d.\ und $\textrm{Gamma}(a, \lambda_1)$-verteilt sowie
$Y_1,\ldots,Y_n$ i.i.d.\ und $\textrm{Gamma}(a,\lambda_2)$-verteilt. Man nehme an,
dass die Vektoren $(X_1,\ldots,X_n)$ und $(Y_1,\ldots,Y_n)$ unabhängig sind. 
Bestimmen Sie die Verteilung von $\bar X/\bar Y$.

\problem{$\chi^{2}$-Verteilung mit einem Freiheitsgrad.} Sei $X$ 
standarisiert normalverteilt. Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item Die Verteilungsfunktion der Zufallsgröße $X^2$ ist $2\, \Phi(\sqrt{t})-1$,
        wobei $\Phi$ die Verteilungsfunktion der standarisierten Normalverteilung 
        bezeichnet.
    \item Die Dichtefunktion von $X^2$ ist 
        \begin{align*}
            f(x) = \begin{cases}
                \frac{1}{\sqrt{2 \pi x}} e^{-\frac{x}{2}} & \text{falls x>0}, \\
                0 & \text{sonst.}
            \end{cases}
        \end{align*}
\end{enumerate}

\problem{Dichte der $\chi^2$-Verteilung. } 
Seien $X_1,\ldots,X_n$ unabhängige und standardnormalverteilte
Zufallsvariablen. Dann folgt $Y= \sum_{i=1}^{n} X_i^2$ einer
$\chi^2$-Verteilung mit $n$ Freiheitsgraden. 
\begin{enumerate}
    \item Die Verteilungsfunktion von $X_1^2$ ist $2\Phi(\sqrt{t})-1$, wobei
        $\Phi$ die Verteilungsfunktion der Standardnormalverteilung bezeichnet. 
    \item Die Dichte von $X_1^2$ ist die $\text{Gamma}(\frac{1}{2},\frac{1}{2})$-Dichte. 
    \item Die Dichte von $Y$ ist die $\text{Gamma}(\frac{n}{2},\frac{1}{2})$-Dichte, d.h.\
        \begin{equation*}
            p(x) = \frac{1}{ 2^{\frac{n}{2}} 
            \Gamma(\frac{n}{2})}e^{-\frac{x}{2}} x^{\frac{n-2}{2}} 1_{\R>0}(x).
        \end{equation*}
\end{enumerate}
\solution
Wir berechnen zunächst die Verteilungsfunktion $F$ von $X_1$. Für ein positives $t$
erhalten wir
\begin{align*}
    F(t) &= P \left( X^2 \leq t \right) \\
    &= P \left( X \in \left[ -\sqrt{t}, \sqrt{t} \right]  \right) \\
    &= \int_{-\sqrt{t}}^{\sqrt{t}} \phi(x) dx \\
    &= \Phi(\sqrt{t}) - \Phi(-\sqrt{t}) \\
    &= \Phi(\sqrt{t}) - (1 - \Phi(\sqrt{t})) = 2 \Phi(\sqrt{t}) -1. 
\end{align*}
Das Differenzieren liefert die Dichte $f$ von $X_1$, und zwar
\begin{align*}
    \frac{\partial}{\partial t} \left( 2 \Phi(\sqrt{t}) - 1 \right) 
    &= \phi(\sqrt{t}) \frac{1}{\sqrt{t}} \\
    &= \frac{1}{\sqrt{2\pi t}} e^{-\frac{1}{2}t} \, 1_{\R_{>0}}(t) \\
    &= \frac{1}{\sqrt{2}\Gamma(\frac{1}{2})} t^{-\frac{1}{2}} e^{-\frac{1}{2}t} \, 1_{\R_{>0}}(t).
\end{align*}
Dies ist die $\text{Gamma}(\frac{1}{2}, \frac{1}{2})$-Dichte. Der
Additionstheorem für Gamma-verteilte Zufallsvariablen mit gleichem
Skalenparameter $\lambda$ liefert die Dichte der $\chi^{2}$-Verteilung mit $n$
Freiheitsgraden. 



\section{Stetige Zufallsvariablen -- Textaufgaben}

\problem{Widerstände.} Ein Automat produziert Widerstände, deren Werte
erfahrungsgemäß Realisierungen einer normalverteilten Zufallsgröße mit $\mu =
1000\ \Omega$ und $\sigma = 20\ \Omega$ sind. Widerstände außerhalb der
Toleranzgrenzen von $960\ \Omega$ und $1050\ \Omega$ gelten als Ausschuss.
\begin{enumerate}
    \item Wie viele von $950$ Widerständen sind im Mittel Ausschuss?
    \item Der Anteil der Widerstände mit einem Wert größer aus $1050\ \Omega$
        soll auf durchschnittlich $0.1\%$ gesenkt werden. Wie groß muss dann
        $\mu$ sein, wenn die mittlere quadratische Abweichung $\sigma^2$
        unverändert bleibt?
\end{enumerate}

\problem{Lebensdauer der Glühlampen.} Die Lebensdauer einer Glühlampe sei
eine exponentialverteilte Zufallsvariable.  Es sei bekannt, dass im Durchschnitt
$75\%$ der Glühlampen eine Mindestbrenndauer von $500$ Stunden erreichen.
Berechnen Sie Erwartungswert und Standardabweichung der Lebensdauer. 

\solution Aus $X \sim \text{Exp}(\lambda)$ mit $P(X>500) = 0.75$
folgt $\lambda = - \frac{\log 0.75}{ 500} = 0.0005753641$.  $\bE X = \sqrt{
\Var X } = \frac{1}{\lambda} = 1738.03$


\problem{Radioaktivität.} Eine radioaktive Substanz gebe im Verlauf
von $7.5 s$ im Mittel $3.87$ $\alpha$-Teilchen ab. Bestimmen Sie die
Wahrscheinlichkeit dafür, dass
\begin{enumerate}
    \item diese Substanz während einer Sekunde mindestens ein
        $\alpha$-Teilchen emittiert,
    \item zwischen der Emission zweier Teilchen mindestens zwei
        Sekunden vergehen,
    \item die Zeit zwischen zwei Emissionen zwischen einer und
        zwei Sekunden liegt.
\end{enumerate}

\solution $X_t$ ist Anzahl der Teilchen emmitiert im Laufe von
$[0,t]$. $X_t$ kann als Poisson-Verteilung modelliert werden. $X_t \sim \text{Poiss}(\mu t)$ mit $P(X_t = k) = \frac{ (\mu t)^{k} }{k!} e^{- \mu
t}$. Es gilt $\bE X_t = \Var X_t = \mu t$. $\bE X_{7.5} = 3.87 = 7.5 \mu$
impliziert $\mu = \frac{3.87}{7.5} = 0.516$.
\begin{enumerate}
    \item Wir nutzen die Homogenität der Poisson-Verteilung und berechnen
        \begin{align*}
            P(X_1 > 0) &= 1 - P(X_1=0) \\
            &= 1 - \frac{\mu^0}{0!}e^{-\mu} \\ 
            &= 1-e^{-0.516} \approx 0.4030966.
        \end{align*}
    \item Die Zwischenankunftszeiten $T_i$ sind Exponentialverteilt mit dem Parameter $\mu$
        und es gilt
        \begin{align*}
            P(T_i \geq 2) &= \exp( -2 \,\mu ) \approx 0.3562937.
        \end{align*}
    \item Analog erhalten wir
        \begin{align*}
            P( 1 \leq T_i \leq 2) &= e^{-\mu}-e^{-2\mu} \approx 0.2406097.
        \end{align*}
\end{enumerate}


\problem{Entfernungsmessung.} Die Entfernung eines Objekts werde mit Hilfe
eines Längenmessgerätes ermittelt. Dabei sei der gemessene Wert die Realisierung
einer normalverteilten Zufallsvariable mit dem Erwartungswert $\mu$ und der
Standardabweichung von $\sigma = 40\,m$. Es sei bekannt, dass bei der Messung
kein systematischer Fehler auftritt. Berechnen Sie die Wahrscheinlichkeit
dafür, dass bei einem $2\,500\,m$ entfernten Objekt bei der Messung ein um
$60\,m$ bis $80\,m$ zu großes Messergebnis ermittelt wird. 

\solution Für $X \sim \cN(2500, 160)$ berechne 
\begin{equation*}
    P(2500 + 60 \leq X \leq 2500 + 80) = P(Z \leq 2) - P(Z \leq \frac{3}{2}).
        \approx 0.04405707.
\end{equation*}


%\problem{Ersatzteillieferung.} Eine Ersatzteillieferung enthalte eine Kiste
%Kugellager, zwei Kisten Zahnräder und drei Kisten Schrauben.  Die Masse der
%Kisten (Bruttogewicht in kg) kann durch unabhängige und normalverteilte
%Zufallsgrößen $X, Y_1, Y_2, Z_1, Z_2, Z_3$ mit
%\begin{align*}
%    X&\sim \bfN(125, 1), & Y_i&\sim \bfN(84, 4)\;\;(i=1,2), & Z_j&\sim \bfN(65, 3)\;\; (j=1,2,3)
%\end{align*}
%beschrieben werden.
%\begin{enumerate}
%    \item Berechnen Sie die Wahrscheinlichkeit dafür, dass die Masse einer
%        Ersatzteillieferung größer ist als 500 kg.
%
%    \item Wie viele solcher Ersatzteillieferungen darf man maximal auf einem
%        Lastwagen laden, wenn die Gesamtmasse aller Ersatzteillieferungen mit
%        einer Wahrscheinlichkeit von mindestens $99\%$ unter $18$ Tonnen liegen
%        soll?
%\end{enumerate}

\problem{Ersatzteillieferung.} Eine Ersatzteillieferung enthalte eine Kiste
Kugellager, zwei Kisten Zahnräder und drei Kisten Schrauben.  Die Masse der
Kisten (Bruttogewicht in kg) kann durch unabhängige und normalverteilte
Zufallsvariablen $X, Y_1, Y_2, Z_1, Z_2, Z_3$ mit
\begin{align*}
    X&\sim \cN(125, 1), & Y_i&\sim \cN(84, 4)\;\;(i=1,2), & Z_j&\sim \cN(65, 3)\;\; (j=1,2,3)
\end{align*}
beschrieben werden.
\begin{enumerate}
    \item Berechnen Sie die Wahrscheinlichkeit dafür, dass die Masse einer
        Ersatzteillieferung größer ist als 500 kg.

    \item Wie viele solcher Ersatzteillieferungen darf man maximal auf einem
        Lastwagen laden, wenn die Gesamtmasse aller Ersatzteillieferungen mit
        einer Wahrscheinlichkeit von mindestens $99\%$ unter $18$ Tonnen liegen
        soll?
\end{enumerate}

\solution
\begin{enumerate}
    \item $V := X + Y_1 + Y_2 + Z_1+Z_2+Z_3 \sim  \bfN(488, 18)$. $\frac{500 -
        488}{\sqrt{18}} \approx 2.8284271$, und $P(V>500)= 1 - P(V\leq 500) =
        0.002338867$.
    \item Größes $n\in \bN$ für das
        \begin{equation*}
            P\left( W \leq \frac{18.000 - 488 n }{\sqrt{18 n}} \right) \geq 0.99
        \end{equation*}
        gilt, ist $n=36$. 
\end{enumerate}

\problem{Grenzverteilungsätze. Cloudrechner.}
Ein bestimmter Rechnertyp wird für eine Langzeitrechnung eingesetzt. Es ist
bekannt, dass ein unter Volllast arbeitender Rechner innerhalb der Dauer der
Berechnung $T$ mit Wahrscheinlichkeit $p=0.006$ ausfällt. Für die geplante
Rechendauer werden $n=56$ Rechner gebraucht. Es wird angenommen, dass die
Rechner unabhängig von einander ausfallen und jeder Rechner höchstens ein mal
ausgetauscht werden muss. Unter Verwendung eines geeigneten  Grenzwertsatzes,
beantworten Sie folgende Fragen:
\begin{enumerate}
    \item Mit welcher Wahrscheinlichkeit fällt höchstens ein Rechner von den
        $56$ Rechnern aus?
    \item Wie viele Rechner müssen zusätzlich in der Reserve gehalten werden,
        damit die Rechnung mit $99\%$ Wahrscheinlichkeit fertiggestellt werden
        kann. 
\end{enumerate}

\solution
Dies ist eine Anwendung des Grenzwertsatzes von Poisson. Es gilt $np= 0.336 =
\lambda$ und wegen $np\leq 10$ und $1500p = 9 \leq n$ hat die Approximation der
Anzahl der Ausfälle $Y \sim \text{Bin}(n,p)$ mit Hilfe der Possionverteilung
$\text{Poiss}(\lambda)$ ausreichende Qualität.
\begin{enumerate}
    \item Höchstens ein Rechner fällt mit Wahrscheinlichkeit 
        \begin{align*}
            P(Y \leq 1) &= \pi_{\lambda}(0) + \pi_{\lambda}(1) \\
            &= e^{-\lambda}\left( 1 + \lambda \right) = 0.95473646937025258693 
        \end{align*}
        aus.

    \item Wir haben auch $P(Y\leq 2) = 0.99507551444735739085$. Es reicht also aus $2$
        Rechner in der Reserve zu halten. 
\end{enumerate}


\problem{Grenzverteilungsätze. Schaltkreise.} Ein spezieller
Schaltkreis sei mit der Wahrscheinlichkeit $p=0.7$ voll funktionsfähig.
Berechnen Sie die Wahrscheinlichkeit dafür, dass in einer 
gefertigten Serie von $1\,000$ Schaltkreisen 
\begin{enumerate}
    \item wenigstens $680$ Schaltkreise voll funktionsfähig sind,

    \item die Anzahl der voll funktionsfähigen Schaltkreise mindestens
        $675$ aber höchstens $725$ Stück beträgt.
\end{enumerate}

\solution Wir verwenden den Grenzverteilungsatz in der
Moivre-Laplace Version.  Dazu betrachten wir $X_1,\dots ,X_{1000} \sim B(p)$
und $\sum_{i} X_i \sim \text{Bin}(1000,p)$.  Dann ist die Zufallsgröße
\begin{equation*}
    Y = \frac{ \sum_{i} X_i - np}{ \sqrt{np(1-p)} } 
\end{equation*}
annähernd normalverteilt. Wir berechnen
\begin{align*}
    P( \sum_{i} X_i \geq 680 ) &= 1 - P\left( \sum_{i} X_i < 680 \right) \\
    &= 1- P\left( \frac{\sum_{i} X_i - np }{\sqrt{np(1-p)}} < \frac{680-np}{\sqrt{np(1-p)}} \right)\\
    &= 1-P( Y < - 1.380131 ) \approx 0.9162269.
\end{align*}
Analog ist 
\begin{align*}
    P( 675 \leq \sum_{i} X_i \leq 725 ) &\approx 0.9155021.
\end{align*}

\problem{Grenzverteilungsätze. Münzen.} Eine ideale Münze werde $n$-mal 
geworfen. Berechnen Sie unter Nutzung von Grenzverteilungseigenschaften die Wahrscheinlichkeit dafür, dass 
\begin{enumerate}
    \item bei $100$ Würfen die relative Häufigkeit für das Ereignis 
        ``Zahl'' um weniger als $0.1$ Einheiten von der Wahrscheinlichkeit
        $0.5$ abweicht.
    \item Wie oft müsste man die ideale Münze werfen, um mit mindestens
        $99\%$-iger Sicherheit eine Abweichung der relativen Häufigkeit
        von der Wahrscheinlichkeit zu erreichen, die kleiner ist als $0.1$
        Einheiten. 
    \item Lösen Sie die Aufgaben (a) und (b) alternativ mit Hilfe der Tschebyscheff-Ungleichung und vergleichen Sie die Ergebnisse. 
\end{enumerate}

\solution Die Anwendung des Satzes von Moivre-Laplace liefert
\begin{align*}
    P\left( | H_n(A) p | \leq \varepsilon \right) &= 
    2 \Phi\left ( \frac{\sqrt{n} \varepsilon}{ \sqrt{ p(1-p) }  } \right) -1 \approx 0.9544997.
\end{align*}
Für $n=166$ wird gerade die $99\%$ Wahrscheinlichkeit erreicht. Mit Hilfe der Tschbyschffschen
Ungleichung erhält man die Abschätzung
\begin{equation}
    P \left( | \frac{1}{n} \sum_{i}^{} X_i - 0.5 | \leq \varepsilon \right) > 1 - \frac{1}{4 n \varepsilon^2}.
\end{equation}
Für $n=100$ und $\varepsilon=0.1$ die obige Abschätzung gleich $0.8502994$. Die $99\%$-ige
Sicherheit erreichen wir erst bei $n=2500$. 

\problem{Grenzverteilungsätze. Kondensatoren.} Die Wahrscheinlichkeit dafür,
dass ein Kondensator während der Zeit $T$ ausfällt, betrage $0.5$. 
\begin{enumerate}
    \item Bestimmen Sie die Wahrscheinlichkeit dafür, dass von 100 solchen
        unabhängig voreinander arbeitenden Kondensatoren mehr als $60$ Stück in
        der Zeit $T$ ausfallen. 
    \item Wie viele Kondensatoren muss man mindestens in Reserve haben, damit
        nach der Zeit $T$ mit einer Wahrscheinlichkeit von mindestens $95\%$
        alle ausgefallenen Kondensatoren ersetzt werden können?
\end{enumerate}

\solution 
\begin{enumerate}
    \item $X\sim B(p)$, $Y=\sum_{i=1}^{100} \sim \text{Bin}(100,0.5)$, $E Y =
        np$, $\Var Y = np(1-p)= 25$. Wende den Grenzwertsatz von Moivre-Laplace
        da die Fausregel $np(1-p)>9$ erfüllt ist.
        \begin{equation*}
            P(Y>60) = P(Y < 40) \approx 
            \Phi\left( \frac{40 - np}{ \sqrt{np(1-p)}} \right) =
            \Phi(-2) = 0.02275013.
        \end{equation*}

    \item $P(Y \leq x) = 0.95$, $\Phi\left( \frac{x-50}{5} \right) \geq 0.95$
        liefert $x=59$. 
\end{enumerate}

\problem{Grenzverteilungsätze. Schaltgehäuse.} Beim Gießen spezieller
Schaltgehäuse trete ein Ausschussprozentsatz von $10\%$ auf. Für einen
reibungslosen Produktionsverlauf müssen je Gießvorgang (mindestens) $100$
brauchbare Gehäuse hergestellt werden.  Wie viel Gehäuse müssen mindestens bei
einem Vorgang gegossen werden, um mit mindestens $99\%$-iger Sicherheit diese
Forderung zu erfüllen?

\solution $Y\sim \sum_{i=1}^{n} X_i$, $X_i \sim B(p)$,
$p=1-0.1=0.9$.  $P(Y\geq 100) \geq 0.99$. $\Phi(x) \geq 0.99$, wenn $x\geq
2.326348$. 
\begin{align*}
    \frac{np - 100}{ \sqrt{np(1-p)} } &\geq 2.3263 \\
    n - 2.3263 \sqrt{p(1-p)} \sqrt{n} -100 &\geq 100 \\
    \Phi^{-1}(0.99) \sqrt{p(1-p)} &= 0.6979044
\end{align*}
Optimierung liefert $n=108$.

TODO: Check this calculation. A student reports $n=119$, which is more
plausible than $104$. 


\section{Mehrdimensionale Verteilungen.}

\problem{Korrelationskoeffizient. Eigenschaften.} Seien $X$ und $Y$
Zufallsvariablen mit existierenden und endlichen Varianzen $\sigma^2_{X}$, $\sigma^2_{Y}$ und bezeichne mit
$\rho_{X,Y}$ den Korrelationskoeffizienten von $X$ und $Y$. Zeigen Sie folgende
Aussagen: 
\begin{enumerate}
    \item Für $a>0$, $c>0$ und $b,d\in\bR$ gilt
        \begin{equation*}
            \rho_{aX + b, cY + d} = \rho_{X,Y}.
        \end{equation*}
        Geben Sie eine Interpretation dieser Aussage an. 
    \item Für $a\neq 0$ und $b\in\bR$ gilt
        \begin{equation*}
            \rho_{X, aX+b} = \frac{a}{|a|}.
        \end{equation*}

    \item Sei
        \begin{equation*}
            Z = \frac{Y}{\sigma_{Y}} - \frac{\rho_{X,Y}}{\sigma_{X}} X.
        \end{equation*}
        Dann gilt
        \begin{equation*}
            \sigma^2_{Z} = 1 - \rho^{2}_{X,Y}.
        \end{equation*}
\end{enumerate}

\solution$\operatorname{Cov}(aX+b, cY + b)= ac
\operatorname{Cov}(X,Y)$. $\Var aX = a^2 \Var X$. 

\problem{Bivariate Normalverteilung. Maximum.} Seien $(X,Y)$ bivariat normalverteilt
mit $\bfE X=\bfE Y=0$, $\bfD^{2}Y=\bfD^{2}X=1$ und $\rho_{XY}$ dem Korrellationskoeffizienten
von $X$ und $Y$
Zeigen Sie, dass
\begin{equation*}
    \bfE \left[ \max\left\{ X,Y \right\} \right] = \sqrt{\frac{1-\rho}{\pi}}
\end{equation*}
gilt.
% @biNormalMax #bivariateNormal #multivariateNormal #unsolved &shaoExercises 

\problem{Korrelation und Symmetrie.} Eine Zufallsgröße $X$ nennen wir
symmetrisch, wenn die Verteilungen von $X$ und $-X$ gleich sind. Seien $X$ eine
stetige Zufallsgröße mit endlicher Varianz und $Y = |X|$. Beweisen Sie: Ist $X$
symmetrisch, so sind $X$ und $Y$ unkorreliert aber nicht unabhängig.

\solution Ist $X$ symmetrisch, so sind $X^{+}$ und $X^{-}$
gleichverteilt und $\operatorname{Cov}(X, |X|) = \bfD^2 X^{+} - \bfD^{2} X^{-}
= 0$. Klarerweise sind $X$ und $|X|$ nicht unabhängig. 

\problem{Gleichverteilung auf dem Einheitskreis.} Sei $(X,Y)$
ein Zufallsvektor mit der (zweidimensionalen) Dichtefunktion 
\begin{equation*}
    f(x,y) = \begin{cases}
        \pi^{-1}, & \text{falls } x^{2}+y^{2} \leq 1 \\
        0,& \text{sonst.}
    \end{cases}
\end{equation*}
\begin{enumerate}
    \item Berechnen Sie die Randdichten $f_{X}(x)$ und $f_{Y}(y)$, und zeigen
        Sie, dass dies tatsächlich Dichten sind.
        
    \item Berechnen Sie die Erwartungswerte $\E X$ und $\E Y$.

    \item Zeigen Sie, dass die Zufallsvariablen $X$ und $Y$ unkorreliert sind.
    \item Zeigen Sie, dass die Zufallsvariablen $X$ und $Y$ nicht unabhängig sind. 
\end{enumerate}
% sources: Shao Exercises ch. 1, Jacod/Protter ch. 12.

\solution 
\begin{enumerate}
    \item Die Randdichte $f_{X}(x)$ ist gegeben durch
        \begin{align*}
            f_{X}(x) &= \int_{}^{} f(x,y) dy \\
            &= \int_{x^2+y^2 \leq 1}^{} \pi^{-1} dy \\
            &= \int_{- \sqrt{1-x^2}}^{\sqrt{1-x^2}} \pi^{-1} dy \\
            &= \frac{2}{\pi} \sqrt{1-x^2} 1_{[-1,1]}.
        \end{align*}
    \item Erwartungswert von $X$ ist demnach
        \begin{align*}
            \E X &= \int_{-1}^{1} x \frac{2}{\pi} \sqrt{1-x^2} dx = 0
        \end{align*}
        da wir eine ungerade Funktion bezüglich eines um $0$ symmetrischen 
        Intervalls integrieren.
    \item 
        \begin{align*}
            \operatorname{Cov}(X,Y) &= \E \left( X-\E X \right)\left( Y- \E Y \right) \\
            &= \E XY \\
            &= \int_{-1}^{1} \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} xy \pi^{-1} dy dx \\
            &= \int_{-1}^{1} x \pi^{-1} \left( \frac{y^2}{2} \right)^{\sqrt{1-x^{2}}}_{-\sqrt{1-x^{2}}} dx = 0.
        \end{align*}
        Die Zufallsvariablen $X$ und $Y$ sind daher unkorreliert.
    \item 
        Aus $X \upmodels Y$ folgt $f(x,y) = h(x) g(y)$ für alle $(x,y) \in
        \bR^{2}$. Für $x=1$ ergibt sich $g(y) \sim 1_{y=0}$. $g$ ist aber keine
        Dichte. Widerspruch.
\end{enumerate}

\problem{Gleichverteilung auf der Ellipse.} Sei $(X,Y)$
ein Zufallsvektor mit der (zweidimensionalen) Dichtefunktion 
\begin{equation*}
    f(x,y) = \begin{cases}
        \left( \pi a b \right)^{-1}, & \text{falls } \left( \frac{x}{a} \right)^{2} + \left( \frac{y}{b} \right)^{2} \leq 1 \\
        0,& \text{sonst,}
    \end{cases}
\end{equation*}
und reellwertigen Parametern $a,b > 0$. 
\begin{enumerate}
    \item Berechnen Sie die Randdichten $f_{X}$ und $f_{Y}$.
    \item Berechnen Sie die Erwartungswerte $\E X$ und $\E Y$.
    \item Zeigen Sie, dass die Zufallsvariablen $X$ und $Y$ unkorreliert sind.
\end{enumerate}

\solution 
Die Lösung ist eine einfache Verallgemeinerung der Berechnung für die Gleichverteilung
auf dem Einheitskreis.
\begin{enumerate}
    \item Die Randdichte $f_{X}(x)$ ist gegeben durch
        \begin{align*}
            f_{X}(x) &= \int f(x,y) dy \\
            &= \frac{2}{\pi a} \sqrt{1- \left( \frac{x}{a} \right)^2} \ 1_{[-a,a]}(x).
        \end{align*}
    \item Erwartungswert von $X$ ist demnach
        \begin{align*}
            \E X &= \int_{-a}^{a} x f_{X}(x) dx = 0
        \end{align*}
        da wir eine ungerade Funktion bezüglich eines um $0$ symmetrischen 
        Intervalls integrieren.
    \item 
        \begin{align*}
            \operatorname{Cov}(X,Y) &= \E \left( X-\E X \right)\left( Y- \E Y \right) \\
            &= \E XY \\
            &= \int_{-b}^{b} \int_{-b\sqrt{1- x^2/a^2}}^{b\sqrt{1-x^2/a^2}} \frac{xy}{\pi ab} dy dx \\
            &= \int_{-b}^{b} \frac{x}{ \pi ab} \left( \frac{y^2}{2} \right)^{b\sqrt{1-x^{2}/a^2}}_{-b\sqrt{1-x^{2}/a^2}} dx = 0.
        \end{align*}
        Die Zufallsvariablen $X$ und $Y$ sind daher unkorreliert.
\end{enumerate}


\problem{Bedingte Dichten. Ein Rechenbeispiel.} Sei $(X,Y)$ ein 
Zufallsvektor mit der Dichtefunktion
\begin{equation*}
    f(x,y) = \begin{cases}
        \frac{3}{5}y(x+y), & \text{falls } 0<x<2 \text{ und } 0<y<1 \\
        0,& \text{sonst.}
    \end{cases}
\end{equation*}
\begin{enumerate}
    \item Bestimmen Sie die Randdichten von $X$ und $Y$. 
    \item Bestimmen Sie die bedingte Dichte $f_{Y|X=x}(y)$, für $x\in (0,2)$. 
    \item Zeigen Sie, dass $P( Y \leq \frac{1}{2} \,|\, X=1) = \frac{1}{5}$ gilt.
%    \item Zeigen Sie, dass $\mathbf{Cov} (X+Y, X-Y) = \frac{73}{100}$ gilt.
\end{enumerate}

\solution 
\begin{enumerate}
    \item Wir berechnen die Randdichten $f_{X}$ und $f_{Y}$:
        \begin{align*}
            f_{X}(x) &= \int_{}^{} f(x,y) dy \\
            &= \int_{0}^{1} \frac{3}{5} y(x+y) dy \\
            &= \frac{3}{5} x \int_{0}^{1} y dy + \frac{3}{5} \int_{0}^{1} y^{2} dy \\
            &= \left( \frac{3}{10}x + \frac{1}{5} \right) 1_{[0,2]}(x). \\
            f_{Y}(y) &= \int f(x,y) dx \\
            &= \int_{0}^{2} \frac{3}{5} y(x+y) dx \\
            &= \left( \frac{6}{5} y \left( y+1 \right) \right) 1_{[0,1]}(y).
        \end{align*}

    \item Bedingte Dichte $f_{Y | X=x}(y)$ is gegeben durch
        \begin{align*}
            f_{Y \,|\, X=x}(y) &= \frac{f(x,y)}{f_{X}(x)} \\
            &= \frac{y(x+y)}{ x/2 + 1/3} 1_{[0,2]}(y).
        \end{align*}

    \item 
        \begin{align*}
            P\left( Y \leq \frac{1}{2} \,|\, X=1 \right) &=
            \int^{1/2}_{0} f_{Y \,|\, X=1} (y) dy \\
            &= \frac{6}{5} \int^{1/2}_{0} y(1+y) dy = \frac{1}{5}. 
        \end{align*}
\end{enumerate}

\problem{Bivariate Rademacherverteilung.} Sei $\bfX = (X_1, X_2)$ ein Zufallsvektor mit
$X_k \in \left\{ -1, 1 \right\}$, $k=1,2$ und \[P(X_1=i, X_2=j)=p_{ij}\] wobei
\begin{equation*}
    \begin{pmatrix}
        p_{-1,-1} & p_{-1, 1} \\
        p_{1,-1} & p_{1,1}
    \end{pmatrix}
    = \frac{1}{8}
    \begin{pmatrix}
        1 & 3 \\
        3 & 1
    \end{pmatrix}.
\end{equation*}
\begin{enumerate}
    \item Zeigen Sie dass $X_1$ und $X_2$ Rademacherverteilt sind, d.h.\ 
        \[P(X_k = -1) = P(X_k = 1) = \frac{1}{2}. \]
        Berechnen Sie dabei die Wahrscheinlichkeitsfunktionen von $X_1$ und $X_2$. 

    \item Berechnen Sie den Korrelationskoeffizienten von $X_1$ und $X_2$.
    \item Berechnen Sie die Kovarianzmatrix $\mathbf \Sigma$ des Zufallsvektors $\bfX$.
    \item Sind die Zufallsvariablen $X_1$ und $X_2$ stochastisch unabhängig?
\end{enumerate}

\solution 
Wegen $\E X_1 = \E X_2 = 0$ gilt
\begin{align*}
    \operatorname{Cov}(X_1, X_2) &= \E XY
\end{align*}
und
\begin{align*}
    P(X_{1} X_{2} = 1) &= \frac{1}{4} & 
    P(X_{1} X_{2} = -1) &= \frac{6}{8} &
    \E X_{1}X_{2} &= -\frac{1}{2}. 
\end{align*}
Es gilt daher 
\begin{equation*}
   \rho_{X_1,X_2} = -\frac{1}{2}.
\end{equation*}
Die Kovarianzmatrix $\mathbf \Sigma$ von $\bfX$ ist demnach
\begin{equation*}
    \mathbf \Sigma = \begin{pmatrix}
        1 & -1/2 \\
        -\frac{1}{2} & 1
    \end{pmatrix}.
\end{equation*}



\section{Quantile}

\problem{Exponentialverteilung. Quantile.} Seien 
$X\sim \mathbf{Ex}(\lambda)$ exponentialverteilt mit dem Paramter $\lambda>0$
und $\gamma\in (0,1)$ gegeben. 
\begin{enumerate}
    \item Berechnen Sie das $\gamma$-Quantil zur $X$. 
    \item Berechnen Sie den Median von $X$. 
\end{enumerate}

\solution Dichte und Verteilungsfunktionen der Exponentialverteilung
sind
\begin{align*}
    f(x) &= \lambda e^{-\lambda x} 1_{x \geq 0} & F(x) &= \left( 1-e^{-\lambda x} \right)1_{x \geq 0}. 
\end{align*}
Daraus ergibt sich für $\gamma\in (0,1)$ 
\begin{equation*}
    q_{\gamma} = F^{-1}(\gamma) = -\frac{1}{\lambda} \log \left( 1-\gamma \right). 
\end{equation*}
Median ist damit gleich $-\frac{1}{\lambda} \log \frac{1}{2}$. 


\problem{Logistische Verteilung. Quantile.} Eine Zufallsgröße heißt
logistisch verteilt mit den Parametern $\alpha\in\bR$ und $\beta>0$, wenn ihre
Dichtefunktion die Gestalt
\begin{equation*}
    f(x) = \frac{e^{-\frac{x-\alpha}{\beta}}}
    {\beta\left( 1+e^{-\frac{x-\alpha}{\beta}} \right)^{2}}
\end{equation*}
hat.
\begin{enumerate}
    \item Zeigen Sie, dass die Verteilungsfunktion von $X$ durch
        \begin{equation*}
            F(x) = \frac{1}{1+e^{-\frac{x-\alpha}{\beta}}}
        \end{equation*}
        gegeben ist.

    \item Berechnen Sie das $\gamma$-Quantil zur $X$ für ein $\gamma\in (0,1)$. 
    \item Berechnen Sie den Median von $X$. 
\end{enumerate}

\solution Die Quantilfunktion ist gegeben durch
\begin{equation*}
    Q(\gamma) = \alpha - \beta \log\left( \frac{1}{\gamma} - 1 \right). 
\end{equation*}
Der Median ist $Q(0.5) = \alpha$.

\section{Erwartungstreue}

\problem{Schätzung der Varianz bei bekanntem Erwartungswert.} 
Sei $X_{1},\dots ,X_n$, $n>1$ eine $n$-elementige Stichprobe mit $\E X_i =
\mu$ und $\Var X_i = \sigma^2>0$ gegeben. Zeigen Sie, dass die Schätzfunktion 
\begin{equation*}
    V^{2}\left( X_1,\dots ,  X_n \right) = \frac{1}{n} \sum_{i=1}^{n} \left( X_i - \mu \right)^2
\end{equation*} 
erwartungstreu für $\sigma^2$ ist. 


\problem{Diskrete Gleichverteilung. Erwartungstreuer Schätzer.} Seien
$X_1,\dots ,X_k$ i.i.d.\ und diskret gleichverteilt auf $\left\{ 1,\dots ,N
\right\}$ mit $N>1$. Zeigen Sie, dass der Schätzer
\begin{equation*}
    T\left( X_1,\dots ,X_k \right) = \frac{k+1}{k} X_{(k)}-1
\end{equation*}
erwartungstreu für $N$ ist. $X_{(k)}$ bezeichnet dabei das Maximum von $\left\{
X_1,\dots ,X_k \right\}$. 

\problem{Log-Normalverteilung. Erwartungstreuer Schätzer.} Seien $X_1,\dots
,X_n$ i.i.d.\ und log-normalverteilt mit der Dichtefunktion
\begin{equation*}
    f(x) = \begin{cases}
        \frac{1}{ x\sqrt{2\pi} } \exp \left( - \frac{ (\log x - \mu)^2 }{ 2 } \right), & x\geq 0 \\
        0, & x<0
    \end{cases}
\end{equation*}
und dem Parameter $\mu\in\bR$. Zeigen Sie, dass der Schätzer 
\begin{equation*}
    T\left( X_1,\dots ,X_n \right) = \frac{1}{n} \sum_{i=1}^{n} \log X_i
\end{equation*}
erwartungstreu für $\mu$ ist. 

\solution Aus der Definition der Log-Normalverteilung oder durch
direktes Berechnen, erhalten wir
\begin{equation*}
    \bfE X_1 = \mu.
\end{equation*}
Damit ist $T$ erwartungstreu.


\section{Einfache Schätzer}

\problem{Einfache Schätzer. Telefonzentrale.} In einer Telefonzentrale wurden für $11$
aufeinanderfolgende Anrufe die Zeitabstände zwischen zwei Anrufen ermittelt. Es
ergaben sich die folgenden Werte:
\begin{lstlisting}
38.1  74.1  17.7  17.7  1.2  13.2  22.8  35.7  108.6  20.7
\end{lstlisting}
Geben Sie auf der Grundlage dieser Stichprobenwerte je eine Schätzung für den
mittleren Zeitabstand und die mittlere quadratische Abweichung der Zeitabstände
vom Erwartungswert an. 

Diskutieren Sie die Eigenschaften dieser Schätzfunktionen.

\solution Erwartungstreue Schätzfunktionen für den Erwartungswert und
die Varianz sind
\begin{align*}
    \bar X_n &= \frac{1}{n} \sum_{i=1}^{n} X_n & S_n^{2} &= \frac{1}{n-1} \sum_{i=1}^{n} \left( X_i - \bar X_n \right)^{2}. 
\end{align*}
Für diese Stichprobe gilt
\begin{align*}
    \sum_{i=1}^{10} X_i &= 349.8 \\
    \bar X_n &= 34.98 \\
    S^2_n &= 1058.384.
\end{align*}




\section{Konfidenzintervalle}

\problem{Kondensatoren. Konfidenzintervall.} Aus einer Lieferung von
Kondensatoren wurden $n$ Stück zufällig ausgewählt und folgende Kapazitäten (in
$\mu$F) festgestellt:
\begin{lstlisting}
    281  221  220  221  219  221  220  218  222  219
\end{lstlisting}
\begin{enumerate}
    \item Berechnen Sie je eine Schätzung für den Erwartungswert und für die
        Varianz der Kapazität. 
    \item Berechnen Sie auf der Grundlage dieser Stichprobenergebnisse je ein
        zweiseitiges Konfidenzintervall für den Mittelwert und die Varianz der
        Kapazität zum Konfidenzniveau $1-\alpha = 0.95$. Von welchen
        Verteilungsvoraussetzungen gehen Sie dabei aus?
\end{enumerate}

\solution
\begin{enumerate}
    \item $\bar X_n = 226.2 = \bar \mu$, $S^2_n = 372.1778 = \bar \sigma^2$.
    \item Wir nehmen an, dass $X \sim \bfN(\bar \mu, \bar \sigma^2)$ gilt. 
        Für die Berechnung des Konfidenzintervall für $\mu$ bei unbekannter Varianz
        benutzen wir die Beziehung
        \begin{equation*}
            \frac{\bar X_n - \mu}{ S_n} \sim t_{n-1}. 
        \end{equation*}
        Daraus folgt
        \begin{align*}
            P\left( - t_{n-1, 1-\frac{\alpha}{2}} \leq 
            \frac{\bar X_n - \mu}{ S_n}\sqrt{n}
            \leq t_{n-1, 1-\frac{\alpha}{2}} \right) = 1-\alpha. 
        \end{align*}
        Die $t_{n}$-Verteilung ist symmetrisch -- das folgt aus der Konstruktion: Für 
        $X\sim \bfN(0,1)$ und $Y\sim \chi^{2}_{n}$ ist 
        \begin{equation*}
            \frac{X}{\sqrt{Y/n}} \sim t_{n}. 
        \end{equation*}
        Insgesamt bekommen wir
        \begin{align*}
            P\left( \bar X_n - t_{n-1, 1-\frac{\alpha}{2}} \frac{S_{n}}{\sqrt{n}} 
            \leq \mu \leq \bar X_{n} + t_{n-1, 1-\frac{\alpha}{2}} \frac{S_{n}}{\sqrt{n}}
            \right) = 1-\alpha. 
        \end{align*}
        Konkret in unserem Fall heißt das
        \begin{equation*}
            t_{9, 1-\frac{0.05}{2}} = 2.262
        \end{equation*}
        \begin{equation*}
            P\left( \mu \in \left[ 212 , 240 \right] \right) = 0.95.
        \end{equation*}

        $0.95$-Konfidenzintervall für die Varianz ist 
        \begin{equation*}
            \left[ 176.08, 1057.25 \right].
        \end{equation*}
\end{enumerate}

\problem{Krankenversicherung. Konfidenzintervall.} Einer Krankenversicherung
seien für eine spezielle Gruppe von $15$ Versicherungsnehmern mittlere Kosten
in Höhe von $\bar x_n = 5\,500$ EUR bei einer Stichprobenstandardabweichung
$s_n = 1\, 500$ EUR entstanden.
\begin{enumerate}
    \item Geben Sie ein zweiseitiges Konfidenzintervall für die mittleren
        Kosten zum Konfidenzniveau $1-\alpha=0.95$ an. 
    \item Die Versicherung habe ihren Tarif unter der Annahme kalkuliert, dass
        die mittleren Kosten $5\, 000$ EUR betragen. Diskutieren Sie anhand des
        erhaltenen Konfidenzintervalls, ob eine Neukalkulation des Tarifs
        angezeigt erscheint. 
\end{enumerate}

\solution
\begin{enumerate}
    \item Konfidenzintervall für $\mu$ bei unbekannter Varianz ist 
        \begin{align*}
            P\left( \bar X_n - t_{n-1, 1-\frac{\alpha}{2}} \frac{S_{n}}{\sqrt{n}} 
            \leq \mu \leq \bar X_{n} + t_{n-1, 1-\frac{\alpha}{2}} \frac{S_{n}}{\sqrt{n}}
            \right) = 1-\alpha. 
        \end{align*}
        Konkret heißt das in diesem Fall
        \begin{align*}
            P\left( \mu \in\left[ 5500-830.6723,5000+830.6723 \right]\right) &= 0.95 \\
            P\left( \mu \in\left[ 4669.328, 6330.672 \right]\right) &= 0.95
        \end{align*}

    \item $5000$ liegt im Konfidenzintervall, es gibt also keinen Grund unruhig
        zu werden.
\end{enumerate}

\problem{Qualitätssicherung. Konfidenzintervall.} Zur Schätzung des Ausschussanteils eines
umfangreichen Lieferpostens werde diesem eine Stichprobe von $200$ Teilen entnommen. 
Dabei wurden $190$ einwandfreie Teile festgestellt.
\begin{enumerate}
    \item Geben Sie eine Schätzung für den Ausschussanteil an.
    \item Wie groß kann maximal die Varianz der Schätzung sein? 

    \item Berechnen Sie ein Konfidenzintervall für den Ausschussanteil zum
        Konfidenzniveau $0.95$ und interpretieren Sie das Ergebnis.
\end{enumerate}

\solution 
\begin{enumerate}
    \item Es gilt $\sum_{i=1}^{200} X_i \sim \bfB(200, p)$. Die Schätzung für
        den Anteil ist die Schätzung für $p$. 
        \begin{equation*}
            \hat p = \frac{1}{n} \sum_{i=1}^{n} X_i
        \end{equation*}
        ist ein Erwartungstreuer Schätzer für $p$. Für die obige Stichprobe
        gilt $\hat p = \frac{1}{20}$.
    \item Varianz von $\hat p$ ist 
        \begin{equation*}
            \var \hat p = \frac{1}{n} p(1-p)
        \end{equation*}
        Diese wird maximal für $p=\frac{1}{2}$ und beträgt in diesem Fall
        $V^{2}_{\star} = 1/800$. Maximale Standardabweichung ist demnach
        $V_{\star} = \frac{1}{20\sqrt{2}}$.
    \item Nach dem Satz von Moivre/Laplace gilt 
        \begin{equation*}
            \bar X_{n} \approx \bfN \left( p, \frac{p(1-p)}{n} \right).
        \end{equation*}
        Die Annahme $\bar X_{n} \approx \bfN\left( p, V_{\star} \right)$ führt zur 
        Vergrößerung des Konfidenzintervalls. Wir erhalten so
        \begin{align*}
            P\left( z_{\frac{\alpha}{2}} \leq \frac{\bar X - p }{\sqrt{V_{\star}}} \leq z_{1- \frac{\alpha}{2}} \right) &=1-\alpha \\
            P\left( \bar X_{n} -  \sqrt{V_{\star}} z_{1-\frac{\alpha}{2}} \leq p
            \leq \bar X_{n} + \sqrt{V_{\star}} z_{1-\frac{\alpha}{2}}\right) &= 1-\alpha \\
            P\left(  -0.01929519 \leq p \leq 0.1192952  \right) &= 0.95.
        \end{align*}
        Nachdem $\hat p \geq 0$ gelten muss erhalten wir
        \begin{equation*}
            P\left(  0 \leq p \leq 0.1192952  \right) = 0.95.
        \end{equation*}
        Negatives Intervall kommt durch die normale Approximation zustande. 

        Alternative Lösung. Es kann auch mit Hilfe der Tabelle vorgegangen werden. 
        Die Grenzen des Konfidenzintervalls sind 
        \begin{align*}
            U &= \bar X_n - z_{1 - \frac{\alpha}{2}} \sqrt{ \frac{\bar X_n (1-\bar X_n)}{n}} \\
            O &= \bar X_n + z_{1 - \frac{\alpha}{2}} \sqrt{ \frac{\bar X_n (1-\bar X_n)}{n}}
        \end{align*}
        Mit $\alpha=0.05$, $z_{0.975}=1.96$, 
        \begin{equation*}
            \sqrt{ \frac{\bar X_{n}(1-\bar X_n)}{n}} = 0.154
        \end{equation*}
        ergibt sich
        \begin{equation*}
            \left[ 0.0198, 0.0802 \right]
        \end{equation*}
        als Konfidenzintervall für den Ausschussanteilt $p$.
\end{enumerate}


\problem{Justierung. Konfidenzintervall.} Bei einer Arbeitsstudie wurden folgende
Werte für die Zeitdauer der Justierung eines Gerätes gemessen:
\begin{lstlisting}
    Minuten:       3  4  5  6  8  9
    Haeufigkeiten:  2  4  7 12  4  1
\end{lstlisting}
\begin{enumerate}
    \item Geben Sie erwartungstreue Schätzungen für den Erwartungswert und die
        Varianz der Zeitdauer an.

    \item Bestimmen Sie ein zweiseitiges Konfidenzintervall zum Konfidenzniveau
        $1-\alpha=0.9$ für den Erwartungswert der Zeitdauer. 
    \item Ermitteln Sie eine obere Grenze für die Standardabweichung der Zeitdauer,
        so dass mit $95\%$-iger Sicherheit der wahre Wert von $\sigma$ überdeckt
        wird.
\end{enumerate}
Geben Sie dabei alle notwendigen Voraussetzungen an.

\solution 
\begin{enumerate}
    \item Erwartungstreue Schätzungen:
        \begin{lstlisting}
            Y <- c(3,4,5,6,8,9) 
            H <- c(2,4,7,12,4,1)
            mu <- sum(Y*H)/sum(H)         
            mu
            # 5.666667
            s <- sqrt( sum(H*(Y - mu)^2)/(sum(H)-1 ) )
            s
            # 1.470007
        \end{lstlisting}
        Es ist also $\bar X_n= 5.666667$ und $S_n = 1.470007$.
    \item Unter der Annahme, dass die Justierungen normalverteilt sind erhalten wir ein
        $(1-\alpha)$-Konfidenzintervall für $\mu$ bei unbekannter Varianz:
        \begin{align*}
            P\left( U \leq \mu \leq O  \right) &= 1-\alpha \\
            U(X_1,\dots ,X_n) &= \bar X_n - t_{n-1, 1-\alpha/2} \frac{S_n}{\sqrt{n}} \\
            O(X_1,\dots ,X_n) &= \bar X_n + t_{n-1, 1-\alpha/2} \frac{S_n}{\sqrt{n}}.
        \end{align*}
        In diesem Fall heißt das:
        \begin{lstlisting}
            mu - qt(1-alpha/2, 30-1)*s/sqrt(30)
            # 5.210646
            mu + qt(1-alpha/2, 30-1)*s/sqrt(30)
            #  6.122687
        \end{lstlisting}
    \item Ein Konfidenzintervall für $\sigma^{2}$ bei unbekanntem $\mu$
        erhalten wir aus
        \begin{align*}
            P\left( 
            \chi^{2}_{n-1, \alpha/2} \leq 
            \frac{(n-1)S^2_n }{\sigma^2} \leq \chi^2_{n-1, 1-\alpha}\right) &=
            1-\alpha \\
            P\left( U \leq \sigma^2 \leq O \right) &= 1-\alpha \\
            U &= \frac{(n-1) S^{2}_{n}}{ \chi^{2}_{n-1, 1-\alpha/2} } \\
            O &= \frac{(n-1) S^{2}_{n}}{ \chi^{2}_{n-1, \alpha/2} } \\
        \end{align*}
        Einseitiges Konfidenzintervall für $\sigma$ ist dann
        \begin{equation*}
            \left[ 0, \frac{\sqrt{n-1} S_n}{ \sqrt{\chi^{2}_{n-1, \alpha}} } \right].
        \end{equation*}
        Für $\alpha=0.05$ heißt das konkret 
        \begin{equation*}
            \left[ 0, 1.881174 \right].
        \end{equation*}
\end{enumerate}


\section{Tests}


\problem{Waschmittel. Test.} Bei einem Verbrauchertest für Waschmittel werde auch 
die Abfüllmenge kontrolliert. Dabei ergaben sich bei $10$ zufällig ausgewählten $5$ kg
Packungen einer bestimmten Sorte folgende Abfüllungen (in kg):
\begin{lstlisting}
    4.6  4.95  4.8  4.9  4.75  5.05  4.9  5.1  4.8  4.95
\end{lstlisting}
\begin{enumerate}
    \item Bestimmen Sie Schätzungen für den Erwartungswert und die Varianz 
        der Abfüllmenge.
    \item Ist auf der Basis dieser Beobachtung die Auffassung vertretbar, 
        dass die Packungen im Mittel weniger Waschmittel als angegeben enthalten?
\end{enumerate}

\solution
\begin{enumerate}
    \item Erwartungstreue Schätzungen:
        \begin{lstlisting}
            X <- c( 4.6,  4.95 , 4.8,  4.9 , 4.75,  5.05 , 4.9 , 5.1 , 4.8 , 4.95  )
            mean(X)
            # 4.88
            var(X)
            # 0.02177778
        \end{lstlisting}
        Es ist also $\bar X_n= 4.88$ und $S^2_n = 0.02177778$.
    \item Wir testen
        \begin{align*}
            H_0: \mu \geq \mu_0 \text{ gegen } H_1: \mu < \mu_0
        \end{align*}
        mit $\mu_0=5$ und verwenden die Teststatistik
        \begin{equation*}
            T = \frac{\bar X_n - \mu_0}{ S_n } \sqrt{n} \sim t_{n-1}.
        \end{equation*}
        Als kritischer Bereich ergibt sich $K_{\alpha} = ( -\infty, -t_{n-1,
        1-\alpha})$.  Für $\alpha = 0.05$ und $n=9$ erhalten wir zum Beispiel
        $K_{0.05} = (-\infty, -1.833113)$. Ebenso $K_{0.01} = (-\infty,
        -2.821438 )$.  Der Wert der Teststatistik $T$ ist $-2.439471$. Die
        Nullhypothese wird also mit Irtumswahrscheinlichkeit $\alpha=0.05$
        verworfen, kann aber bei $\alpha=0.01$ nicht verworfen werden. 
\end{enumerate}

\problem{Messgerät. Test.} Die Genauigkeit eines Messgerätes verschlechtere
sich im Laufe der Zeit infolge von Abnutzungserscheinungen. Das Messgerät gelte
als brauchbar, solange die Varianz $\sigma^2$ des Messfehlers einen Wert von
$10^{-4}$ nicht überschreitet.

Darf man bei einer zugelassenen Irrtumswahrscheinlichkeit $\alpha=0.05$ auf die
Brauchbarkeit des Messgerätes schließen, wenn aus $25$ Kontrollwerten für die
Varianz eine Schätzung $s^2 = 0.65 \cdot{} 10^{-4}$ ermittelt werde? 

\solution Wir wollen auf Brauchbarkeit des Geräts schließen, daher 
testen wir
\begin{align*}
    H_0 : \sigma^2 \geq \sigma^2_0 \quad\text{gegen}\quad H_1 : \sigma^2 < \sigma^2_0 
\end{align*}
mit $\sigma^2_0=10^{-4}$. Der Kritische Bereich ist 
\begin{align*}
    K = \left\{ T = \frac{(n-1) S^2_n}{\sigma^2_0} < \chi^2_{n-1, \alpha}   \right\}.
\end{align*}
Der Wert der Teststatistik ist $T = 15.6$. Dabei ist $\chi^2_{n-1, \alpha} =
13.84843$. Die Nullhypothese kann also nich verworfen werden. 

\problem{Entfernungsmessung. Konfidenzintervall und Test.} Beim Messen der
Entfernung zwischen zwei Punkten sei der Messwert die Realisierung einer
$\cN(\mu,\sigma^2)$-verteilten Zufallsgröße. Aus $20$ Messungen einer
bestimmten Entfernung ergaben sich die folgenden Schätzwerte:
\begin{align*}
    \bar x &= 327.0\,\text{m}, & s^2 &= 17.3\, \text{m}^2.
\end{align*}
\begin{enumerate}
    \item Bestimmen Sie ein $99\%$-iges Konfidenzintervall für den Erwartungswert.
    \item Geben Sie ein einseitiges nach oben beschränktes Konfidenzintervall für 
        $\sigma$ zum Konfidenzniveau $1-\alpha = 0.95$ an. 
    \item Lässt sich bei einer zugelassenen Irrtumswahrscheinlichkeit von $5\%$ die Aussage
        $\mu>320\,\text{m}$ rechtfertigen?
    \item Darf man bei einer zugelassenen Irrtumswahrscheinlichkeit $\alpha=0.05$ auf die
        Brauchbarkeit des Messgerätes schließen, wenn als Kriterium für die Tauglichkeit
        die Forderung $\sigma<5.0\,\text{m}$ dient?
\end{enumerate}

\solution
\begin{enumerate}
    \item Ein zweiseitiges $99\%$-Konfidenzintervall erhalten wir aus
        \begin{align*}
            U &= \bar X_n - t_{n-1, 1-\alpha/2}\frac{S_n}{\sqrt{n}} = 324.3987 \\
            O &= \bar X_n + t_{n-1, 1-\alpha/2}\frac{S_n}{\sqrt{n}} = 329.6013
        \end{align*}

    \item Einseitiges $95\%$-Konfidenzinvervall für $\sigma^2$ ist 
        \begin{equation*}
            \left[ 0, (n-1)\frac{S^2_n}{ \chi^{2}_{n-1, 1 - \frac{\alpha}{2}} } \right] = \left[ 0, 32.48983 \right].
        \end{equation*}

    \item $\mu>320$ rechtfertigen heißt $\mu\leq 320$ verwerfen, also
        \begin{align*}
            H_0: \mu \leq 320 \quad\text{gegen}\quad H_1 : \mu > 320
        \end{align*}
        Teststatistik:
        \begin{equation*}
            T = \frac{\bar X_n - \mu_0}{S_n} \sqrt{n} = 8.414823
        \end{equation*}
        Kritischer Bereich:
        \begin{equation*}
            K = \left\{ T > t_{n-1, 1-\alpha} \right\} = [ 1.729133, \infty].
        \end{equation*}

    \item Wir wollen auf die Brauchbarkeit des Geräts schliessen und test
        \begin{align*}
            H_0 : \sigma^2 \geq \sigma^{2}_{0} \quad\text{gegen}\quad H_1 : \sigma^2 < \sigma^2_0
        \end{align*}
        mit $\sigma^{2}_{0} = 25$. Teststatistik:
        \begin{equation*}
            T = (n-1)\frac{S^{2}_{n}}{\sigma^{2}_{0}} = 13.148
        \end{equation*}
        Kritischer Bereich:
        \begin{equation*}
            K = \left\{ T < \chi^{2}_{n-1, \alpha} = 10.11701 \right\}
        \end{equation*}
        Die Nullhypothese wird also nicht verworfen. 
\end{enumerate}



\problem{Erneuerbare Energien. Test.} 
In einer einfachen Analyse des Strommarktes wird davon ausgegangen, dass die
täglich von Solar- und Windgeneratoren zwischen 12 und 13 Uhr erzeugte Energie
in den Monaten Juni, Juli und August (92 Tage) die Realisierung einer
normalverteilten Zufallsvariable $X\sim\cN(\mu, \sigma^2)$ ist. Es wird
angenommen, dass die Standardabweichung $\sigma$ bekannt ist und beträgt $\sigma=5.5 GW$.
Ein Analyst hat für die $92$ Beobachtungen $x_1,\dots ,x_{92}$ folgendes
ausgerechnet:
\begin{align*}
    \sum_{i=1}^{92} x_i &= 1802.142.
\end{align*}
\begin{enumerate}
    \item Geben Sie eine erwartungstreue Schätzung für den Erwartungswert $\mu$
        der erzeugten Energie.
    \item Lässt sich anhand dieser Beobachtung die Hypothese $\mu\leq 19$
        mit Irrtumswahrscheinlichkeit $\alpha=0.1$ ablehnen?
\end{enumerate}

\solution
\begin{enumerate}
    \item $\hat{\mu} = 19.5885$.
    \item Wir testen $H_0: \mu\leq 19$ gegen $H_1: \mu> 19$. Die Teststatistik ist
        $T = \frac{\bar X_n - \mu_{0}}{\sigma}\sqrt{n} = 1.026308$. Der
        kritische Bereich ist $K = \left[ z_{1-\alpha},\infty\right) = \left[
            1.281552, \infty \right)$. $H_0$ kann nicht abgelehnt werden.
\end{enumerate}


\problem{Test. Trendvergleich bei Funds.} 
Zur Analyse der Performance zweier Funds werden die Kursveränderungen über den
Zeitraum von einem Jahr (250 Businesstage) analysiert. Aus den Beobachtungen
$x^{(1)}_{1},\dots ,x^{(1)}_{250}$ und $x^{(2)}_{1},\dots ,x^{(2)}_{250}$
wurden folgende Kennzahlen ermittelt:
\begin{align*}
    \sum_{i=1}^{250} x^{(1)}_i &= 29.96, & \sum_{i=1}^{250} x^{(2)}_{i} &= 32.97
\end{align*}
Es wird angenommen, dass die Kursveränderungen normalverteilt sind, d.h.\
$X^{(1)}\sim \cN(\mu_1, \sigma_1^{2})$ und $X^{(2)}\sim\cN(\mu_2,
\sigma_2^{2})$, und dass die Varianzen $\sigma_{(1)}^{2}=0.5$ und

$\sigma^{2}_{(2)}=0.55$ betragen.
\begin{enumerate}
    \item Geben Sie je eine erwartungstreue Schätzung für die Erwartungswerte
        $\mu_1$ und $\mu_2$. 
    \item Lässt sich anhand dieser Beobachtung die Hypothese $\mu_1 = \mu_2$
        mit der Irrtumswahrscheinlichkeit $\alpha=0.05$ ablehnen?
\end{enumerate}

\solution
\begin{enumerate}
    \item $\hat{\mu_1} = 0.1198547$, $\hat{\mu_2} = 0.1318836$.
    \item Teststatistik:
        \begin{equation*}
            T = \frac{ \bar X^{(1)} - \bar X^{(2)}}{ \sqrt{ \sigma^{2}_{(1)}/n + \sigma^{(2)}_{(2)}/n }} = -0.1856103.
        \end{equation*}
        Kritischer Bereich ist $| T | \geq z_{1-\alpha/2} = 1.96$. Die
        Nullhypothese kann nicht abgelehnt werden.
\end{enumerate}
R source code:
\begin{lstlisting}
> mu1=0.1; mu2=0.13; 
> s21=0.5; s22=0.55;
> X1 = rnorm(250, mu1, sqrt(s21))
> X2 = rnorm(250, mu2, sqrt(s22))
> sum(X1); sum(X2);
[1] 29.96367
[1] 32.9709
> mean(X1); mean(X2)
[1] 0.1198547
[1] 0.1318836
> n=250
> T = (mean(X1)-mean(X2)  )/sqrt( s21/n + s22/n )
> T
[1] -0.1856103
> qnorm( 0.975)
[1] 1.959964
\end{lstlisting}

\problem{Konfidenzintervall und Test. Normalverteilung.} Beim Messen der
Entfernung zwischen zwei Punkten sei der Messwert die Realisierung einer
$\bfN(\mu, \sigma^2)$-verteilten Zufallsgröße. Aus $20$ Messungen einer
bestimmten Entfernung ergaben sich die folgenden Schätzwerte:
\begin{align*}
    \bar x &= 327.3\,m, & s^2 &= 17.3\,m^2.
\end{align*}
\begin{enumerate}
    \item Bestimmen Sie ein $99\%$-iges Konfidenzintervall für den
        Erwartungswert.

    \item Geben Sie ein einseitiges nach oben beschränktes Konfidenzintervall
        für $\sigma$ zum Konfidenzniveau $1-\alpha= 0.95$. 

    \item Lässt sich bei einer zugelassenen Irrtumswahrscheinlichkeit von $5\%$
        die Aussage $\mu > 320\,m$ rechtfertigen?

    \item Darf man bei einer zugelassenen Irrtumswahrscheinlichkeit
        $\alpha=0.05$ auf die Brauchbarkeit des Messgerätes schließen, wenn als
        Kriterium für die Tauglichkeit die Forderung $\sigma < 5.0\,m $ dient?
\end{enumerate}


\problem{Analyse des Zuckerpreises.}
Es seien folgende $41$ monatliche Beobachtungen $x_{1},\dots ,x_{41}$ des internationalen
Zuckerpreises aus dem Zeitraum von Januar 2011 bis Mai 2014 (quotiert in US-Cent
pro Pfund) gegeben:
\lstset{basicstyle=\ttfamily\footnotesize,basewidth={.5em,0.4em}}
\begin{lstlisting}
29.74 29.31 25.90 23.90 21.84 24.92 29.47 28.87 26.64 26.30 24.52 23.42 24.02
23.42 23.79 22.48 20.27 20.10 22.76 20.56 20.21 20.39 19.31 19.20 18.85 18.21
18.34 17.66 17.43 16.96 17.10 17.24 17.62 18.81 17.75 16.54 15.71 16.89 17.87
17.61 17.50
\end{lstlisting}
Wir nehmen an, dass diese Beobachtungen Realisierungen einer normalverteilten Zufallsgröße
$X \sim \NormDist(\mu, \sigma^{2})$ sind. Darüberhinaus gilt
\begin{align*}
    \sum_{i=1}^{41} x_{i} &= 869.40 & \sum_{i=1}^{41} x_{i}^{2} &= 19090.47.
\end{align*}
\begin{enumerate}
    \item Geben Sie je eine erwartungstreue Schätzfunktion für den Mittelwert
        $\mu$ und für die Varianz $\sigma^2$ der Zufallsgr\"o{\ss}e an.
        Erkl\"aren Sie in diesem Zusammenhang den Begriff der Erwartungstreue
        einer Sch\"atzfunktion.

    \item Bestimmen Sie ein Konfidenzintervall $[0,\bar \sigma]$ für die
        Standardabweichung $\sigma$ zum Konfidenzniveau $1-\alpha = 0.95$.

    \item Lässt sich anhand dieser Beobachtungen mit einer zugelassenen
        Irrtumswahrscheinlichkeit von 10\% die Behauptung widerlegen, dass
        der durchschnittliche Zuckerpreis in diesem Zeitraum unter $18$ Cent
        pro Pfund lag?
\end{enumerate}

\solution
\begin{enumerate}
    \item \begin{align*}
            \bar X_n &= \frac{1}{n} \sum_{i=1}^{n} X_i = 21.2048.\\
            S^{2}_n &= \frac{1}{n-1} \sum_{i=1}^{n} \left( X_i - \bar X_n \right)^{2} \\
            &= \frac{1}{n-1} \left\{ \sum_{i=1}^{n} X_i^{2} - \frac{1}{n} \left( \sum_{i=1}^{n} X_i \right)^2 \right\} \\
            &= 16.3737. 
        \end{align*}
    \item \begin{align*}
            \bar \sigma = \sqrt{ (n-1) \frac{S^{2}_{n}}{ \chi^{2}_{n-1,\alpha}} } = \sqrt{ 24.7066 } = 4.9705.
        \end{align*}

    \item Wir testen $H_{0}: \mu \leq 18$ gegen $H_1: \mu > 18$. Die Testfunktion und der 
        kritische Bereich sind
        \begin{align*}
            T &= \frac{\bar X_{n} - \mu_{0}}{ S_{n}} \sqrt{n} = 5.0714, \\
            K &= \left( t_{n-1,1-\alpha}, +\infty \right) = ( 1.303, +\infty).
        \end{align*}
        Nachdem die Bedingung $T\in K$ erfüllt ist, kann $H_0$ abgelehnt werden. 
\end{enumerate}


\problem{Analyse des Kaffeebohnenpreises.} In einer einfachen Analyse wird der
Preis des Kaffees der Sorte "`Robusta"', quotiert in US-Cent pro Pfund durch International
Coffee Organization, auf einen Aufwärtstrend untersucht. 
Für die $51$ Monatsbeobachtungen $r_{1},\dots ,r_{51}$ der Preisänderungen
%\lstset{basicstyle=\ttfamily\footnotesize,basewidth={.5em,0.4em}}
%\begin{lstlisting}
%-0.97 3.82 -0.14 6.28 7.44 -0.90 -1.95 3.46 7.37 0.38 7.71 8.59 7.84 -1.17 5.00
%-4.15 -5.57 2.42 -5.06 -6.60 0.84 5.89 -4.66 1.85 2.34 -1.88 4.29 -2.66 0.02
%-0.36 -2.15 -0.98 -6.95 -0.64 3.49 3.91 2.97 -5.09 -1.82 -8.71 5.36 -1.68 -7.24
%-3.47 -4.34 9.83 -2.57 8.21 10.76 -1.23 -2.33
%\end{lstlisting}    
berechnet ein Analyst die Kenngrößen
\begin{align*}
    \sum_{i=1}^{51} r_{i} &= 34,83, & \sum_{i=1}^{51} r^{2}_{i} &= 1225,01.
\end{align*}
Es wird angenommen, dass die Kursveränderungen die Realisierungen einer normalverteilten
Zufallsgröße $X\sim \bfN(\mu, \sigma^2)$ sind.
\begin{enumerate}
    \item Geben Sie je eine erwartungstreue Schätzfunktion für den Mittelwert
        $\mu$ und für die Varianz $\sigma^2$ der Kursveränderungen an.
        Berechnen Sie dafür aus der Stichprobe Schätzwerte im Sinne von
        Punktschätzungen.
    \item Der Analyst behauptet $\mu>0$ und gibt eine Kaufempfehlung aus. Lässt
        sich diese Aussage anhand der Beobachtungen mit einer zugelassenen
        Irrtumswahrscheinlichkeit von $5\,\%$ rechtfertigen, d.\,h.~muss man die
        Hypothese $H_0: \, \mu \le 0$ mit dieser Irrtums\-wahrscheinlichkeit
        ablehnen?
\end{enumerate}

\solution
\begin{enumerate}
    \item $\bar X = 0.6829578849$, $\bar{S^{2}} = 24.02442243$.
    \item Wir testen $H_0: \mu \leq 0$ vs.\ $H_1 : \mu > 0$. Da $\sigma^{2}$ unbekannt
        ist, ist die Teststatistik gegeben durch $T = \frac{\bar X_n}{ \sqrt{\bar S_n^2} } \sqrt{n}$
        und der kritische Bereich des Tests lautet $\left\{ T \geq t_{n-1, 1-\alpha} \right\}$
        mit $\alpha=0.05$. 
        In unserem Fall gilt $T = 6.966865879$ und $t_{n-1,1-\alpha}=
        t_{50,0.95} = 1.676$. Somit kann $H_0$ abgelehnt werden.
\end{enumerate}


\problem{Aktie im Bachelier-Modell.} Die Kursveränderungen einer Aktie eines
börsennotierten Unternehmens werden über den Zeitraum von 3 Wochen (15
Börsentage) analysiert. Die beobachteten Werte $x_{1},\dots ,x_{15}$ waren:
\begin{lstlisting}
-1.10 -1.52  0.25  1.65 -0.90  2.70  1.34 -1.26  0.22
-0.49  1.59 -0.47  3.62  1.89  0.36
\end{lstlisting}    
Ein Analyst berechnete daraus
\begin{align*}
    \sum_{i=1}^{15} x_i &= 7.86, & \sum_{i=1}^{15} x^{2}_i &= 37.62.
\end{align*}
Es wird angenommen, dass die Kursveränderungen die Realisierungen einer normalverteilten
Zufallsgröße $X\sim \bfN(\mu, \sigma^2)$ sind.
\begin{enumerate}
    \item Geben Sie je eine erwartungstreue Schätzfunktion für den Mittelwert $\mu$ und für
    die Varianz $\sigma^2$ der Kursveränderungen an. Berechnen Sie daf\"ur aus der Stichprobe Schätzwerte im
    Sinne von Punktsch\"atzungen.
    \item Bestimmen Sie ein einseitiges Konfidenzintervall $[0,\overline \sigma]$ f\"ur die Standardabweichung $\sigma$
    zum Konfidenzniveau $1-\alpha=0.9$, d.h.~bestimmen Sie die Zahl $\overline \sigma>0$.
    \item Ein Analyst behauptet $\mu>0$ und gibt eine Kaufempfehlung aus. Lässt
    sich diese Aussage anhand der Beobachtungen mit einer zugelassenen
    Irrtumswahrscheinlichkeit von $5\%$ rechtfertigen, d.h.~muss man die Hypothese $H_0: \, \mu \le 0$ 
    mit dieser Irrtums\-wahrscheinlichkeit   ablehnen ?
\end{enumerate}

\solution
\begin{enumerate}
    \item Erwartungstreue Schätzer sind
        \begin{align*}
            \bar X_n &= \frac{1}{n} \sum_{i=1}^{n} X_i, & S^2_n = \frac{1}{n-1} \sum_{i=1}^{n} \left( X_i - \bar X_n \right)^2.
        \end{align*}
        Daraus ergibt sich
        \begin{align*}
            \bar X_n &= 0.524  \\
            S^2_n &= \frac{1}{n-1} \left( \sum_{i=1}^{n} X_i^2 - n \left( \bar X_n \right)^2 \right) \\
            &= \frac{1}{n-1} \left[  \sum_{i=1}^{n} X_i^{2 } - \frac{1}{n} \left( \sum_{i=1}^{n} X_i \right)^2 \right] = 2.392954. 
        \end{align*}
    \item
        \begin{align*}
            \bar \sigma &= (n-1) \frac{S^2_n}{\chi^{2}_{n-1, \alpha}} = 4.300817. 
        \end{align*}
    \item Wir testen $H_0: \mu \leq \mu_0$ gegen $H_1: \mu>\mu_0$ mit
        $\mu_0=0$. Der Wert der Teststatistik ist
        \begin{align*}
            T = \frac{\bar X_n - \mu_0}{S_n} \sqrt{n} = 1.311927. 
        \end{align*}
        Der kritische Bereich ist $\left\{ T > t_{n-1, 1-\alpha}\right\}=
        \left\{ T > 1.76131 \right\}$. Die Nullhypothese wird nicht verworfen.
\end{enumerate}

% vim: set spelllang=de: set spell
