

\section{Elementary probability}


\paragraph{Ereignisse. Mengenoperationen.}
Gegeben seien jeweils die Ergebnismenge $\Omega$ sowie zwei Teilmengen
$A$ und $B$:
\begin{enumerate}
\item $\Omega=\{1,2,...,20\}$, $A=\{4,5,6,7,9,11\}$, $B=\{3,5,9,20\}$
\item $\Omega=[-1,3]$, $A=[0,1)$, $B=(\frac{1}{2},2]$
\item $\Omega=\mathbb R$, $A=\{x\in \mathbb R: |x-1|<3\}$, $B=[0,\infty)$.
\end{enumerate}
Bilden Sie die Mengen $\overline{A}$, $\overline{B}$, $A\cap B$, $A\cup B$,
$\overline{ A\cup B}$, $\overline{A}\cap\overline{B}$, $B\cap\overline{A}$,
$(\overline{A}\cup\overline{B})\cap\overline{B}$,
$B\cup(\overline{B\cap\overline{A}})$.


\paragraph{Ereignisse. Kraftwerk.}
Die Arbeit eines Kraftwerkes wird durch drei unabhängig voneinander arbeitende
Kontrollsysteme überwacht, die jedoch auch einer gewissen Störanfälligkeit
unterliegen. Es bezeichne $S_i$ das Ereignis, dass das $i$-te System
störungsfrei arbeitet $(i=1,2,3)$.
\begin{enumerate}
    \item Finden Sie einen geeigneten Wahrscheinlichkeitsraum, der diese
        Zufallssituation beschreibt. Geben Sie die Ergebnismenge $\Omega$
        explicit an. Ist die Ergebnismenge eindeutig bestimmt?
    \item Drücken Sie folgende Ereignisse mit Hilfe der Ereignisse $S_1$, $S_2$
        und $S_3$ aus:
        \begin{itemize}
            \item[$A$:] Alle drei Systeme arbeiten störungsfrei.
            \item[$B$:] Kein System arbeitet störungsfrei.
            \item[$C$:] Mindestens ein System arbeitet störungsfrei.
            \item[$D$:] Genau ein System arbeitet störungsfrei.
            \item[$E$:] Höchstens zwei Systeme sind gestört.
        \end{itemize}\label{ereignisse-kraftwerk-1}
    \item Welche der unter \ref{ereignisse-kraftwerk-1} genannten Ereignisse
        sind Elementarereignisse?
    \item Aus wie vielen Elementen bestehen die Ereignisse $D$ und $C$?
\end{enumerate}

\paragraph*{Lösung.} Die Ergebnismenge $\Omega$ kann als $\Omega= \left\{ (ijk)
: i,j,k\in \left\{ 0,1 \right\} \right\}$ gewählt werden. Diese Wahl ist aber nicht 
eindeutig. Die Ereignisse $A$ und $B$ sind in diesem Fall Elementarereignisse. 


\paragraph{Ereignisse. Aktienmarkt.}
Beim Monatsvergleich zweier Technologieaktien wird für jede Aktie
festgestellt, ob es zu einem Gewinn von mindestens 3\% kam, ob sich ein Verlust
um mehr als 3\% ergab oder ob sich die jeweilige Aktie innerhalb der 6\%-Spanne
bewegte.
\begin{enumerate}
    \item Finden Sie einen geeigneten Wahrscheinlichkeitsraum, der diese
        Zufallssituation beschreibt. Geben Sie die Ergebnismenge $\Omega$
        explicit an.
    \item Stellen Sie folgende Ereignisse mit Hilfe der Elementarereignisse
        dar:
        \begin{itemize}
            \item[$A$:] Beide Aktien erzielten einen Kursgewinn von mindestens $3\%$.
            \item[$B$:] Die Kurse der beiden Aktien lagen innerhalb der
                $6\%$-Spanne. %Keine der beiden Aktien veränderte sich signifikant.
            \item[$C$:] Der Kurs von höchstens einer der beiden Aktien
                verschlechterte sich um mehr als $3\%$. 
            \item[$D$:] Der Kurs von mindestens einer der beiden Aktien
                verschlechterte sich um mehr als $3\%$. 
        \end{itemize}
    \item Welche Bedeutung haben die Ereignisse\\
        $E_1=A\cup C$, $E_2=A\cup D$,
        $E_3=A\cap C$, $E_4=A\cap\overline{C}$, $E_5= \overline{A\cap D}$ ?
\end{enumerate}



\paragraph{Ereignisse. Fertigungsstraße.}
Eine Fertigungsstraße bestehe  aus einer Maschine vom Typ I, vier Maschinen vom
Typ II und zwei Maschinen vom Typ III. Wir bezeichnen mit $A$, $B_k$ bzw.\
$C_j$ ($k=1,2,3,4$; $j=1,2$) die Ereignisse, dass die Maschine vom Typ I
bzw.~die $k$-te Maschine vom Typ~II bzw.~die $j$-te Maschine vom Typ III intakt
ist. Die Fertigungsstraße sei arbeitsfähig, wenn mindestens eine Maschine von
jedem Maschinentyp intakt ist. Dieses Ereignis werde mit $D$ bezeichnet.

Beschreiben Sie die Ereignisse $D$ und $\overline{D}$ mit Hilfe der Ereignisse
$A$, $B_k$, $C_j$.


\paragraph{Ereignisse. Kosten.}
Drei Betriebsteile werden auf Einhaltung eines bestimmten
Kostenfaktors überprüft. Das Ereignis $A$ liege vor, wenn mindestens ein
Betriebsteil nicht den geforderten Kostenfaktor einhält, das Ereignis $B$ liege
vor, wenn alle drei Betriebsteile den geforderten Kostenfaktor einhalten.

Was bedeuten dann die Ereignisse $A\cup B$ und $A\cap B$ ?


\paragraph{Ereignisse. Würfel und Münze.}
Ein Experiment bestehe aus dem Werfen eines fairen Würfels und einer fairen Münze.
\begin{enumerate}
    \item Geben Sie eine geeignete Ergebnismenge $\Omega$ an.
    \item Zeigt die Münze Wappen, so wird die doppelte Augenzahl des Würfels
        notiert, bei Zahl nur die einfache. Wie groß ist die
        Wahrscheinlichkeit, dass eine gerade Zahl notiert wird?
\end{enumerate}


\paragraph{Ereignisse. Zerlegung des Würfels.}
Ein Würfel, dessen Seitenflächen gleichartig gefärbt sind, werde in 1000
kleine Würfel einheitlicher Größe zerlegt.

Wie groß ist die Wahrscheinlichkeit dafür, dass ein zufällig ausgewählter
Würfel auf mindestens einer Seite gefärbt ist?


\paragraph{Ereignisse. Elementare Wahrscheinlichkeiten.}
Für die Ereignisse $A$ und $B$ seien folgende Wahrscheinlichkeiten bekannt:
$P(A)=0.25$, $P(B)=0.45$, $P(A\cup B)=0.5$. Berechnen Sie die
Wahrscheinlichkeiten:
\begin{enumerate}
    \item $P(A\cap\overline{B})$,
    \item $P(\overline{A}\cap\overline{B})$ und
    \item $P\left((A\cap\overline{B})\cup(\overline{A}\cap B)\right)$.
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item \begin{align*}
            P(A \cup B) &= P( (A \cap \bar B) \cup B ) = 
            P( A \cap \bar B  ) + P(B) \\
            P(A \cup \bar B) &= P(A \cup B) - P(B) = 0.05.
        \end{align*}
    \item \begin{align*}
            P(\bar A \cap \bar B) &= P(\overline{A \cup B}) = 0.5.
        \end{align*}
    \item Die symmetrische Differenz $A \Delta B = (A \setminus B) \cup (B \setminus A)$
        ist eine disjunkte Vereinigung, und daher
        \begin{align*}
            P(A \Delta B) &= P(A \cap \bar B) + P(\bar A \cap B).
        \end{align*}
        Die vorherigen Überlegungen liefern
        \begin{align*}
            P( A \cap \bar B) &= P(A \setminus B) = P(A \cup B) - P(B) = 0.05 \\
            P( \bar A \cap B) &= P(B \setminus A) = P(A \cup B) - P(A) = 0.25.
        \end{align*}
        Insgesamt gilt also $P(A \Delta B) = 0.3$.
\end{enumerate}


\section{Wahrscheinlichkeitsmaße.}

\paragraph{Wahrscheinlichkeitsmaße. Monotonie.} Seien ein Wahrscheinlichkeitsraum \linebreak
$(\Omega, \fA, P)$ und die Ereignisse $A,B \in \fA$ gegeben. Zeigen Sie
\begin{equation*}
A \subseteq B \quad  \impl \quad P(A) \leq P(B).
\end{equation*}

\paragraph*{Lösung.} $P(B) = P(A \cup (B \setminus A))= P(A) + P(B\setminus A) \geq P(A)$. 


\paragraph{Wahrscheinlichkeitsmaße. Subaditivität.} Seien ein
Wahrscheinlichkeitsraum $(\Omega, \fA, P)$ und die Ereignisse $A_1, A_2, \dots
\in \fA$ gegeben. 
\begin{enumerate}
    \item Zeigen Sie, dass für alle  $n\in \bN$ 
        \begin{equation*}
            P \left(  \bigcup \limits_{i=1}^n A_i \right) \leq \sum_{i=1}^{n} P\left( A_i \right)
        \end{equation*}
        gilt. 

    \item Zeigen Sie, dass sogar gilt 
        \begin{equation*}
            P \left(  \bigcup \limits_{i=1}^\infty A_i \right) \leq \sum_{i=1}^{\infty} P\left( A_i \right).
        \end{equation*}
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item Betrachte folgende Darstellung der Vereinigung als Summe disjunkter
        Elemente von $\fA$.
        \begin{align*}
            \bigcup_{i=1}^n A_i &= A_1 \cup (\bar A_1 \cap A_2) \cup 
            (\bar A_1 \cap \bar A_2 \cap A_3) \cup \dots \cup
            (\bar A_1 \cap \bar A_2 \cap \dots \cap \bar A_{n-1} \cap A_n).
        \end{align*}

    \item Genau das gleiche Argument funktioniert für $n=\infty$. Das ist 
        die Konsequenz der $\sigma$-Additivität der Wahrscheinlichkeitsfunktion $P$. 
\end{enumerate}


\paragraph{Wahrscheinlichkeitsmaße. Zerlegung der Vereinigung.} Seien ein
Wahrscheinlichkeitsraum $\left( \Omega, \fA, P \right)$ und die Ereignisse
$A,B,C\in \fA$ sowie $A_1, A_2, \dots, A_n \in \fA$ gegeben.
Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item \begin{equation*}
            P \left( A \cup B \cup C \right) = 
            P(A)+P(B)+P(C) - P(A\cap B) - P(A\cap C) - P(B\cap C) + P(A\cap B \cap C). 
        \end{equation*}

    \item
        \begin{align*}
            P\left( \bigcup \limits_{i=1}^{n} A_i \right) =&
            \sum_{i=1}^{n} P(A_i) - \sum_{i<j} P(A_i \cap A_j) + \\
            & \sum_{i<j<k} P(A_i \cap A_j \cap A_k) - \dots + 
            (-1)^{n+1} P\left( A_1 \cap \dots \cap A_n \right). 
        \end{align*}
\end{enumerate}

\paragraph*{Lösung.} 
\begin{enumerate}
    \item Hier genügt eine Überlegung mit Hilfe der Venn-Diagrame. 

    \item Die Menge $A_1 \cup \dots \cup A_n$ ist Vereinigung der Mengen der
        Form $A_{i_1} \cap \dots \cap A_{i_k} \cap \bar A_{i_{k+1}} \cap
        \dots \cap \bar A_{i_{n}}$, $k\geq 1$, wobei $i_1,\dots, i_n$ eine
        Umnumerierung von $1, \dots, n$ ist. Nun kommt solche Menge in der
        ersten Summe als Teilmenge von $A_i$ genau $\binom{k}{1}
        $ vor. In der zweiten Summe wird die Menge $\binom{k}{2}$ mal abgezogen. 
        Nachdem $\sum_{i=0}^{k} (-1)^{i} \binom{k}{i} = 0$, erhalten wir 
        $\sum_{i=1}^{k} (-1)^{i+1} \binom{k}{i}=1$.
\end{enumerate}


\paragraph{Arithmetik der Wahrscheinlichkeitsmaße.} Seien ein
Wahrscheinlichkeitsraum $\left( \Omega, \fA, P \right)$ und die Ereignisse $A,
B \in \fA$ gegeben. 
\begin{enumerate}
    \item Angenommen $P(\bar A)=\frac{1}{3}$, $P(A\cap B)=\frac{1}{4}$ und
        $P(A \cup B)=\frac{2}{3}$. Berechnen Sie $P(\bar B)$, $P(A \cap \bar B)$
        und $P( B \cap \bar A )$.
        
    \item Angenommen $P(A \cup B) = \frac{1}{2}$, $P(A \cap B) = \frac{1}{4}$
        und $P(A \cap \bar B) = P(B \cap \bar A)$. Berechnen Sie $P(A)$ und
        $P(A \cap \bar B)$. 
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item $B \subset A$.

    \item $P(A) = \frac{3}{8}$ und $P(A \setminus B) = \frac{1}{8}$.
\end{enumerate}



\section{Kombinatorik.}


\paragraph{Kombinatorik. Passwörter.} Wir betrachten die Menge
$\cA = \left\{ a, b, \dots, z, 0, \dots, 9 \right\}$ als gegebenen Zeichensatz.
\begin{enumerate}
    \item Wie viele voneinander verschiedene Passwörter der Länge 8 können
        aus $\cA$ gebildet werden?
    \item Betrachten wir nun die Passwörter aus dem obigen Zeichensatz, die an der
        letzen Stelle eine Ziffer aufweisen und sonst aus lauter Buchstaben bestehen.
        Wie viele solche Passwörter gibt es?
    \item Vergleichen Sie die Größenordnungen der Mächtigkeiten der obigen
        Passwortmengen. 
\end{enumerate}

\paragraph*{Lösung.} 
\begin{enumerate}
    \item Die Menge $\cA$ enthält $36$ Elemente. Nachdem in einem Passwort die
        Reihenfolge der Zeichen wichtig ist und die Zeichen mehrmals vorkommen
        können, ist die Anzahl der Passwörter gegeben durch die Anzahl der
        Variationen mit Zurücklegen, also durch
        \begin{equation}
            36^{8} = 2.821.109.907.456 \approx 10^{12.45}.
        \end{equation}
    \item Es gibt $26^7$ Passwörter mit Länge $7$, die nur aus Buchstaben bestehen. 
        Wenn wir an jedem solchen Passwort eine Ziffer am Ende hinzufügen, 
        erhalten wir $10\cdot 26^7$ Möglichkeiten. 
        \begin{equation}
            10\cdot 26^7 = 80.318.101.760 \approx 10^{10.9}.
        \end{equation}
    \item Die Mächtigkeiten der obigen Mengen sind also fast zwei Größenordnungen 
        auseinander.
\end{enumerate}



\paragraph{Kombinatorik. PINs und runs.} In einer Folge $\left( a_1, a_2, a_3,
\dots \right)$ wird eine Teilfolge $(a, a, \dots, a)$, die aus $n$-facher
Wiederholung eines Elements $a$ gebildet wird, als ein $n$-run bezeichnet.
Zum Beispiel hat die Folge $(0, 0, 1, 1, 1, 0)$ acht runs, nämlich einen $2$-run
$(0,0)$ und einen $3$-run $(1,1,1)$, sowie sechs $1$-runs.

Betrachten wir nun die Menge der 4-stelligen PINs, die aus den Ziffern $\left\{
0,1, \dots, 9 \right\}$ gebildet werden können. 
\begin{enumerate}
    \item Wie viele solche PINs gibt es?
    \item Wie viele PINs gibt es, die keine $2$-runs, $3$-runs, sowie keine
        $4$-runs enthalten?
    \item Vergleichen Sie die Größenordnungen der Mächtigkeiten der obigen
        PIN-Mengen. 
\end{enumerate}
\textbf{Zusatz:} Führen Sie die obige Berechnung für die $5$-stelligen PINs durch.

\paragraph*{Lösung.}
\begin{enumerate}
    \item Es gibt $10^4$ solche PINs.
    \item Um die Anzahl der PINs ohne runs zu erhalten, zählen wir alle
        möglichen PINs mit runs auf. Erste Spalte gibt die Form des PINs,
        zweite die Anzahl solcher PINs und dritte die Anzahl der symmetrischen
        Fälle.
\begin{lstlisting}
----    10          1
---*    10 9        2
--==    10 9        1
--**    10 9 8      2
*--*    10 9 9      1
\end{lstlisting}
        Das sind insgesamt $2530$ Fälle. Es gibt also $7470$ PINs ohne runs.
    \item Wenn wir PINs mit runs ausschliessen, reduziert sich unser pool
        möglicher PINs um ein Viertel. 
\end{enumerate}


\paragraph{Kombinatorik. Zwei Würfel.}
Wie groß ist die Wahrscheinlichkeit dafür, beim Werfen von zwei Würfeln eine
Augensumme zu erzielen, die größer oder gleich 10 ist?

\paragraph*{Lösung} Wir lösen die Aufgabe einmal unter der Annahme, dass die Würfel
unterschieden werden können und einmal ohne diese Annahme. 
\begin{enumerate}
    \item Wenn wir zwei wohlunterscheidbare Würfel werfen, ist die
        $36$-elementige Ergebnismenge gegeben durch $\Omega = \left\{ (i,j) :
        i,j \in \left\{ 1, \dots, 6 \right\} \right\}$. Die günstigen Fälle
        sind 
        \begin{equation}
            (4,6), (5,5), (5,6), (6,4), (6,5), (6,5).
        \end{equation}
        Nachdem es sich hier um ein Laplace-Modell handelt, ist die
        Wahrscheinlichkeit, dass die Summe größer als $10$ ist, gleich
        $\frac{6}{36}= \frac{1}{6}$.

    \item In Falle der nichtunterscheidbaren Würfel ist das kein
        Laplace-Experiment mehr. Deswegen muss geeignetes
        Wahrscheinlichkeitsmaß $P$ betrachtet werden. 
\end{enumerate}


\paragraph{Kombinatorik. Würfel. 4 vs.~24.}
Was ist wahrscheinlicher:
\begin{enumerate}
    \item Beim Werfen von vier Würfeln auf wenigstens einem eine Sechs zu
        erzielen, oder
    \item bei 24 Würfen von zwei Würfeln wenigstens einmal zwei Sechsen zu
        erhalten?
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item Das Betrachen des komplementären Ereignisses liefert
        \begin{equation}
            1 - \frac{5^4}{6^4} = 1.
            \label{}
        \end{equation}
    \item Ein äquivalentes Model ist, $24$ mal einen $36$-Würfel werfen. Die
        Wahrscheinlichkeit wenigstens ein mal die Zahl $36$ zu erhalten,
        berechnen wir indem wir das komplementäre Ereignis betrachten:
        \begin{equation} 
            1 - \frac{35^{24}}{36^{24}}.  
        \end{equation}
    \item Was ist größer? $\frac{5^4}{6^4} = 0.4822530864$ und
        $\frac{35^{24}}{36^{24}} = 0.5085961239$.
\end{enumerate}




\paragraph{Kombinatorik. Single choice test.} Bei einem single choice test
werden $n$ Fragen gestellt, wobei jeweils genau eine richtige Antwort aus $m$
Möglichkeiten gewählt werden soll. Der Test wird als bestanden angesehen, wenn
mindestens Hälfte der Fragen richtig beantwortet wurden.

Wir testen die Prüfmethode indem wir zufällig jeweils eine Antwort bei jeder Frage
ankreuzen. 
\begin{enumerate}
    \item Finden Sie einen geeigneten Wahrscheinlichkeitsraum, der diese
        Zufallssituation \linebreak beschreibt.
    \item Wie hoch ist die Wahrscheinlichkeit, dass die zufällige Antwortwahl zum 
        Bestehen des Tests führt?
    \item Berechnen Sie die obige Wahrscheinlichkeit explizit für $n=25$ und $m=4$.
\end{enumerate}

\paragraph*{Lösung.} Laplace-Modell. Anzahl der Möglichen: $| \Omega| = m^n$. 
Anzahl der Günstigen:
\begin{equation*}
    \binom{n}{ \lceil\frac{n}{2}\rceil }(m-1)^{n - \lceil\frac{n}{2}\rceil } +
    \binom{n}{   \lceil\frac{n}{2}\rceil +1 }(m-1)^{n -   \lceil\frac{n}{2}\rceil-1   } + \dots +
    \binom{n}{n} (m-1)^{n-n}. 
\end{equation*}
Für den Fall $n=25$ und $m=13$, ist $\lceil \frac{n}{2} \rceil = 13$ und die Anzahl 
der günstigen Fälle gleich 
\begin{equation}
    3794787166756 \approx 10^{12.5}.
\end{equation}
Nachdem Anzahl der möglichen Fälle ist 
\begin{equation}
    m^{n} = 4^{25} = 1125899906842624 \approx 10^{15.05},
\end{equation}
ist die Wahrscheinlichkeit für eine zufällig bestandene Prüfung ungefähr gleich
$0.0033$.




\section{Bedingte Wahrscheinlichkeiten}


\paragraph{Bedingte Wahrscheinlichkeiten. Symmetrie.}
Seien ein Wahrscheinlichkeitsraum $\left( \Omega, \fA, P \right)$ und die
Ereignisse $A,B\in \fA$ mit $P(A)>0$ und $P(B)>0$ gegeben.  Zeigen Sie die
Äquivalenz:
\begin{equation*}
    P(A | B ) > P(A) \quad \iff \quad P(B | A) > P(B).
\end{equation*}

\paragraph*{Lösung.}
\begin{equation*}
    P(A|B)>P(A) \iff \frac{ P(A \cap B) }{ P(B)} > P(A) \iff P(A \cap B) > P(A)P(B).
\end{equation*}


\paragraph{Bedingte Wahrscheinlichkeiten. 3 Würfel.} Wir werfen drei ideale Würfel. 
\begin{enumerate}
    \item Wie groß ist die Wahrscheinlichkeit, dass dabei kein Sechser geworfen wurde? 
    \item Wie groß ist die Wahrscheinlichkeit, dass dabei kein Sechser geworfen wurde, wenn
        bekannt ist, dass drei paarweise verschiedene Zahlen geworfen wurden. 
\end{enumerate}

\paragraph*{Lösung.} Die Menge aller Tripel aus Zahlen $1,\dots,6$ ist die
Ergebnismenge.  Sei $A$ das Ereignis, dass keine Sechser geworfen wurden und
$B$ das Ereignis, dass geworfene Zahlen verschieden sind. Nun ist $P(A) =
\frac{5^3}{6^3}$ und $P(B)= 6\cdot 5\cdot 4$. Mit $P(A \cap B) = 5\cdot 4\cdot
3$ erhalten wir $P(A|B)=\frac{1}{2}$.


\paragraph{Bedingte Wahrscheinlichkeiten. 2 Münzen.} Wir werfen zwei ideale Münzen.
\begin{enumerate}
    \item Die erste Münze zeigt Kopf. Berechnen Sie die Wahrscheinlichkeit,
        dass beide Münzen Kopf zeigen.

    \item Eine der Münzen zeigt Kopf. Berechnen Sie die Wahrscheinlichkeit,
        dass beide Münzen Kopf zeigen.
\end{enumerate}

\paragraph*{Lösung.} Bezeichne mit $K_1$ bzw.\ $K_2$ das Ereignis, dass die erste
bzw.\ die zweite Münze Kopf zeigt. 
\begin{enumerate}
    \item \begin{equation*}
            P( K_1 \cap K_2 | K_1 ) = \frac{ P( K_1 \cap K_2 \cap K_1) }{ P(K_1)} = \frac{1}{2}.
        \end{equation*}

    \item \begin{equation*}
            P( K_1 \cap K_2 | K_1 \cup K_2 ) = \frac{ P( K_1 \cap K_2) }{ P(K_1 \cup K_2) }
            = \frac{1}{3}. 
        \end{equation*}
\end{enumerate}


\paragraph{Bedingte Wahrscheinlichkeit ist ein Wahrscheinlichkeitsmaß.}
Seien ein Wahrscheinlichkeitsraum $(\Omega, \fA, P)$ und ein Ereignis $B\in\fA$
mit $P(B)>0$ gegeben. 
\begin{enumerate}
    \item Zeigen Sie, dass die Funktion $Q: \fA \to \bR, A \mapsto P(A|B)$ ein
        Wahrscheinlichkeitsmaß auf $(\Omega, \fA)$ ist.
    \item Sei $\fA_B = \left\{ A \cap B : A\in \fA \right\}$. Zeigen Sie, dass
        die Funktion $R: \fA_B \to \bR, A \mapsto P(A|B)$ ein Wahrscheinlichkeitsmaß
        auf $\left( B, \fA_B \right)$ ist. 
    \item \textbf{Zusatz:} Zeigen Sie, dass $\fA_B$ alle Eigenschaften eines Ereignisfeldes ($\sigma$-Algebra) aufweist.
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item Es sollen die Axiome von Kologorov gelten: (A1) $0 \leq P(A) \leq 1$
        für alle $A\in \fA$, (A2) $P(\Omega)=1$, (A3) $P\left(
        \cup_{i=1}^{\infty} A_i \right) = \sum_{i=1}^{\infty} P(A_i)$ für
        paarweise disjunkte Mengen $(A_i)_{i\in \bN}$. 

        Nun ist $Q(A) = P( A | B) =  \frac{P(A \cap B)}{ P(B)}$.

    \item Ein Ereignisfeld erfüllt folgende Axiome: (1) $\Omega \in \fA$, (2)
        $A \in \fA \impl \bar A \in \fA$, (3) $A_1, A_2, \dots \in \fA \impl
        \cup_{i\in\bN} A_i \in \fA$. 
\end{enumerate}


\paragraph{Bedingte Wahrscheinlichkeiten. Urne mit Kugeln.} In einer Urne
befinden sich $w$ weiße Kugeln und $s$ schwarze Kugeln. Es wird eine Kugel aus
der Urne gezogen, beiseite gelegt und anschließend eine weitere Kugel gezogen.
Berechnen Sie die Wahrscheinlichkeiten der folgenden Ereignisse:
\begin{enumerate}
    \item Beide Kugeln sind weiß.
    \item Die erste Kugel ist weiß und die zweite Kugel ist schwarz.
\end{enumerate}

\paragraph*{Lösung. } Wir bezeichnen mit $W_1$ das Ereignis, dass eine weiße Kugel als
erste gezogen wird, und mit $W_2$ ein Ereignis, dass eine weiße Kugel als zweite
gezogen wird. Wir sind daran interessiert, die Wahrscheinlichkeit $P(W_1 \cap W_2)$ zu
berechnen. Wir wissen aber $P(W_1 \cap W_2) = P(W_2 | W_1) P(W_1)$ und
$P(W_1) = \frac{w}{w+s}$. Es gilt auch $P(W_2 | W_1) = \frac{w-1}{w-1+s}$.
Insgesamt ist also \begin{equation*}
    P(W_1 \cap W_2) = \frac{w(w-1)}{(w+s)(w-1+s) }.
\end{equation*}

Ähnlich bezeichnen wir mit $S_2$ das Ereignis, dass eine schwarze Kugel als zweite
gezogen wird. Damit ergibt sich
\begin{equation*}
    P(W_1 \cap S_2)= P(S_2 | W_1) P(W_1) = \frac{ w s }{(w+s)(w+s-1)}.
\end{equation*}


\paragraph{Bedingte Wahrscheinlichkeiten. Urnenmodell von Polya.} Eine Urne
enthält $s$ schwarze und $w$ weiße Kugeln. Es wird eine Kugel zufällig gezogen,
ihre Farbe notiert und die Kugel gemeinsam mit $d$ weiteren Kugeln von der
gleichen Farbe in die Urne gelegt. Die Prozedur wird anschließend wiederholt. 
\begin{enumerate}
    \item Wie groß ist die Wahrscheinlichkeit, dass die zweite gezogene Kugel weiß ist?
    \item Wie groß ist die Wahrscheinlichkeit, dass die erste gezogene Kugel
        weiß ist, unter der Bedingung, dass die zweite gezogene Kugel weiß ist?
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item \begin{align*}
            P(W_2) &= P(W_2 | W_1) P(W_1) + P(W_2 | S_1) P(S_1) \\
            &= \frac{w+d}{w+s+d}\ \frac{w}{w+s} + \frac{w}{w+s+d}\ \frac{s}{w+s} = \frac{w}{w+s}.
        \end{align*}

    \item \begin{align*}
            P(W_1 | W_2 ) &= \frac{ P(W_2 | W_1) P(W_1)  }{ P(W_2) } = \frac{w+d}{w+s+d}.
        \end{align*}
        
\end{enumerate}




\paragraph{Bedingte Wahrscheinlichkeiten. Unfaire Münzen.} 
In einem Wurfexperiment werden $n$ Münzen verwendet. Davon sind $k$ Münzen
symmetrisch und zeigen Kopf mit Wahrscheinlichkeit $1/2$. Die restlichen $n-k$
Münzen zeigen Kopf mit Wahrscheinlichkeit $1/3$. 
\begin{enumerate}
    \item Es wurde zufällig eine Münze gewählt und geworfen: Kopf. Wie groß ist 
        die Wahrscheinlichkeit, dass eine asymmetrische Münze gewählt wurde?
    \item Eine zufällig gewählte Münze wird zweimal geworfen und in beiden 
        Würfen fällt Kopf. Wie groß ist die Wahrscheinlichkeit, dass eine asymmetrische
        Münze gewählt wurde. 
\end{enumerate}

\paragraph*{Lösung. } Wir bezeichnen mit $S$ bzw. $A$ das Ereignis, dass eine
symmetrische bzw. asymmetrische Münze gewählt wurde. Es gilt $P(S) = \frac{k}{n}$
und $P(A)=\frac{n-k}{n}$. Sei $K$ das Ereignis, dass mit einer festen zufällig
gewählten Münze Kopf geworfen wird. Wir suchen $P(A | K)$. Es gilt aber nach dem
Satz von Bayes
\begin{equation*}
    P( A | K) = \frac{ P( K | A) P(A) }{ P(K | A) P(A) + P(K | S) P(S) }
    = \frac{\frac{1}{3} \frac{n-k}{n} }{ \frac{n-k}{3 n} + \frac{1}{2} \frac{k}{n}} 
    = \frac{n-k}{2n + k}.
\end{equation*}

Wenn zwei Münzen geworfen werden, müssen wir das Ereignis $K$ entsprechend erweitern und 
in die obige Berechnung nochmal einsetzen. 



\paragraph{Bedingte Wahrscheinlichkeiten. Urne mit Loch.} In einer Urne befinden
sich $s$ schwarze und $w$ weiße Kugeln. Während die Urne transportiert wird,
gehen $d$ Kugeln verloren. Es wird eine Kugel zufällig aus der Urne gezogen.
\begin{enumerate}
    \item Angenommen $d=1$. Wie groß ist die Wahrscheinlichkeit, dass die
        gezogene Kugel weiß ist?
    \item Nun ist $d<s$ und $d<w$. Wie groß ist die Wahrscheinlichkeit, dass die
        gezogene Kugel weiß ist?
    \item Wie groß ist die Wahrscheinlichkeit, dass die gezogene Kugel weiß
        ist, wenn nur noch $d<w+s$ gefordert wird?
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item Im Fall $d=1$ können wir annehmen, dass eine Kugel entnommen wurde und
        den Satz von der vollständigen Wahrscheinlichkeit verwenden. 
        \begin{equation*}
            P(W_2) = P(W_2 | W_1 ) P(W_2 ) + P(W_2 | S_1) P(S_1).
        \end{equation*}

    \item Alle Arten auf die $i$ weiße und $k-i$ schwarze Kugeln für $i\in
        {0,\dots, k}$ verloren gehen können, sind gleichwahrscheinlich.
        Bezeichnen wir dieses Ereignis als $Z_i$.  Daher haben wir
        \begin{align*}
            P(W) &= \sum_{i=0}^{d} P(W | Z_i ) P(Z_i) \\
            &= \sum_{i=0}^{d} \frac{ w -i}{w+s-i} P(Z_i).
        \end{align*}
        Nun ist 
        \begin{align*}
            P(Z_i) &= \binom{k}{i} P( W_1\cap \dots\cap W_i\cap S_{i+1}\cap \dots\cap S_k ) \\
            &= \binom{k}{i} P(W_1) P(W_2 | W_1) \dots P( S_k | W_1\cap \dots\cap S_{i+1}\cap \dots\cap S_k) \\
            &= \binom{k}{i} \frac{w}{w+s}\frac{w-1}{w+s-1}\dots \frac{w-i+1}{s+w-i-1}
            \frac{s}{s+w-i}\dots\frac{s-(k-i)+1}{s+w -k+1} \\
            &= \binom{k}{i} \frac{ (w)_{i} (s)_{k-i} }{ (w+s)_{k}}.
        \end{align*}

\end{enumerate}




\paragraph{Bedingte Wahrscheinlichkeiten. Urnen mit unbekanntem Inhalt.} Es
sind zwei Urnen mit Kugeln gegeben. Eine der Urnen enthält $k_1$ weiße und
$n_1$ schwarze Kugeln und die andere enthält $k_2$ weiße und $n_2$ schwarze
Kugeln. Die Urnen sind äußerlich nicht unterscheidbar.
\begin{enumerate}
    \item Geben Sie einen Ziehungsprozedur an, die die Wahrscheinlichkeit, dass
        zwei weiße Kugeln gezogen werden unter allen Ziehungsprozeduren maximiert.

    \item Seien nun $k_1=3, n_1=7, k_2=2$ und $n_2=8$. Wie groß ist die
        Wahrscheinlichkeit, dass unter Verwendung der von Ihnen gefundenen
        Prozedur zwei weiße Kugeln gezogen werden?
\end{enumerate}

\paragraph*{Lösung.}
Nachdem die Urnen äußerlich nicht unterscheidbar sind, können wir eine
Urne nur zufällig und nur mit Wahrscheinlichkeit $1/2$ wählen. Wir wählen 
also eine Urne zufällig und ziehen eine Kugel. Es kann nur auf folgende Weisen
weiterfahren werden:

\begin{enumerate}
    \item Für die zweite Ziehung wählen wir die Urnen zufällig.
    \item Zweite Kugel ziehen wir aus der anderen Urne. 
    \item Zweite Kugel ziehen wir aus derselben Urne.
    \item Zweite Kugel ziehen wir aus derselben Urne falls die erste weiß war.
    \item Zweite Kugel ziehen wir aus anderen Urne falls die erste weiß war.
\end{enumerate}
Bezeichnen wir mit $U_{ij}$, $i,j\in \left\{ 1,2 \right\}$ das Ereignis, dass
die $j$-te Urne bei der $i$-ten Ziehung gewählt wurde. Nun berechnen wir die 
Wahrscheinlichkeit, dass zwei weiße Kugel gezogen werden, also $P(W_1 \cap W_2)$.
\begin{align*}
    P_1 = P(W_1 \cap W_2) = 
\end{align*}


\paragraph{Bedingte Wahrscheinlichkeiten. Versicherungsunternehmen.} Ein
Versicherungsunternehmen versichert $n$ Autofahrer, davon $f$ Frauen und $m$
Männer. Die Wahrscheinlichkeit, dass innerhalb eines Jahres ein zufällig
gewählter männlicher Autofahrer einen Unfall hat, ist $\alpha$. Entsprechende
Wahrscheinlichkeit für einen weiblichen Autofahrer ist $\beta$. Die Unfälle in
verschiedenen Jahren sind unabhängig. Ein Autofahrer wird zufällig gewählt. 
Berechnen Sie folgende Wahrscheinlichkeiten:
\begin{enumerate}
    \item Wie groß ist die Wahrscheinlichkeit, dass der gewählte Autofahrer 
        innerhalb des nächsten Jahres einen Unfall hat?

    \item Wie groß ist die Wahrscheinlichkeit, dass der gewählte Autofahrer
        Unfälle in zwei aufeinanderfolgenden Jahren hat?

    \item Der gewählte Autofahrer hat im letzten Jahr einen Unfall gehabt. Wie
        groß ist die Wahrscheinlichkeit, dass der Autofahrer männlich ist?
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item $\frac{\alpha m + \beta f}{n}$.
    \item $\frac{\alpha^2 m + \beta^2 f}{n}$.
    \item $\frac{\alpha m}{\alpha m + \beta f}$.
\end{enumerate}



\paragraph{Bedingte Wahrscheinlichkeiten. Urnenmodell von P\'olya. II.} Eine Urne
enthält $s$ schwarze und $w$ weiße Kugeln. Es wird eine Kugel zufällig gezogen,
ihre Farbe notiert und die Kugel gemeinsam mit $d$ weiteren Kugeln von der
gleichen Farbe in die Urne gelegt. Die Prozedur wird anschließend wiederholt.
\begin{enumerate}
    \item Berechnen Sie die Wahrscheinlichkeit $p_{k,n}$, dass in einer Sequenz von $n$
        Ziehungen $k$ schwarze Kugeln gezogen werden.
\end{enumerate}

\paragraph*{Lösung.}
%    \item Bezeichne mit $S_n$ das Ereignis, dass bei der $n$-ten Ziehung
%        schwarze Kugel gezogen wird. Zeigen Sie, dass $P(S_1)=P(S_n)$ für alle
%        $n\in\bN$ gilt.
\begin{enumerate}
    \item Es gibt $\binom{k}{n}$ mögliche Ziehungen von $n$ Kugeln, die $k$
        schwarze Kugeln beinhalten. Alle diese Ziehungen sind
        gleichwahrscheinlich, denn eine Ziehung $(I_1, \dots, I_n)$ kann in
        eine andere durch einfache Vertauschungen der Form $(I_1, \dots,
        I_{k}, I_{k+1}, \dots, I_n) \to \left( I_1, \dots, I_{k+1}, I_{k},
        \dots, I_n \right)$ überführt werden. Diese Transformationen lassen
        die Wahrscheinlichkeiten der Ziehungen unverändert. Man kann noch 
        genauer Argumentieren und zeigen
        \begin{equation*}
            P( I_1, \dots, S_k, W_{k+1}, \dots, I_n) = 
            P( I_1, \dots, W_{k}, S_{k+1}, \dots, I_n).
        \end{equation*}
        Mit Hilfe der Multiplikationsregel erhalten wir
        \begin{align*}
            P( I_1, \dots, S_k, W_{k+1}, \dots, I_n) = \\
            P(I_1)\dots P(S_k | I_{k-1}\cap\dots\cap I_1) 
            P( W_{k+1} | S_k\cap I_{k-1}\dots) \dots P(I_n | I_{n-1}\cap \dots)
        \end{align*}
        Es sind nur die Wahrscheinlichkeiten in der Mitte des obigen Produkts
        interessant, da die übrigen von der Ersetzung $(S_k\cap W_{k+1} ) \to
        (W_k \cap S_{k+1})$ nicht beeinflusst werden. Wir definieren 
        \begin{equation*}
            Q(A) = P(A | I_{k-1} \cap \dots\cap I_1)
        \end{equation*}
        und zeigen
        \begin{equation*}
            Q( S_k ) Q(W_{k+1} | S_k) = Q(W_k) Q( S_{k+1} | W_k)
        \end{equation*}
        indem wir die konkrete Zahlen einsetzen.

        Demzufolge können wir Ziehungen betrachten in denen zuerst $k$
        schwarze Kugeln und dann die $n-k$ weiße Kugeln gezogen werden.
        Wir verwenden die Multiplikationsregel:
        \begin{align*}
            P( S_1 \cap \dots \cap S_k \cap W_{k+1} \cap \dots \cap W_{n} ) = \\
            P(S_1) \dots P(S_k | S_1\cap\dots\cap S_{k-1})\dots
            P(W_{n} | S_1\cap \dots\cap W_{n-1} ) = \\
            \frac{s}{s+w} \frac{s+d}{s+w+d}\dots\frac{s+(k-1)d}{s+w+(k-1)d}
            \frac{w}{s+w+ kd}\dots\frac{w + (n-k-1)d}{ w+s+(n-1)d}
        \end{align*}
        Wir erhalten also insgesamt
        \begin{equation*}
            p_{k,n} = \binom{k}{n} P( S_1 \cap \dots \cap S_k \cap W_{k+1} \cap \dots \cap W_{n} ).
        \end{equation*}
\end{enumerate}


\paragraph{Bedingte bedingte Wahrscheinlichkeiten.} 
Seien ein Wahrscheinlichkeitsraum $(\Omega, \fA, P)$ und die Ereignisse
$A,B,C\in\fA$ mit $P(A)>0$, $P(B)>0$ und $P(C)>0$ gegeben. Wir definieren
$P(A|B|C)= Q(A|B)$ für ein Wahrscheinlichkeitsmaß $Q$ mit $Q(D)=P(D|C) \ \forall
D\in\fA$. Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item $P(A|B|C) = P(A| B \cap C) = P(A|C|B)$. 
    \item $P(A|B|C) = P(A| C)$ falls $C \subset B$.
\end{enumerate}



\section{Unabhängigkeit}

\paragraph{Unabhängigkeit. Disjunkte Ereignisse.} Seien $(\Omega, \fA, P)$ ein
Wahrscheinlichkeitsraum und die Ereignisse $A,B \in \fA$ gegeben. Zeigen Sie
folgende Aussagen:
\begin{enumerate}
    \item Für $A\cap B=\emptyset$ sind die Ereignisse $A$ und $B$ sind genau dann
        unabhängig, wenn $P(A)=0$ oder $P(B)=0$ gilt.

    \item Sind $A$ und $B$ unabhängig und ist $A \cup B = \Omega$, dann gilt entweder
        $P(A)=1$ oder $P(B)=1$. 
\end{enumerate}


\paragraph{Unabhängigkeit. Gleichwahrscheinliche Ereignisse.} Seien $(\Omega,
\fA, P)$ ein Wahrscheinlichkeitsraum und $A_1,\dots, A_n \in\fA, n\in\bN$
vollständig stochastisch unabhängige Ereignisse mit $P(A_i)=p\in[0,1]\ \forall
i\in \left\{ 1,\dots,n \right\}$ gegeben. Berechnen Sie die
Wahrscheinlichkeiten der folgenden Ereignisse:
\begin{enumerate}
    \item Alle Ereignisse $A_1,\dots, A_n$ treten ein.
    \item Keines der Ereignisse $A_1, \dots, A_n$ tritt ein. 
    \item Genau eines der Ereignisse $A_1, \dots, A_n$ tritt ein. Warum 
        ist diese Wahrscheinlichkeit kleiner als $1$?
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item $P(A_1 \cap \dots \cap A_n) = p^n$.
    \item $P(\overline{A_1 \cup \dots \cup A_n}) = (1-p)^n$.
    \item \begin{align*}
            P( (A_1 \cap \bar A_2 \cap \dots \cap \bar A_n)  \cup\dots\cup (\bar A_1 \cap \dots \cap \bar A_{n-1} \cap A_n) &= \\
            \sum_{i=1}^{n} P(\bar A_1 \cap \dots \cap A_i \cap \dots \cap \bar A_n) &= n p (1-p)^{n-1}. 
        \end{align*}
\end{enumerate}



\paragraph{Unabhängigkeit. Münzen.} Eine ideale Münze wird $n$ mal geworfen.
Bezeichnen wir als $K_i, i\in \left\{1,\dots,n\right\}$ das Ereignis, dass
beim $i$-ten Wurf Kopf fällt. 
\begin{enumerate}
    \item Finden Sie einen geeigneten Wahrscheinlichkeitsraum, der diese
        Zufallssituation beschreibt.
    \item Zeigen Sie, dass die Ereignisse $K_1,\dots, K_n$ vollständig 
        stochastisch unabhängig sind.
\end{enumerate}


\paragraph{Unabhängigkeit. Karten.} Aus einem gut gemischten Spielkartensatz
bestehend aus 52 Karten wird zufällig eine Karte gezogen. Bezeichnen wir mit
$A$ das Ereignis, dass ein Ass gezogen wird und mit $K$ das Ereignis, dass eine
Karte mit der Farbe Karo gezogen wird. Zeigen Sie, dass die Ereignisse $A$ und
$K$ unabhängig sind. 


\paragraph{Unabhängigkeit. Abschätzung mit der Exponentialfunktion.} Seien
$(\Omega, \fA, P)$ ein Wahrscheinlichkeitsraum und $A_1, \dots, A_n \in\fA$
vollständig stochastisch unabhängige Ereignisse  gegeben. Zeigen Sie:
\begin{equation*}
    P( \overline{A_1 \cup \dots \cup A_n} ) \leq \exp \left( - \sum_{i=1}^{n} P(A_i) \right).
\end{equation*}

\paragraph*{Lösung.} Benutze $1-x \leq e^{-x}$ und vollständige Induktion.


\section{Elementare Maßtheorie}

\paragraph{Maßtheorie. Borel-messbare Abbildungen.} Sei ein messbarer Raum
$(\Omega, \fA)$ mit $\Omega=\left\{ \omega_1, \omega_2, \omega_3, \omega_4
\right\}$ und der $\sigma$-Algebra $\fA = \left\{ \emptyset, \Omega, \left\{
\omega_1, \omega_2 \right\}, \left\{ \omega_3, \omega_4 \right\} \right\}$
gegeben. Wir betrachten Abbildungen von $\Omega$ nach $\bR$, wobei $\bR$ mit
der Borel'schen $\sigma$-Algebra $\cB(\bR)$ ausgestattet ist.
\begin{enumerate}
    \item Zeigen Sie, dass die Abbildung $f: \Omega \to \bR, \omega \mapsto 5$
        messbar ist.
    \item Zeigen Sie, dass die Abbildung $g: \Omega \to \bR$ mit $g(\omega_i) = i$
        für $i\in \left\{ 1,2,3,4 \right\}$ nicht messbar ist.
    \item Beschreiben Sie nun alle messbaren Abbildungen $h: \Omega \to \bR$. 
\end{enumerate}



\paragraph{Maßtheorie. Dirac-Maß.} Sei ein messbarer Raum $(\bR, \cB(\bR))$
gegeben. Für ein festes $x\in\bR$ definieren wir die Mengenfunktion $\delta_x
: \cB(\bR) \to \R$ als
\begin{equation*}
    \delta_{x} (A) =
        \begin{cases}
            1 & \textrm{ falls } x\in A \\
            0 & \textrm{ sonst. }
        \end{cases}
\end{equation*}
\begin{enumerate}
    \item Zeigen Sie, dass $\delta_x$ für alle $x\in\bR$ ein Maß ist.

    \item Betrachten wir ein $n$-Tupel $(x_1, \dots, x_n)$ mit $x_i>0$ für
        alle $i\in \left\{ 1, \dots, n \right\}$ und eine Mengenfunktion
        $\delta: \cB(\bR) \to \bR, A \mapsto \sum_{i=1}^{n} \delta_{x_i} (A)$.
        Zeigen Sie, dass $\delta$ ein Maß auf $\left( \bR, \cB(\bR)
        \right)$ ist.

    \item Sei nun $(c_1, \dots, c_n)$ ein $n$-Tupel positiver reeller Zahlen
        mit $\sum_{i=1}^{n} c_i = 1$. Zeigen Sie, dass die Mengenfunktion
        $\delta$ ein Wahrscheinlichkeitsmaß auf $\left( \bR, \cB(\bR) \right)$
        ist. 
\end{enumerate}


\section{Einfache Zufallsgrößen}


\paragraph{Maßtheorie. Bildmaß.}
Seien als Ma{\ss}raum ein Wahrscheinlichkeitsraum $\left( \Omega, \fA, P \right)$ und eine
Zufallsgröße $X: \Omega \to \bR$ gegeben, welche als messbare Abbildung zwischen den messbaren R\"aumen $\left( \Omega,\fA\right)$ und  $\left( \bR,\cB(\bR) \right)$ definiert ist.
Dabei bezeichnet $\cB(\bR)$ die Borel'sche $\sigma$-Algebra auf den reellen Zahlen. Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item Die Mengenfunktion $P_{X}: \cB(\bR) \to [0,1], A \mapsto P( X \in A
        )$ ist ein Wahrscheinlichkeitsmaß auf $(\bR, \cB(\bR))$. 

    \item Ist die Ergebnismenge $\Omega = \left\{ \omega_{1},\dots ,\omega_{n} \right\}$ endlich,
        so gibt es $n$-Tupel reeller Zahlen $(x_1,\dots ,x_n)$ und $(p_1,\dots ,p_n)$ mit 
        $p_i>0 \ \forall i\in \left\{ 1,\dots ,n \right\}$ und $\sum_{i=1}^{n} p_i = 1$, sodass 
        \begin{equation*}
            P_X = \sum_{i=1}^{n} p_i \delta_{x_i}.
        \end{equation*}
        Dabei ist $\delta_x : \cB(\bR) \to \R$ für ein festes $x\in\bR$ ein
        Diracmaß mit $$\delta_x(B)=\left\{\begin{array}{lll} 1 & \mbox{falls} & x \in B\\ 0 & \mbox{falls} & x \notin B \end{array}\right., \quad B \in \cB(\bR).$$
        Sind die Tupel
        $(x_1,\dots ,x_n)$ und $(p_1,\dots ,p_n)$ eindeutig durch $X$ bestimmt?
        Welche Bedeutung haben sie?
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item $P_X$ ist wohldefiniert, normiert und $\sigma$-additiv: $P_X \left( \bigcup_{k\geq 0} A_i \right) = P\left( X \in \bigcup_{k\geq 0} A_i \right)= P\left( \bigcup_{k\geq 0} \left\{ X\in A_i \right\} \right) = \sum_{k\geq 0}^{} P_X\left( X \in A_i \right)$.
\end{enumerate}

\paragraph{Maßtheorie. Ereignisfelder und Information.} Die Ergebnismenge
$\Omega = \left\{ (i,j) : i,j \in \left\{ 1,\dots ,6 \right\} \right\}$
gemeinsam mit dem Wahrscheinlichkeitsmaß $P( (i,j) ) = \frac{1}{36}$ beschreiben
einen Wurfexperiment, in dem zwei ideale wohlunterscheidbare Würfel geworfen
werden. 
Für ein Ergebnis $\omega = (i,j) \in \Omega$ definieren wir die Funktionen
\begin{align*}
    X_0(\omega) &= 0 \\
    X_1(\omega) &= i+j, \\
    X_2(\omega) &= i, \\
    X_3(\omega) &= 
    \begin{cases}
        1 & \text{ falls } i+j=6 \\
        0 & \text{ sonst.}
    \end{cases}
\end{align*}
\begin{enumerate}
    \item Finden Sie die kleinsten Ereignisfelder $\fA_{i}$, $i\in \left\{
        0,1,2,3 \right\}$, sodass $X_i$ Zufallsgrößen auf dem Wahrscheinlichkeitsraum
        $(\Omega, \fA_i, P)$ sind. 
    \item Beschreiben Sie den Zusammenhang zwischen den Ereignisfeldern $\fA_i$
        und der Information, die die Zufallsgrößen $X_i$ über die Ergebnisse
        $\omega \in \Omega$ liefern.
\end{enumerate}

\paragraph*{Lösung.} 
\begin{align*}
    \fA_0 &= \left\{ \emptyset, \Omega \right\} \\
    \fA_1 &= \sigma \left\{ \left\{ 11 \right\}, \left\{ 21,12 \right\}, \left\{ 13,22,31
    \right\},\dots , \left\{ 16,25,34,\dots ,61 \right\} \right\} \\
    \fA_2 &= \sigma \left\{ \left\{ 11,12,13,14,15,16 \right\},\dots ,\left\{ 61,62,\dots ,66
    \right\} \right\} \\
    \fA_3 &= \sigma \left\{ \left\{ 15,24,\dots ,51 \right\}, 
    \overline{ \left\{ 15,24,\dots ,51 \right\}  } \right\}.
\end{align*}


\paragraph{Zufallsgrößen. Bernoulli-Verteilung.} Eine Zufallsgröße $X$ mit den
Werten in der Menge $\left\{ 0,1 \right\} \subset \bR$ und der
Wahrscheinlichkeitsfunktion $P(X=1)= p$, $P(X=0)=1-p$ mit $p\in(0,1)$ heißt
Bernoulli-verteilt mit dem Parameter $p$. 
\begin{enumerate}
    \item Geben Sie die Verteilungsfunktion $F: \bR\to [0,1], x\mapsto F(x) =
        P(X<x)$ von $X$ an.

    \item Berechnen Sie den Erwartungswert $\bfE X$ und die Varianz $\bfD^2X$ von $X$. 

    \item Berechnen Sie den Erwartungswert $\bfE Y$ und die Varianz $\bfD^2Y$ der Zufallsgröße \linebreak
        $Y = \exp\left( X \right)$.

    \item Berechnen Sie die $k$-ten Momente $m_k=\bfE X^k$ und die $k$-ten zentralen Momente $\mu_k=\bfE (X-\bfE X)^k$ von 
        $X$ für alle $k\in \bN$. 
\end{enumerate}


\paragraph{Zufallsgrößen. Diskrete Gleichverteilung.} Eine Zufallsgröße heißt
diskret gleichverteilt auf $\left\{ 1,\dots ,n \right\}$ falls
$P(X=i)=\frac{1}{n}$ für alle $i\in\left\{ 1,\dots ,n \right\}$.
\begin{enumerate}
    \item Geben Sie die Verteilungsfunktion $F: \bR\to [0,1], x\mapsto F(x) =
        P(X<x)$ von $X$ an.

    \item Berechnen Sie den Erwartungswert und die Varianz von $X$. 

    \item Berechnen Sie den Erwartungswert und die Varianz der Zufallsgröße
        $Y = \log\left( X \right)$.

    \item Berechnen Sie das $k$-te Moment und das $k$-te zentrale Moment von 
        $X$ für alle $k\in \bN$. 
\end{enumerate}

\paragraph*{Lösung.}
$\bfE X = \frac{n+1}{2}$, $\sum_{k=1}^{n} k^2 = \frac{n(n+1)(2n+1)}{6}$,
$\bfE X^2 = \frac{(n+1)(2n+1)}{6}$, $\bfD^2 X = \bfE X^2 - \left( \bfE X \right)^2$.

$\bfE \log X = \frac{1}{n} \log n!$.

Die Berechnung der $k$-ten Momente führt zu der Formel von Faulhaber: \url{https://en.wikipedia.org/wiki/Faulhaber\%27s\_formula}


\paragraph{Zufallsgrößen. Funktionen der Augenzahl.} In einem Wurfexperiment
werden zwei ideale wohlunterscheidbare Würfel geworfen. Die Zufallsgrößen
$X_i$ geben die geworfene Augenzahl des $i$-ten Würfels an.
\begin{enumerate}
    \item Berechnen Sie die Wahrscheinlichkeiten $\;P(X_1-X_2 > 2)$,\; $P(X_1/X_2 > 1)$
        und \linebreak $P(|X_1-X_2| \leq 1)$. 
    \item Berechnen Sie die Verteilungsfunktion $F$ der Zufallsgröße $Y = X_1 + X_2$. 
\end{enumerate}

\paragraph*{Lösung.} $P(X_1-X_2 > 2)=\frac{6}{36}$, $P(X_1/X_2>1)=\frac{15}{36}$, 
$P(|X_1-X_2| \leq 1) =\frac{6+5+5}{36}$. 


\paragraph{Erwartungswert. Einfache Eigenschaften.} Sei $X$ eine diskrete
Zufallsgröße. Beweisen Sie folgende Aussagen.
\begin{enumerate}
    \item $\bfE \left( X - \bfE X \right)^2 = \bfE X^2 - \left( \bfE X \right)^2$, falls alle diese
        Erwartungswerte existieren.
    \item $\left( \bfE X \right)^2 \leq \bfE X^{2}$, falls beide Erwartungswerte existieren. 
    \item Angenommen $X$ hat Werte in $\bN$. Dann gilt
        \begin{equation*}
            \bfE X = \sum_{n=0}^{\infty} P(X > n). 
        \end{equation*}
\end{enumerate}

\paragraph*{Lösung.} 
\begin{align*}
    \bfE X &= \sum_{n\geq 0} n\, P\left( X=n \right) \\
    &= P(X=1)+P(X=2)+P(X=2)+\dots+ \underbrace{P(X=n)+\dots+P(X=n)}_{n \text{times}} + \dots \\
    &= \sum_{n\geq 0}^{} P(X>n).
\end{align*}



\paragraph{Geometrische Verteilung. Erwartungswerte.} Sei $X$ geometrisch
verteilt mit dem Parameter $p\in (0,1)$. Zeigen Sie folgende Aussagen: 
\begin{enumerate}
    \item \begin{equation*}
            \bfE \left[ \frac{1}{1+X} \right] = \log \left( (1-p)^{\frac{p}{p-1}} \right).
        \end{equation*}

    \item Für alle $n\in \left\{ 2,3, \dots \right\}$ gilt
        \begin{equation*}
            \bfE \left[ X(X-1)\cdot \dots \cdot (X - n+1) \right] = \frac{n!\ p^n}{(1-p)^n}.
        \end{equation*}
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item \begin{align*}
            \log (1+z) &= z - \frac{z^2}{2} + \frac{z^3}{3} - \cdots \\
            E \left( \frac{1}{1+X} \right) &= \frac{p}{1-p} \sum_{k\geq 0} \frac{(1-p)^{k+1}}{k+1}
            = \frac{p}{p-1} \log \left( 1-p \right) = \log (1-p)^{\frac{p}{p-1}}. 
        \end{align*}

    \item MISSING!!!
\end{enumerate}

\paragraph{Geometrische Verteilung. Gedächtnislosigkeit.} Sei $X$ geometrisch
verteilt mit dem Parameter $p$.
\begin{enumerate}
    \item Zeigen Sie, dass 
        \begin{equation*}
            P \left( X > i+j \,|\, X \geq i \right) = P\left( X > j \right) 
        \end{equation*}
        für alle $i,j>0$ gilt.

    \item Finden Sie eine Interpretation für die obige Beziehung.
\end{enumerate}


\paragraph{Binomialverteilung. Eigenschaften.} Sei $X \sim \bfB(n,p)$ 
binomialverteilt mit den Parametern $n$ und $p$. Zeigen Sie folgende Aussagen: 
\begin{enumerate}
    \item Das Maximum der Wahrscheinlichkeitsfunktion von $X$ wird an der Stelle
        \begin{equation*}
            \operatorname{argmax}_{k\in \left\{ 0,\dots ,n \right\}} P(X=k) = \lfloor (n+1)p \rfloor.
        \end{equation*}
        angenommen. Dabei bezeichnet $\lfloor x \rfloor$ für ein $x\in\bR$ die
        größte ganze Zahl, die nicht größer als $x$ ist. 

    \item Zeigen Sie, dass die folgende Formel gilt:
        \begin{equation*}
            P\left( X \text{ ist eine gerade Zahl} \right) = \frac{1}{2}\left( 1 + (1-2p)^n \right). 
        \end{equation*}
\end{enumerate}

\paragraph*{Lösung.} 
\begin{enumerate}
    \item Wir berechnen
        \begin{align*}
            P(X=k)/P(X=k-1) &= \frac{n+1-k}{k} \frac{p}{1-p}.
        \end{align*}
        Dieser Quotient ist größer als $1$ genau dann, wenn 
        \begin{equation*}
            (n+1)p > k. 
        \end{equation*}
    \item Sei $p_n$ die Wahrscheinlichkeit, dass eine
        $\bfB(n,p)$-verteilte Zufallsgröße gerade ist. $p_n$ erfüllt folgende rekursive
        Relation
        \begin{align*}
            p_n &= p_{n-1}(1-2p) + p, &  p_0 &= 1.
        \end{align*}
        Nun ist es einfach zu überprüfen, dass $\frac{1}{2}(1+ (1-2p)^n)$ die Lösung
        dieser Rekursion ist. 
\end{enumerate}


\paragraph{Poisson-Verteilung. Eigenschaften.} Sei $X$
Poisson-verteilt mit der Wahrscheinlichkeitsfunktion $P(X = k) =
\frac{\lambda^k}{k!} e^{-\lambda}$, $k\in \left\{ 0,1,\dots \right\}$ und dem
Parameter $\lambda>0$. Beweisen Sie folgende Aussagen: 
\begin{enumerate}
    \item Die Varianz von $X$ ist 
        \begin{equation*}
            \bfD^2 X = \lambda.
        \end{equation*}
    \item  Das Maximum der Wahrscheinlichkeitsfunktion von $X$ wird an der Stelle
        \begin{equation*}
            \operatorname{argmax}_{k\in \bN} P(X= k) = \lfloor \lambda \rfloor 
        \end{equation*}
        angenommen.

    \item Für $n\in \left\{ 2,3,\dots  \right\}$ gilt 
        $\bfE \left[ X(X-1)\cdot\dots\cdot (X-n+1) \right] = \lambda^n$.
\end{enumerate}

\paragraph*{Lösung.}


\paragraph{Binomialverteilung. Additionstheorem.} 
Sei $b(k, n, p)=P(X=k)$ die Wahrscheinlichkeitsfunktion einer binomialverteilten
Zufallsgröße $X\sim\bfB(n,p)$.
\begin{enumerate}
    \item Zeigen Sie, dass für $n_1>0$ und $n_2>0$ die Gleichung
        \begin{equation*}
            \sum_{i=0}^{k} b(i, n_1, p)\, b( k-i, n_2, p) = b( k, n_1+n_2, p ).
        \end{equation*}
        gilt. 

    \item Geben Sie die wahrscheinlichkeitstheoretische Interpretation der
        obigen Aussage.
\end{enumerate}


\paragraph{Poisson-Verteilung. Additionstheorem.} Seien $X_1\sim
\pi_{\lambda_1}$ und $X_2\sim \pi_{\lambda_2}$ unabhängige Poisson-verteilte
Zufallsgrößen mit den Parametern $\lambda_1>0$ und $\lambda_2>0$. Zeigen Sie,
dass die Zufallsgröße
\begin{equation*}
    Y = X_1 + X_2
\end{equation*}
ebenfalls Poisson-verteilt mit dem Parameter $\lambda=\lambda_1 + \lambda_2$ ist. 


\paragraph{Binomialverteilung. Verteilungsfunktion.} Seien $X\sim\bfB(n,p)$ binomialverteilt
und $B(k, n, p) = P( X <k)$ die Verteilungsfunktion von $X$. Zeigen Sie:
\begin{equation*}
    B(k+1, n, p) = (n-k) \binom{n}{k} \int_{0}^{1-p} t^{n-k-1} (1-t)^k dt.
\end{equation*}

\paragraph*{Lösung.}
\begin{enumerate}
    \item Die Formel kann mit Hilfe der partiellen Integration hergeleitet werden.
        \begin{align*}
            B(k+1, n, p) &= (n-k) \binom{n}{k} \int_{0}^{1-p} t^{n-k+1} (1-t)^{k} dt \\
            &= (n-k)\binom{n}{k} \left[  \frac{1}{n-k}(1-p)^{n-k} p^{k} + 
            \frac{k}{n-k} \int_{0}^{1-p} t^{n-k} (1-t)^{k-1} dt \right] \\
            &= \binom{n}{k} (1-p)^{n-k} p^{k} + 
            (n-(k-1)) \binom{n}{k-1} \int_{0}^{1-p} t^{n-(k-1)-1} (1-t)^{k-1} dt \\
            &= \binom{n}{k} (1-p)^{n-k} p^{k} + B(k, n, p). \\
            B(1,n,p) &= n \binom{n}{0} \int_{0}^{1-p} t^{n-1} dt = (1-p)^n.
        \end{align*}
        Damit erhalten wir
        \begin{equation*}
            B(k+1, n,p) = \sum_{i=0}^{k} \binom{n}{k} p^{i}(1-p)^{n-i}.
        \end{equation*}
\end{enumerate}

\paragraph{Geometrische Verteilung. Zusammenhang mit der Gleichverteilung.}
Die Zufallsgrößen $X$ und $Y$ sind stochastisch unabhängig und geometrisch verteilt mit dem Parameter $p\in(0,1)$.
\begin{enumerate}
    \item Berechnen Sie die Wahrscheinlichkeiten der Ereignisse $X=Y$ und
        $X\geq 2Y$.

    \item Zeigen Sie, dass für $k,n\in\bN$ und $n>k$
        \begin{equation*}
            P \left( X = k \,|\, X+Y = n \right) = \frac{1}{n+1}
        \end{equation*}
        gilt. Mit anderen Worten: Die bedingte Verteilung von $X$ bei gegebenem Wert von 
        $X+Y$ ist die Gleichverteilung.
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item \begin{align*}
            P\left( X = Y \right) &= \sum_{k=0}^{\infty} P\left( X=k \wedge Y=k \right) = \frac{p}{1+p} \\
            P\left( X \geq 2y \right) &= \left( 1-p \right)^{2y} \\
            P\left( X \geq 2Y \right) &= \frac{p}{1- (1-p)^{3}}.
        \end{align*}

    \item Für $X_1 \sim \text{Poiss}(p_1)$ und $X_2\sim\text{Poiss}(p_2)$ erhalten wir
        \begin{align*}
            P\left( X_1 = k \,|\, X_1+X_2 = n \right) &= 
            \frac{(1-p_1)^{k} (1-p_2)^{n-k}  }{ \sum_{i=0}^{n} (1-p_1)^i (1-p_2)^{n-i} }. 
        \end{align*}
\end{enumerate}


\paragraph{Negative Binomialverteilung. Konstruktion.} $X$ ist negativ
binomialverteilt mit den Parametern $n\in\bN$ und $p\in \left( 0,1 \right)$, wenn
für $k\in\bN$
\begin{equation*}
    P(X = k) = \binom{k+n-1}{n-1} p^{n} (1-p)^{k} 
\end{equation*}
gilt. Seien $Z_1,Z_2\dots$ stochastisch unabhängige und geometrisch verteilte Zufallsgrößen mit 
$Z_i \sim \text{Geom}(p)$. Zeigen Sie folgende Aussagen: 
\begin{enumerate}
    \item Die Zufallsgröße $Z_1+Z_2$ ist negativ binomialverteilt mit den
        Parametern $(2,p)$. 
    \item Für $k, n \in\bN$ gilt
        \begin{equation*}
            \binom{k+n}{n} = \sum_{j=0}^{k} \binom{j+n-1}{n-1}.
        \end{equation*}
    \item Die Zufallsgröße $Z_1+\dots+Z_n$ ist negativ binomialverteilt mit den
        Parametern $(n,p)$. 
\end{enumerate}

\paragraph*{Lösung.}
\begin{enumerate}
    \item \begin{equation*}
            P\left( Z_1+Z_2 = k \right) = (k+1)p^{2}(1-p)^k.
        \end{equation*}
    \item Beweisen wir zuerst
        \begin{align*}
            \binom{n}{k} &= \binom{n-1}{k-1} + \binom{n-1}{k} \\
            \binom{n+k}{k} &= \binom{n+k}{n}
        \end{align*}
        und den Rest mit Induktion.
    \item \begin{align*}
            P\left( \sum_{i=1}^{n+1} Z_i = k \right) &= 
            \sum_{j=0}^{k} P\left( \sum_{i=1}^{n} Z_i = j \right) P\left( Z_{n+1} =k-j \right) \\
            &= \sum_{j=0}^{k} \binom{j+n-1}{n-1} p^{n}(1-p)^{j} (1-p)^{k-j}p \\
            &= \sum_{j=0}^{k} \binom{j+n-1}{n-1} p^{n+1}(1-p)^{k} = \binom{k+n}{n} p^{n+1}(1-p)^{k}. 
        \end{align*}
\end{enumerate}


\paragraph{Diskrete Gleichverteilung. Ordnungsstatistiken.} Die Zufallsgrößen
$X_1,\dots ,X_m$ seien stochastisch unabhängig und diskret gleichverteilt auf der Menge
$\left\{ 1,\dots ,n \right\}$, mit $n>1$. Wir bezeichnen mit $X_{(1)} = \min
(X_1,\dots ,X_m)$ das Minimum und mit $X_{(m)} = \max \left( X_1,\dots ,X_n
\right)$ das Maximum der Zufallsgrößen $X_1,\dots ,X_m$.
%\begin{enumerate}
%    \item 
Finden Sie die Wahrscheinlichkeitsfunktion der Zufallsgrößen
        $X_{(1)}$ und $X_{(n)}$.
%    \item Finden Sie die Wahrscheinlichkeitsfunktion der Zufallsgröße $X_{(n)}-X_{(1)}$.
%\end{enumerate}

\paragraph*{Lösung.}
\begin{align*}
    P\left( X_{(1)} \leq k \right) &= 1 - \left( \frac{n-k}{n} \right)^m \\
    P\left( X_{(m)} \leq k \right) &= \left( \frac{k}{n} \right)^{m}. 
\end{align*}
Daraus Wahrscheinlichkeitsfunktion berechnen. 


\paragraph{Geometrische Verteilung. Minima und Maxima.} Seien $Y_1,\dots ,Y_m$
stochastisch unabhängig und geometrisch verteilt mit dem Parameter $p\in(0,1)$.  Finden Sie
die Wahrscheinlichkeitsfunktion der Zufallsgrößen $Y_{(1)}$ und $Y_{(n)}$.

\paragraph*{Lösung.}
\begin{align*}
    P\left( Y_{(1)} < k \right) &= 1 - \left( 1-p \right)^{mk} \\
    P\left( Y_{(n)} < k \right) &= \left( \left( 1-p \right)^{k-1} p \right)^n.
\end{align*}


\paragraph{Geometrische Verteilung. Summen.} Seien $X$ und $Y$ unabhängig und geometrisch
verteilt mit den Parametern $p_1, p_2 \in (0,1)$. Zeigen Sie folgende Aussagen: 
\begin{enumerate}
    \item Die Wahrscheinlichkeitsfunktion der Summe $X+Y$ ist gegeben durch
        \begin{equation*}
            P(X+Y = n ) = \frac{p_1 p_2}{p_1 - p_2} \left( (1-p_2)^{n+1}- (1-p_1)^{n+1} \right).  
        \end{equation*}

    \item 
\end{enumerate}


\paragraph{Exponentialverteilung. Minima.}
Seien $X_1, \dots, X_n$ stochastisch unabhängig und exponentialverteilt mit
$X_i\sim \mathbf{Ex}(\lambda_i)$ und $\lambda_i>0$. Zeigen Sie, dass die
Zufallsgröße 
\begin{equation*}
    X_{(1)} = \min \left\{ X_1, \dots, X_n \right\} 
\end{equation*}
exponentialverteilt ist und dabei $X_{(1)} \sim \mathbf{Ex}(\lambda_1+\dots+\lambda_n)$ gilt.


\paragraph{Stetige Gleichverteilung. Momente.}
Sei $X$ stetig gleichverteilt auf dem Intervall $\left[ a,b \right]\subset \R$.
Zeigen Sie folgende Aussagen: 
\begin{enumerate}
    \item Für $k=1,2,\dots$ ist das $k$-te Moment $\bfE X^{k}$ gegeben durch
        \begin{equation*}
            \bfE X^k = \frac{1}{k+1}\left( b^{k}a^0 + \cdots + b^{0}a^{k} \right).
        \end{equation*}
    \item Der Erwartungswert und die Varianz von $X$ sind gegeben durch
        \begin{align*}
            \bfE X    & = \frac{1}{2} \left( b+a \right) & 
            \bfD^2 X  & = \frac{1}{12} \left( b-a \right)^{2}.  
        \end{align*}
\end{enumerate}

\paragraph*{Lösung.} Berechnen wir zunächst die $n$-ten Momente
\begin{align*}
    \bfE X^n &= \int_{\R}^{} x^n \frac{1}{b-a} 1_{(a,b)}(x) dx \\
    &= \frac{1}{b-a} \int_{a}^{b} x^n dx \\
    &= \frac{1}{b-a} \left( \frac{x^{n+1}}{n+1} \right) |_{a}^{b} \\
    &= \frac{1}{b-a}\frac{1}{n+1} \left( b^{n+1}-a^{n+1} \right) \\
    &= \frac{a^n b^{0} +\cdots+ a^0 b^{n}}{n+1}.
\end{align*}
Wir brauchen nur noch $n=1,2$ einzusetzen.
\begin{align*}
    \bfE X &= \frac{a+b}{2} & \bfE X^2 &= \frac{b^2 + ab + a^2}{3} & 
    \bfD^2 X = \frac{(b-a)^2}{12}.
\end{align*}


\paragraph{Cauchy-Verteilung. Gegenbeispiel.}
Sei $X$ Cauchy-verteilt mit der Dichtefunktion
\begin{equation*}
    f(x) = \frac{1}{\pi (1+x^2)}.
\end{equation*}
Zeigen Sie folgende Aussagen:
\begin{enumerate}
    \item Die Funktion $f$ ist wirklich eine Dichtefunktion.
    \item Der Erwartungswert $\bfE X$ existiert nicht.
    \item Die Momente $\bfE X^k$ existieren für alle $k=1,2,\dots$ nicht.
\end{enumerate}


\paragraph{L\'evy-Verteilung. Dichtefunktion.} Die Dichtefunktion der
L\'evy-Verteilung mit den Parametern $(\mu,\sigma)\in \bR\times\bR$, $\sigma>0$ ist
\begin{equation*}
    f(x) = 
    \begin{cases}
    \sqrt{ \frac{\sigma}{2\pi}} (x-\mu)^{-\frac{3}{2}} \exp\left( -\frac{\sigma}{2(x-\mu)} \right), & x\geq 0 \\
    0, & x<0.
    \end{cases}
\end{equation*}
Sei $X\sim \bfN (0, \frac{1}{\sigma})$ normalverteilt mit $\sigma>0$.  Zeigen
Sie, dass die Zufallsgröße $\frac{1}{X^2}$ L\'evy-verteilt mit den
Parametern $(0,\sigma)$ ist.

\paragraph*{Lösung.} 
Für ein $y>0$ gilt
\begin{align*}
    P \left( \frac{1}{X^{2}} \leq y \right) &= P \left( 1 \leq y X^{2}   \right) \\
    &= P \left( \sqrt{\frac{1}{y}} \leq | X | \right) \\
    &= 2 \left( 1 - P \left( X < \frac{1}{\sqrt{y}} \right) \right) \\
    &= 2 \left( 1 - \Phi\left( \sqrt{ \frac{\sigma}{y}} \right) \right).
\end{align*}
Die Dichte von $\frac{1}{X^2}$ erhalten wir durchs Differenzieren:
\begin{align*}
    \frac{\partial}{\partial y} P \left( \frac{1}{X^2} \leq y \right) &=
    -2 \phi \left( \sqrt{\frac{\sigma}{y}} \right) \sqrt{\sigma} 
    \left( -\frac{1}{2} y^{-\frac{3}{2}} \right) \\
    &= \sqrt{\frac{\sigma}{2 \pi}} \exp \left( - \frac{\sigma}{2 y} \right) y^{-\frac{3}{2}}.
\end{align*}
Daher ist $\frac{1}{X^2} \sim \text{L\'evy}(\sigma)$.


\paragraph{Mellin-Transformation.} Sei $X$ eine positive Zufallsgröße.
Die Mellin-Transformierte $T_{X}(\theta)$ von $X$ ist definiert durch
\begin{equation*}
    T_{X}(\theta) = \bfE X^{\theta}
\end{equation*}
für alle $\theta\in\bR$ für die der obige Erwartungswert existiert. Zeigen Sie
folgende Aussagen:
\begin{enumerate}
    \item Es gilt
        \begin{equation*}
            T_{X}(\theta) = \varphi_{ \log X} (\theta/i)
        \end{equation*}
        wenn alle obigen Ausdrucke wohldefiniert sind.
    \item Sind $X$ und $Y$ positive und stochastisch unabhängige Zufallsgrößen, so gilt
        \begin{equation*}
            T_{XY}(\theta) = T_{X}(\theta) \ T_{Y}(\theta). 
        \end{equation*}
    \item Es gilt
        \begin{equation*}
            T_{b X^{a}}(\theta) = b^{\theta} T_{X} (a\theta)
        \end{equation*}
        für alle $b>0$ und alle $a$ für die $T_{X}(a\theta)$ exisitiert. 
\end{enumerate}


\paragraph{Log-Normalverteilung. Dichtefunktion.} Ist $X\sim\bfN(\mu,
\sigma^2)$ normalverteilt, dann ist die Zufallsgröße $Y=e^{X}$
log-normalverteilt mit Parametern $\mu$ und $\sigma^{2}$. Zeigen Sie, dass die
Dichtefunktion von $Y$ durch
\begin{equation*}
    f(x) = 
    \begin{cases}
        \frac{1}{\sigma x \sqrt{2\pi}} \exp \left( -\frac{1}{2} \left( \frac{\log x -\mu}{\sigma} \right)^2  \right), & x\geq 0 \\
        0, & x<0
    \end{cases}
\end{equation*} 
gegeben ist.


\paragraph{Log-Normalverteilung. Momente.} Ist $X\sim\bfN(\mu, \sigma^2)$
normalverteilt, dann ist die Zufallsgröße $Y=e^{X}$ log-normalverteilt mit
Parametern $\mu$ und $\sigma^{2}$. Zeigen Sie folgende Aussagen: 
\begin{enumerate}
    \item Für die Mellin-Transformierte $T_{Y}$ von $Y$ gilt
        \begin{equation*}
            T_{Y}(k) =  \bfE Y^{k}, \ k=1,2,\dots
        \end{equation*}

    \item Die Momente $m_k$ von $Y$ sind gegeben durch
        \begin{equation*}
            m_{k} = \bfE Y^{k} = \exp \left( \mu k + \frac{ \sigma^2 k^{2}}{2} \right).
        \end{equation*}
\end{enumerate}


\paragraph{Normalverteilung. Abweichungen vom Mittelwert.} Sei $X\sim\bfN(\mu, \sigma^2)$
normalverteilt. 
\begin{enumerate}
    \item Berechnen Sie die Wahrscheinlichkeiten $P(X - \mu \geq n \sigma)$ für
        $n\in \left\{ 1,2,6 \right\}$.
    \item Berechnen Sie die Wahrscheinlichkeiten $P( | X-\mu| \geq n \sigma)$ für 
        $n\in \left\{ 1,2,6 \right\}$.
\end{enumerate}



