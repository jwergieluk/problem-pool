
\section{Identifizierbarkeit}

\problem{Identifizierbarkeit im linearen Modell. } 
Man nehme an, dass folgendes Modell gegeben sei: 
\begin{equation*}
    Y_i = \sum_{j=1}^{p} z_{ij} \beta_j + \varepsilon_i, \quad i=1,\ldots,n. 
\end{equation*}
Hierbei seien $z_{11},\ldots,z_{np}$ bekante Konstanten und $\varepsilon_1,\ldots,\varepsilon_n$
i.i.d.\ mit $\varepsilon_1 \sim \mathcal N\left( 0,1 \right)$.
\begin{enumerate}
    \item Zeigen Sie, dass $\left( \beta_1,\ldots,\beta_p \right)$ genau dann
        identifizierbar ist, falls $\mathbf z_1,\ldots,\mathbf z_p$ linear
        unabhängig sind, wobei $\mathbf z_j = \left( z_{1j},\ldots,z_{nj}
        \right)^\top$.
    \item Begründen Sie, warum $\left( \beta_1, \ldots,\beta_p \right)$ nicht
        identifizierbar ist, falls $n<p$.
\end{enumerate}

\solution
Seien $Y = \left( Y_1, \cdots, Y_n \right)^\top$ und $\varepsilon = \left(
\varepsilon_1, \cdots, \varepsilon_n \right)^\top$ Zufallsvektoren in $\R^{n}$.
Die konstante Matrix $Z=\left( z_{ij} \right)_{ij}$ liegt in $\R^{n \times p}$
und der Parametervektor $\beta = \left( \beta_1, \cdots, \beta_p
\right)^{\top}$ in $\R^{p}$.  Somit ist $\Theta = \R^{p}$. Das Model ist genau
dann identifizierbar, wenn verschiedene Vektoren $\beta$, verschiedene
Zufallsvariablen $Y(\beta)$ induzieren. Sind die Spalten $\left( \mathbf{z}_1,
\cdots, \mathbf{z}_p \right)$ linear unabhängig, so hat die Matrix $Z$ vollen
Rang und die Multiplikation mit $Z$ ist eine Bijektion. In diesem Fall ist das
Model identifizierbar. 

Wenn $p>n$ kann die Matrix $Z$ nicht vollen Rang haben. In diesem Fall ist die
Multiplikation mit $Z$ keine Bijektion und es gibt $\beta,\tilde \beta \in
\R^{p}$ mit $\beta \neq \tilde \beta$ und $Z \beta = Z \tilde \beta$. Das Model
ist nicht identifizierbar. 



\section{Suffizienz}


\problem{Suffizienz. Invariante Transformationen.} 
Sei $\mathcal P = \left\{ p(x,\theta) : \theta\in\Theta \right\}$ ein reguläres
statistisches Modell, $X\sim \mathcal P$, die Statistiken $T(X)$ und $S(X)$ mit
$T=\eta(S)$ und einer messbaren Abbildung $\eta$ gegeben. Zeigen Sie: Falls
$T(X)$ suffizient für $\theta$ ist so ist auch $S(X)$ suffizient für $\theta$.

\solution $T$ ist suffizient, so folgt
\begin{align*}
    p_\theta(x) &= g(T(x), \theta) h(x) \\
    &= g(\eta(S(x)), \theta) h(x). 
\end{align*}
Somit ist $S$ ebenfalls suffizient. 



\problem{Qualitätskontrolle.} 
Es sei eine \textsc{lkw}-Ladung mit $N$ Fernsehgeräten gegeben, wovon $N
\theta$ defekt sind. Es werden $N$ Fernseher (ohne Zurücklegen) überprüft.  Man
definiere
\begin{equation*}
    X_i = \begin{cases}
        1 & \text{$i$-ter überprüfter Fernseher ist defekt} \\
        0 & \text{sonst}.
    \end{cases}
\end{equation*}
\begin{enumerate}
    \item Zeigen Sie ohne Verwendung des Faktorisierungstheorems, dass $\sum_{i=1}^{n} X_i$ 
        suffizient für $\theta$ ist. 
    \item Zeigen Sie mit Hilfe des Faktorisierungstheorems, dass $\sum_{i=1}^{n} X_i$ suffizient
        für $\theta$ ist.
\end{enumerate}


\problem{Inverse Gamma-Verteilung. Suffizienz.}
Eine i.i.d.-Stichprobe $X_1, \ldots, X_n$ sei invers Gamma-verteilt mit der
Dichte 
\begin{equation*}
    p_{\alpha, \beta}\left( x \right) = 
    \frac{\beta^\alpha}{ \Gamma\left( \alpha \right)} x^{-(\alpha+1)}\exp\left( -\frac{\beta}{x} \right)1_{\R>0}(x)
\end{equation*}
Finden Sie eine zweidimensionale suffiziente Statistik für $\alpha$ und $\beta$.


\problem{Beta-Verteilung. Suffizienz.}
Seien $X_1, \cdots, X_n$ i.i.d.\ Beta-verteilt mit der Dichte
\begin{equation*}
    p_{a,b} = \frac{1}{B(a,b)} x^{a-1} (1-x)^{b-1} 1_{[0,1]}(x)
\end{equation*}
und den Parametern $a>0$ und $b>0$. Dabei ist $B(a,b)=
\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$.
\begin{enumerate}
    \item $a$ sei fix und bekannt.  Finden Sie eine suffiziente Statistik für
        $b$.
    \item Finden Sie eine zweidimensionale suffiziente Statistik für $(a,b)$.
\end{enumerate}


\problem{Pareto-Verteilung. Suffizienz.}
Seien $X_1,\ldots,X_n$ i.i.d.\ Paretoverteilt mit der Dichte
\begin{equation*}
    p_\theta(x)= \frac{\theta a^\theta}{x^{\theta+1}}1_{\R_{>a}}(x),
\end{equation*}
wobei $\theta>0$ und $a$ fix und bekannt sei. Finden Sie eine reellwertige
suffiziente Statistik $T(X_1,\ldots,X_n)$ für $\theta$.


\problem{Poisson-Verteilung. Suffizienz.}
Seien $X_1,\cdots, X_n$ i.i.d.\ Poisson-verteilt mit dem Parameter $\lambda>0$ 
und der Wahrscheinlichkeitsfunktion 
\begin{equation*}
    P(X_1=k) = \frac{e^{-\lambda}\lambda^k}{k!}, \quad k\in \left\{ 0,1,2,\cdots \right\}.
\end{equation*}
Zeigen Sie ohne Verwendung des Faktorisierungstheorems, dass 
\begin{equation*}
    T(X) = \sum_{i=1}^{n} X_i
\end{equation*}
suffizient für $\lambda$ ist. 



\problem{Suffizienz. Beispiele.} 
Seien $X_1,\ldots,X_n$ i.i.d.\ mit jeweils folgender Dichte. Finden Sie in
allen Fällen eine reellwertige suffiziente Statistik $T$ für $\theta$ basierend
auf der Beobachtung von $(X_1, \cdots, X_n)$. 
\begin{enumerate}
	\item $X_1$ ist nichtzentral doppelt exponentialverteilt mit der Dichte 
        \begin{equation*}
            p_\theta (x) = \frac{1}{2\theta} \exp \left( \frac{-|x-\mu|}{\theta} \right),
        \end{equation*}
        wobei $\theta >0$ und $\mu$ bekannt sei.
	\item $X_1$ ist gleichverteilt auf dem Intervall $(-\theta,\theta)$ mit der Dichte 
        \begin{equation*}
            p_\theta (x) = \frac{1}{2\theta} 1_{\left( -\theta ,\theta \right)}(x),
        \end{equation*}
        wobei $\theta >0$.
	\item $X_1$ ist invers Gamma-verteilt mit der Dichte 
        \begin{equation*}
            p_\theta (x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{- (\alpha+1)} 
            \exp\left(  -\frac{\beta}{x} \right) 1_{\R_{>0}}(x), 
        \end{equation*}
        wobei $\theta = (\alpha, \beta)$ und $\alpha, \beta > 0$.
\end{enumerate}

\solution 
Wir stellen die gemeinsame Dichte $p_\theta(x_1, \cdots, x_n)$ von $(X_1, \cdots, X_n)$ in der Form
\begin{equation*}
    p_{\theta} (x_1, \cdots, x_n) = g(T(x_1, \cdots, x_n), \theta) h(x_1, \cdots, x_n)
\end{equation*}
für geeignete Funktionen $g$ und $h$ dar und wenden den Faktorisierungssatz an. 
\begin{enumerate}
    \item \begin{align*}
            p_\theta(x_1, \cdots, x_n) &= \prod_{i=1}^{n} p_\theta(x_i) \\
            &= \frac{1}{ (2\theta)^n} \exp \left( - \frac{1}{\theta} \sum_{i=1}^{n} | x_i - \mu | \right). 
        \end{align*}
        Somit ist 
        \begin{align*}
            T(x_1, \cdots, x_n) &= \sum_{i=1}^{n} | x_i - \mu | \\
            g(x, \mu)  &= \frac{1}{ (2\theta)^n} \exp \left( - \frac{1}{\theta} x \right).
        \end{align*}
    \item \begin{align*}
            p_\theta(x_1, \cdots, x_n) &= \prod_{i=1}^{n} \frac{1}{2\theta} 1_{\left( -\theta ,\theta \right)}(x) \\
            &= \left( \frac{1}{2\theta} \right)^n 1_{(-\theta, \theta)} ( \max \left\{  |x_1|, \cdots, |x_n| \right\}).
        \end{align*}
        Somit ist $T(x_1, \cdots, x_n) = \max \left\{  |x_1|, \cdots, |x_n| \right\}$. 
    \item \begin{align*}
            p_\theta(x_1, \cdots, x_n) &= \prod_{i=1}^{n} 
            \frac{\beta^\alpha}{\Gamma(\alpha)} x_i^{- (\alpha+1)} 
            \exp\left(  -\frac{\beta}{x_i} \right) 1_{\R_{>0}}(x_i) \\
            &= \left( \frac{\beta^\alpha}{\Gamma(\alpha)} \right)^n 
            \left( \prod_{i=1}^n x_i \right)^{-\left( \alpha+1 \right)}
            \exp \left( -\beta \sum_{i=1}^{n} \frac{1}{x_i} \right) 1_{ \R_{>0}^n} (x_1, \cdots, x_n).
        \end{align*}
        Wir erhalten
        \begin{align*}
            T(x_1, \cdots, x_n) &= \left( \prod_{i=1}^n x_i, \sum_{i=1}^{n} \frac{1}{x_i} \right) \\
            g(y,z, \theta) &= \left( \frac{\beta^\alpha}{\Gamma(\alpha)} \right)^n 
            y^{-\left( \alpha+1 \right)} \exp \left( -\beta z \right) \\
            h(x_1, \cdots, x_n) &= 1_{ \R_{>0}^n} (x_1, \cdots, x_n).
        \end{align*}
\end{enumerate}




\problem{Nichtzentrale Exponentialverteilung. Suffizienz.}
Seien $X_1,\ldots,X_n$ i.i.d.\ mit der Dichte
\begin{equation*}
	p_\theta (x) = \frac{1}{\sigma} \exp \left(-\frac{x-\mu}{\sigma} \right) 1_{\R_{\geq\mu}}(x)
\end{equation*}
sowie $\theta = (\mu, \sigma)^\top$ und $\Theta = \R \times \R_{>0}$. 
\begin{enumerate}
    \item Zeigen Sie, dass $\min(X_1,\ldots,X_n)$ eine suffiziente Statistik
        für $\mu$ ist, falls $\sigma$ bekannt ist.
    \item Finden Sie eine eindimensionale, suffiziente Statistik für $\sigma$,
        falls $\mu$ bekannt ist.
    \item Finden Sie eine zweidimensionale suffiziente Statistik für $\theta$ an.
\end{enumerate}

\solution
\begin{enumerate}
    \item Sei $\sigma>0$ fix und bekannt. 
        \begin{align*}
            p_\theta (x_1, \cdots, x_n) &= \prod_{i=1}^{n} 
            \frac{1}{\sigma} \exp \left(-\frac{x_i-\mu}{\sigma} \right) 1_{\R_{\geq\mu}}(x_i) \\
            &= \frac{1}{\sigma^n} \exp \left( -\frac{1}{\sigma} \sum_{i=1}^{n} x_i + \frac{n\mu}{\sigma} \right)
            1_{\R_{\geq \mu}}\left( \min \left\{ x_1, \cdots, x_n \right\} \right)  \\
            &= \underbrace{
            \sigma^{-1} \exp \left( \frac{n\mu}{\sigma} \right) 
            1_{\R^n_{\geq \mu}}\left( \min \left\{ x_1, \cdots, x_n \right\} \right) }_{=g}
            \underbrace{\exp \left( -\frac{1}{\sigma} \sum_{i=1}^{n} x_i  \right) }_{=h}.
        \end{align*}
        Somit ist $T(X) = \min \left\{ x_1, \cdots, x_n \right\}$ eine suffiziente Statistik für $\mu$. 
    \item Sei nun umgekehrt $\mu$ bekannt und fix. Mit derselben Methode erhalten wir
        \begin{align*}
            p_\theta (x_1, \cdots, x_n) &= \sigma^{-1} 
            \exp \left( - \frac{1}{\sigma} \sum_{i=1}^{n} (x_i - \mu) \right) 1_{\R^{n}_{\geq \mu}} (x_1, \cdots, x_n) \\
            T(  x_1, \cdots, x_n) &= \sum_{i=1}^{n} (x_i - \mu). 
        \end{align*}
    \item Eine zweidimensionale suffiziente Statistik für $\theta = (\mu, \theta)$ erhalten wir aus
        \begin{align*}
            p_\theta(x_1, \cdots, x_n) &= \sigma^{-1} \prod_{i=1}^n e^{ -\frac{x_i}{\sigma}} e^{\frac{n\mu}{\sigma}}
            1_{\R^n_{\geq \mu}}\left( \min \left\{ x_1, \cdots, x_n \right\} \right) \\
            &= \sigma^{-1} \left( e^{- \sum_{i=1}^{n} x_i} \right)^{\sigma^{-1}} 
            e^{\frac{n\mu}{\sigma}}
            1_{\R^n_{\geq \mu}}\left( \min \left\{ x_1, \cdots, x_n \right\} \right) \\
            T( x_1, \cdots, x_n ) &= \left( \min \left\{ x_1, \cdots, x_n \right\}, 
            e^{-\sum_{i=1}^{n} x_i} \right) \in \Theta = \R \times \R_{>0}. 
        \end{align*}
\end{enumerate}



\section{Exponentielle Familien}


\problem{Exponentielle Familie mit einer unvollständigen natürlichen Statistik.}
Finde ein Gegenbeispiel um die Notendigkeit der Bedingung $c(\Theta)$ offen zu
illustrieren. 


\problem{Beta-Verteilung als exponentielle Familie.}
Sei $X$ eine Beta-verteilte Zufallsvariable mit der Dichte
\begin{equation*}
    p_{a,b} = \frac{1}{B(a,b)} x^{a-1} (1-x)^{b-1} 1_{[0,1]}(x)
\end{equation*}
und den Parametern $a>0$ und $b>0$.
Zeigen Sie, dass $p_{(a,b)}$ zu einer zweiparametrigen exponentiellen
Familie gehört.

\solution Die Dichte der Beta-Verteilung lässt sich in der Form
\begin{equation*}
    p_{(a,b)} = \frac{1}{B(a,b)} \exp \left( 
        (a-1) \log x + (b-1) \log (1-x) 
        \right) 1_{[0,1]}(x)
\end{equation*}
schreiben. Das ist eine exponentielle Familie mit 
\begin{align*}
    c(a,b) &= \left( a-1, b-1 \right) \\
    T(x) &= \left( \log x, \log (1-x) \right)^\top \\
    d &= - \log B(a,b) \\
    S &= 0 \\
    A &= [0,1].
\end{align*}


\problem{Gamma-Momente und exponentielle Familien.}
Sei $X$ Gamma-verteilt mit den Parametern $a,\lambda>0$ und der Dichte
\begin{equation*}
    p_{a,\lambda} (x) = \frac{\lambda^a}{\Gamma(a)} x^{a-1} e^{-\lambda x} 1_{\R_{\geq 0}} (x).
\end{equation*}
Zeigen Sie, dass 
\begin{align*}
    \E X = \frac{a}{\lambda} && \Var X = \frac{a}{\lambda^2} 
\end{align*}
gilt, indem Sie die Dichte $p_{a,\lambda}$ bei fixiertem $a$ als einparametrige
natürliche exponentielle Familie darstellen und $d_0$ ableiten. 


\problem{Poisson-Momente und exponentielle Familien.}
Sei $X$ Poisson-verteilt mit dem Parameter $\lambda>0$ und der Wahrscheinlichkeitsfunktion
\begin{equation*}
    p_{\lambda}(k)= P(X=k) = 
    \frac{e^{-\lambda} \lambda^{k} }{k!}, \quad k\in \left\{ 0,1,\cdots \right\}.
\end{equation*}
Zeigen Sie, dass 
\begin{align*}
    \E X = \lambda && \Var X = \lambda 
\end{align*}
gilt, indem Sie die Wahrscheinlichkeitsfunktion $p_{\lambda}$ 
als einparametrige natürliche exponentielle Familie darstellen und $d_0$ ableiten. 

\solution
Wir schreiben $p_\lambda$ in exponentieller Form
\begin{equation*}
    p_{\lambda}(k) = \frac{e^{-\lambda} \lambda^{k}}{k!} =
    \exp \left( k \log \lambda - \lambda - \log k! \right)
\end{equation*}
auf. Dabei ist $T(k) = k$, $c(\lambda)= \log \lambda$, $d(\lambda)= -\lambda$, 
$S(k) = - \log k!$. Wir setzen nun $\eta=\log \lambda$ und schreiben die Dichte 
$p_{\eta}(k)$ als natürliche exponentielle Familie
\begin{equation*}
    p_{\eta}(k) = \exp \left( \eta k - e^{\eta} - \log k! \right).
\end{equation*}
Dabei ist $d_0(\eta) = - e^{\eta}$. Nun erhalten wir 
\begin{align*}
    \E T(X) = \E X &= - d_0'(\eta) = e^{\eta} = \lambda \\
    \Var T(X) = \Var X &= -d_0''(\eta) = e^{\eta} = \lambda.
\end{align*}




\problem{Laplace-Verteilung ist keine exponentielle Familie.}
Für jedes $\theta\in\R$ sei eine Dichte
\begin{equation*} 
    p_\theta(x) = \frac{1}{2} \exp\left( -| x-\theta| \right), \quad x\in\R
\end{equation*}
gegeben. Sei $P_\theta$ das zur Dichte $p_\theta$ gehörige
Wahrscheinlichkeitsmaß.  Zeigen Sie, dass $\left\{ P_\theta : \theta\in\Theta
\right\}$ keine exponentielle Familie ist.

\solution
Angenommen $P_\theta$ gehört zu einer einparametrigen exponentiellen Famile.
Dann hat die Dichte $p_\theta$ eine Darstellung
\begin{equation*}
	p_\theta(x) = \exp\left( c(\theta) T(x) + d(\theta) \right)h(x).
\end{equation*}
Das ist äquivalent zu
\begin{align*}
	\frac{1}{2} \exp \left( -| x - \theta | \right) &= 
		\exp\left( c(\theta) T(x) + d(\theta) \right)h(x)
\end{align*}
und
\begin{align*}
    -|x-\theta| &= c(\theta) T(x) + d(\theta) + S(x)
\end{align*}
für alle $x,\theta\in\R$. Wir wählen $\theta_1, \theta_2\in\R$ und 
$\theta_1 \neq \theta_2$. Dann gilt
\begin{equation*}
	- |x-\theta_1| + |x-\theta_2| = 
	\left( c(\theta_1)-c(\theta_2) \right)T(x) + d(\theta_1)-d(\theta_2).
\end{equation*}
Daraus folgt $T(x)$ ist differenzierbar für alle $x\in\R\setminus \left\{
\theta_1,\theta_2 \right\}$. Eine andere Wahl von $\theta_1$ und $\theta_2$
liefert die Differenzierbarkeit von $T(x)$ auf ganz $\R$. Widerspruch.  



\problem{Verteilung der natürlichen suffizienten Statistik. }
Sei $X$ eine Zufallsvariable mit den Werten in $\left( \R^n, \mathcal B(\R^n)\right)$ und der Dichte
\begin{equation*}
	p(x,\theta) = \exp\left( c(\theta)\cdot T(x) + d(\theta) + S(x) \right) 1_{A}(x),
\end{equation*}
die einer einparametrigen exponentiellen Familie $\left\{ P_\theta :
\theta\in\Theta \right\}$ mit stetigem $T$ angehört. Zeigen Sie, dass die Dichte der natürlichen
suffizienten Statistik $T(X)$ die Form
\begin{equation*}
	q(t, \theta) = \exp\left( c(\theta)\cdot t + d(\theta) + S^*(t) \right) 1_{A^*}(t)
\end{equation*}
mit $A^* = \left\{ T(x) : x\in A \right\}$ hat.


\problem{Cauchy-Verteilung ist keine exponentielle Familie.}  Sei eine
Familie von Cauchy-Verteilungen $\left\{ P_\theta : \theta\in\R \right\}$ mit 
den zugehörigen Dichten
\begin{equation*}
    p_\theta(x) = \frac{1}{\pi\left[ 1 + \left( x - \theta \right)^2 \right]}
\end{equation*}
gegeben. Zeigen Sie, dass die Familie $\left\{ P_\theta : \theta\in\R \right\}$ keine 
exponentielle Familie ist.

\solution Nehmen wir an Cauchy-Vereilung ist eine expoentielle Familie. Dann 
gibt es eine Darstellung 
\begin{equation}
    p_\theta(x) = \frac{1}{\pi\left[ 1 + \left( x - \theta \right)^2 \right]} =
    \exp\left( c(\theta) T(x) + d(\theta) \right)h(x).
\end{equation}
TODO: Finish it! 

\problem{Mixtures of Normal Verteilungen sind keine exponentielle Familien.}
Sei eine Familie von Verteilungen $(P_\mu)_{\mu\in\R}$ durch 
\begin{equation*}
    P_\mu = \frac{1}{2} \left( \cN(\mu,1) +  \cN(\mu,2) \right) 
\end{equation*}
gegeben. Die Verteilungen $P_\mu$ sind Mischungen von Normalverteilungen.
Zeigen Sie, dass $(P_\mu)_{\mu}$ keine exponentielle Familie ist.


\problem{Weibull-Verteilung als exponentielle Familie.} 
Sei $X$ Weibull-verteilt mit der Dichte
\begin{equation*}
    p_{a,\lambda} = \frac{a}{\lambda} x^{a-1} e^{-\frac{x}{\lambda}} 1_{\R>0}(x)  
\end{equation*}
und den Parametern $a,\lambda>0$. Zeigen Sie folgende Aussagen.
\begin{enumerate}
    \item Ist $a>0$ bekannt und fix, so ist $(p_\lambda)_{\lambda>0}$ eine
        exponentielle Familie.
    \item Die Familie $(p_{a,\lambda})_{a,\lambda>0}$ ist keine exponentielle Familie. 
\end{enumerate} 


\section{Momentenmethode}

\problem{Stetige Gleichverteilung. Momentenschätzer. } Seien $X_1, \cdots, X_n$ 
i.i.d.\ stetig gleichverteilt auf dem Intervall $[0,\theta]$ mit $\theta>0$. 
Die Dichte von $X_i$ ist also 
\begin{equation*}
    p_{\theta}(x) = \frac{1}{\theta} 1_{[0,\theta]}(x).
\end{equation*}
\begin{enumerate}
    \item Berechnen Sie einen Momentenschätzer für $\theta$ basierend auf dem 
        ersten Moment von $X_i$. 
    \item Berechnen Sie einen Momentenschätzer für $\theta$ basierend auf dem 
        $k$-ten Moment von $X_i$. 
\end{enumerate}

\solution Wir berechnen zunächst alle Momente der stetigen Gleichverteilung. 
\begin{align*}
    \E X^k &= \int_{\R} x^k \frac{1}{\theta} 1_{[0,\theta]} (x) dx \\
    &= \frac{1}{\theta} \int_{0}^\theta x^k dx = \frac{\theta^k}{k+1}. 
\end{align*}
Indem wir $\hat m_{k} = \sum_{i=1}^{n} X_i^k$ setzen, erhalten wir
\begin{align*}
    \hat m_k &= \frac{\hat\theta^k}{k+1} \\
    \hat \theta &= \left( (k+1) \hat m_k \right)^{\frac{1}{k}}.
\end{align*}
Insbesondere ist der auf dem ersten Moment basierende Schätzer gegeben durch
\begin{equation*}
    \hat \theta = \frac{2}{n} \sum_{i=1}^{n} X_i.
\end{equation*}



\problem{Momentenschätzer. Beispiele.} Bestimmen Sie mittels der Momentenmethode
einen Momentenschätzer für $\theta$ bei den folgenden Verteilungen: 
\begin{enumerate}
	\item Die Gleichverteilung mit Dichte
		\begin{equation*}
			p_\theta(x) = \frac{1}{ 2\theta } 1_{(-\theta,\theta)}(x), \quad \theta>0
		\end{equation*}
		Der Schätzer ist $\hat \theta = \left( \bar X \right)^{-1}$.
	\item Die Gamma-Verteilung mit der Dichte 
		\begin{equation*}
			p_\theta(x) = \frac{\beta^\alpha}{ \Gamma(\beta)} x^{\alpha-1} e^{-\beta x}1_{\R>0}(x)
		\end{equation*}
\end{enumerate}

%\problem{Momentenschätzer für Beta-Verteilung. } Die Zufallsvariablen
%$X_1,\ldots,X_n$ seien i.i.d.\ Beta-Verteilt, d.h.\ $X_1 \sim \textrm{Beta}(a,b)$.
%Bestimmen Sie einen Momentenschätzer für $\theta=(a,b)^\top$.

\problem{Weibull-Verteilung. Momentenschätzer.} Seien 
$X_1,\ldots,X_n$ i.i.d.\ mit der Dichte 
\begin{equation*}
	p(x) = \sqrt{ \frac{2 \theta^3}{\pi} } x^2 \exp\left( - \frac{\theta}{2}x^2 \right) 1_{\R>0}(x)
\end{equation*}
wobei der Parameter $\theta>0$ unbekannt ist. Berechnen Sie den Momentenschätzer für $\theta$
basierend auf dem zweiten Moment. 

\problem{Momentenschätzung vs Suffizienz.}   Betrachten 
Sie die Verteilungsfamilie von zweiseitigen Exponentialverteilungen
gegeben durch die Dichte
\begin{equation*}
	p_\theta(x) = \frac{1}{2} \exp\left( -| x- \theta | \right), \quad \theta\in\R
\end{equation*}
\begin{enumerate}
    \item Zeigen Sie mit dem ersten Moment, dass $\bar X$ ein Momentenschätzer
        für $\theta$ ist.
    \item Weisen Sie nach, dass dieser nicht suffizient für $\theta$ ist.
\end{enumerate}

\solution Wir berechnen zunächst den Erwartungswert der zweiseitigen
Exponentialverteilung. Es gilt
\begin{align*}
    \E X &= \int_{\R}^{} x \frac{1}{2} \exp \left( -| x-\theta| \right) dx \\
    &= \frac{1}{2} \int_{\R}^{} (x+\theta) \exp ( -|x|) dx \\
    &= \frac{1}{2} \int_{\R} x \exp \left( -|x| \right) dx + 
        \frac{\theta}{2} \int_{\R} \exp \left( -|x| \right) dx \\
        &= \theta \int_{0}^{\infty} \exp \left( -x \right) dx = \theta.
\end{align*}
Daher erhalten wir $\hat \theta = \bar X$. 

Nun werden wir zeigen, dass $\bar X$ nicht suffizient für $\theta$ ist. Wäre $\bar X$ 
suffizient für $\theta$, so würde nach dem Faktorisierungssatz die Darstellbarkeit
\begin{equation*}
    p_{\theta}(x_1, \cdots, x_n) = g \left( \frac{1}{n} \sum_{i}^{} x_i, \theta \right) h(x_1, \cdots, x_n)
\end{equation*}
folgen. Die Funktion 
\begin{equation*}
    p_\theta(x_1, \cdots, x_n) = \frac{1}{2^n} \exp \left( - \sum_{i=1}^{n} |x_i - \theta | \right)
\end{equation*}
ist positiv, so können wir den Quotienten 
\begin{equation*}
    \frac{p_{\theta_1}(x_1, \cdots, x_n)}{p_{\theta_2}(x_1, \cdots, x_n)} = 
    \exp \left( - \sum_{i=1}^{n} | x_i - \theta_1 | + \sum_{i=1}^{n} |x_i - \theta_2 | \right) 
    = \frac{g(\bar x,\theta_1) }{ g(\bar x, \theta_2)}. 
\end{equation*}
für $\theta_1 \neq \theta_2$ bilden und diesen logarithmieren:
\begin{equation*}
    - \sum_{i=1}^{n} | x_i - \theta_1 | + \sum_{i=1}^{n} |x_i - \theta_2 | 
    = \log \left( \frac{g(\bar x, \theta_1)}{g(\bar x, \theta_2)} \right) = G(\bar x, \theta_1, \theta_2). 
\end{equation*}
Sei nun $n=2$, $x_1 + x_2 = 0$ und $\theta_1=0, \theta_2 = 1$. Dann gilt $x_2=-x_1$ und 
$G(\bar x, \theta_1, \theta_2)= G(0, 0, 1)$ ist konstant aber 
\begin{align*}
    - \sum_{i=1}^{n} | x_i - \theta_1 | + \sum_{i=1}^{n} |x_i - \theta_2 | 
    &= -|x_1| - |x_1| + |x_1-1| + |-x_1-1| \\
    &= -2 |x_1| + |x_1-1| + |x_1+1|
\end{align*}
ist keine konstante Funktion von $x_1$. Widerspruch.


\problem{AR1. }  Die Zufallsvariablen $Z_1,\ldots, Z_n$ seien i.i.d. 
mit $Z_1 \sim \mathcal N(0,\sigma^2)$. Die Zeitreihe $\left( X_i \right)_{1\leq i\leq n}$
heißt \emph{autoregressiv der Ordnung 1} oder \textsc{ar(1)}, falls mit $X_0:= \mu$
und für $1\leq i \leq n$ 
\begin{equation*}
	X_i = \mu + \beta(X_{i-1} - \mu) + Z_i.
\end{equation*}
\begin{enumerate}
	\item Verwenden Sie $\E X_i$, um einen Momentenschätzer für $\mu$ zu finden. 
	\item Nun seien $\mu= \mu_0$ und $\beta=\beta_0$ fix und bekannt und weiterhin
		\begin{equation*}
			U_i = \frac{X_i - \mu_0}{ \sqrt{ \sum_{j=0}^{i-1} \beta^{2j}_0 }}.
		\end{equation*}
		Verwenden Sie $\E (U_i^2)$, um einen Momentenschätzer für $\sigma^2$ zu finden.
\end{enumerate}

\problem{Exponentialverteilung. Momentenschätzer.} 
Die Zufallsvariablen $X_1, \cdots, X_n$ seien i.i.d.\ und exponentialverteilt mit 
Parametern $a\in \R$, $\theta>0$ und der Dichte
\begin{equation*}
    p_{a,\theta} =\frac{1}{\theta} \exp \left( - \frac{x-a}{\theta} \right) 1_{\R_{>a}}(x).
\end{equation*}
Zeigen Sie die folgenden Aussagen.
\begin{enumerate}
    \item Der Erwartungswert, die Varianz und der zweite Moment von $X_1$ sind gegeben durch
        \begin{align*}
            \E X &= a+\theta, & \Var X &= \theta^2, & \E X^2 &= \theta^2 + (a+\theta)^2. 
        \end{align*}
    \item Die momentenerzeugende Funktion von $X_1$ ist 
        \begin{equation*}
            \psi_{a,\theta}(u) = \frac{e^{au}}{1- \theta u}
        \end{equation*}
        für $u<\frac{1}{\theta}$.
    \item Berechnen Sie Momentenschätzer für $(a,\theta)$ basierend auf dem ersten und zweiten 
        Moment von $X_1$. 
\end{enumerate}


\problem{Rayleigh-Verteilung. Momentenschätzer. } Seien $X_1, \cdots, X_n$ i.i.d.\ 
Rayleigh-verteilt mit der Dichte
\begin{equation*}
    p_\theta(x) = \frac{x}{\theta^2} \exp \left( - \frac{x^2}{2 \theta^2} \right) 1_{\R_{>0}}(x)
\end{equation*}
und dem unbekannten Parameter $\theta>0$. 
\begin{enumerate}
    \item Beweisen Sie die Formel
        \begin{equation*}
            \E X^{2n}_i = 2^{n} \theta^{2n} \Gamma(n+1)
        \end{equation*}
        für alle $n\in\bN$. 
        
    \item Finden Sie einen Momentenschätzer für $\theta$ basierend auf dem zweiten Moment von $X_i$.

\end{enumerate}


\section{Kleinste-Quadrate-Schätzer}


\problem{Einfache lineare Regression. Beispielrechnung.} Betrachten wir das
lineare Modell
\begin{equation*}
    Y_i = \theta_1 + \theta_2 x_i + \varepsilon_i, \quad i=1,\cdots, n.
\end{equation*}
Ausgehend von den Daten
\begin{align*}
    n &=10, & \sum_{}^{} y_i &= 8, & \sum_{}^{} x_i &= 40,  \\
    \sum_{}^{} y_i^2 &= 26, & \sum_{}^{} x_i^2 &= 200, 
    & \sum_{}^{} x_i y_i &= 20,
\end{align*}
berechnen Sie die Werte der Kleinste-Quadrate-Schätzer für die Parameter
$\theta_1$ und $\theta_2$. 

\solution Es ist die Funktion
\begin{equation*}
    Q(\theta,y) = \sum_{i=1}^{n} \left( y_i -\theta_1 -\theta_2 x_i \right)^2
\end{equation*}
zu maximieren. Wir stellen also die Normalgleichungen
\begin{align*}
    \frac{\partial}{\partial \theta_1} Q &= 
    \sum_{i=1}^{n} \left( y_i-\theta_1-\theta_2 x_i \right) (-1) =0 \\
    \frac{\partial}{\partial \theta_2} Q &= 
    \sum_{i=1}^{n} \left( y_i-\theta_1-\theta_2 x_i \right) (-x_i) =0\\
\end{align*}
auf und lösen diese. Wir sind nicht an der expliziten Darstellung der Funktion
$\hat \theta(y)$ interessiert, sondern an den Wert $\hat\theta$, der auf
unseren Daten basiert. 
\begin{align*}
    \sum y_i - n \theta_1 -\theta_2 \sum x_i &= 0 \\
    \sum x_i y_i -\theta_1 \sum x_i -\theta_2 \sum x_i^2 &=0 \\
    \theta_1 &= \frac{1}{n} \left( \sum y_i - \theta_2 \sum x_i \right) \\
    \theta_1 &= \frac{1}{10} \left( 8 - 40 \theta_2 \right) 
\end{align*}
Einsetzen in die Gleichung $\frac{\partial}{\partial \theta_2}Q=0$ liefert
\begin{align*}
    2 - 4\theta_1 - 20\theta_2 &=0 \\
    2 - 4\left( \frac{4}{5}-4\theta_2 \right)- 20 \theta_2 &=0 \\
    \theta_2 &= -\frac{3}{10} \\
    \theta_1 &= 2.
\end{align*}


\problem{Gewichtete einfache lineare Regression. }
Finden Sie eine Formel für den Kleinste-Quadrate -Schätzer $\hat \theta^w$
im Modell
\begin{equation*}
	Y_i = \theta_1 + \theta_2 x_i + \varepsilon_i
\end{equation*}
wobei $\varepsilon_1,\ldots, \varepsilon_n$ unabhängig seien mit $\varepsilon_i \sim \mathcal N(0, \sigma^2w_i)$.



\section{Maximum-likelihood Schätzer}

\problem{Mischung von Gleichverteilungen. MLE.}
Seien $X_1,\ldots,X_n$ i.i.d.\ mit Dichte $p_\theta$ 
und $\theta\in \left[ 0,1 \right]$. Zeigen Sie, dass der Maximum-Likelihood-Schätzer
für
\begin{equation*}
	p_\theta (x) = \theta 1_{\left( -1,0 \right)}(x) + \left( 1-\theta \right)1_{\left( 0,1 \right)}(x)
\end{equation*}
gerade $\hat \theta = \frac{1}{n} \sum_{i=1}^{n} 1_{(-1,0)}\left( X_i \right)$ ist.



\problem{Normalverteilung. MLE.}
Seien $X_1,\ldots,X_n$ i.i.d.\ und $X_i \sim \mathcal N\left( \mu, \sigma^2 \right)$ mit bekanntem $\mu$. 
Zeigen Sie, dass der Maximum-Likelihood-Schätzer von $\sigma$ gerade
\begin{equation*}
	\hat \sigma^2 (x) = \frac{1}{n} \sum_{i=1}^{n} (X_i - \mu)^2  
\end{equation*}
ist.

\solution Die Likelihood-Funktion dieses Models ist 
\begin{equation*}
    L_{\sigma}(x_1, \cdots, x_n) = \prod_{i=1}^{n} p_{\sigma}(x_i) = 
    \left( \frac{1}{\sqrt{2\pi}} \right)^n \, \frac{1}{\sigma^n} 
    \exp \left( - \frac{1}{2 \sigma^2} \sum_{i=1}^{n} \left( x_i - \mu \right)^2 \right).
\end{equation*}
Wir erhalten die Log-Likelihood-Funktion indem wir die Likelihood-Funktion 
logarithmieren. 
\begin{align*}
    l_\sigma (x_1, \cdots, x_n) &= \log \sqrt{2\pi}^{-n} + \log \sigma^{-n} 
    - \frac{1}{2 \sigma^2} \sum_{i=1}^{n} \left( x_i - \mu \right)^2   \\
    &= - n\log \sqrt{2 \pi} -n \log \sigma 
    - \frac{1}{2\sigma^2} \sum_{i=1}^{n} \left( x_i - \mu \right)^2.
\end{align*}
Differenzieren liefert
\begin{align*}
    \frac{\partial}{\partial \sigma} l_\sigma(x_1, \cdots, x_n) &= 
    - \frac{n}{\sigma} + \frac{1}{\sigma^3} \sum_{i=1}^{n} \left( x_i - \mu \right)^2 \\
    \frac{\partial^2}{\partial \sigma^2} l_\sigma(x_1, \cdots, x_n) &= 
    n \sigma^{-2} - 3 \sigma^{-4} \sum_{i=1}^{n} \left( x_i - \mu \right)^2. 
\end{align*}
Wir lösen die Gleichung
\begin{align*}
    \frac{\partial}{\partial \sigma} l_t (x_i ) &= 
    - \frac{n}{\sigma} + \frac{1}{\sigma^3} \sum_{i=1}^{n} \left( x_i - \mu \right)^2  = 0 \\
    \hat \sigma^2 &= \frac{1}{n} \sum_{i=1}^{n} \left( x_i - \mu \right)^2. 
\end{align*}
Wir setzen den gefunden Schätzer in den Ausdruck $\frac{\partial^2}{\partial \sigma^2} l_t$
ein und erhalten
\begin{align*}
    \left. \frac{\partial^2}{\partial \sigma^2} l_t(x_i) \right|_{\sigma=\hat\sigma} 
    &= \frac{n^2}{ \sum_{}^{} (x_i-\mu)^2} - \frac{3 n^2}{\sum_{}^{} (x_i -\mu)^2} < 0.
\end{align*}
Die letzte Ungleichung bedeutet, dass die Log-Likelihood-Funktion an der Stelle
$\hat\sigma$ konkav ist und die gefundene Lösung der Gleichung
$\frac{\partial}{\partial \sigma}l_t = 0$ tatsächlich das eindeutige Maximum
der Likelihood-Funktion ist.




\problem{Exponentialverteilung. MLE.} Seien $X_1,\ldots,X_n$
exponentialverteilt zum Parameter $\theta$. Zeigen Sie, dass $\hat \theta =
(\bar X)^{-1}$ sowohl der Maximum-Likelihood-Schätzer als auch ein
Momentenschätzer ist.


\problem{Maximum-Likelihood-Methode und Suffizienz.}
Sei $\left\{ p_\theta : \theta\in\Theta \right\}$ ein reguläres statistisches
Modell und $T\left( X \right)$ eine suffiziente Statistik für $\theta$. 
Weisen Sie nach, dass ein Maximum-Likelihood-Schätzer für $\theta$ 
eine Funktion von $T\left( X \right)$ ist.

\solution Sei $\hat\theta(x_1, \cdots, x_n)=\hat \theta(x_i)$ ein 
Maximum-Likelihood-Schätzer für $\theta$ in einem regulären Model $\left( p_\theta \right)_{\theta\in\Theta}$. Dann maximiert $\hat\theta$ die Likelihood-Funktion
\begin{equation*}
    L_\theta(x_i) = \prod_{i=1}^{n} p_{\theta}(x_i) = g\left( T(x_i), \theta \right) h(x_i)
\end{equation*}
für fixes $(x_1, \cdots, x_n)$ über alle $\theta\in\Theta$. Nachdem die
Likelihood-Funktion $L_\theta$ positiv ist und die Funktion $h$ positiv gewählt
werden kann, können wir anstelle von $L_\theta$ die Funktion $\tilde L_\theta =
g(T(x_i), \theta)$ betrachten. $\tilde L_\theta$ hängt von $(x_1,\cdots,x_n)$
nur über $T(x)$, so können wir $\hat\theta$ als eine Funktion ansehen, die
für ein fixes $T(x_1, \cdots, x_n)$ ein $\theta\in\Theta$ liefert, das die
Funktion $L_\theta(T)=g(T,\theta)$ maximiert. $\hat \theta$ ist also eine
Funktion von $T=T(x_1,\cdots,x_n)$.




\problem{Zweidimensionale Exponentialverteilung. MLE.} Betrachtet werden 
i.i.d.\ Zufallsvariablen $(Y_1, Z_1), \ldots,(Y_n,Z_n)$. Weiterhin seien
$Y_1$ und $Z_1$ unabhängig und exponentialverteilt mit Parametern
$\lambda > 0$ bzw.\ $\mu > 0$. Bestimmen Sie den Maximum-Likelihood-Schätzer
für $(\lambda,\mu)$.

\solution    Nachdem $Y_i$ und $Z_i$ unabhängig sind, genügt es den 
MLE für einen der beiden Zufallsvektoren zu bestimmen. Wir berechnen also den MLE für eine
eindimensionale Exponentialverteilung mit Parameter $\theta$.
Die Likelihood-Funktion ist gegeben durch
\begin{eqnarray}
	L(x, \theta) &=&  \prod_{i=1}^n \theta \exp\left(  -\theta x_i \right) \\
	&=&  \exp \left(  -\theta \sum_{}^{} x_i + \log \theta^n \right)
\end{eqnarray}
Die Log-Likelihood-Funktion ist dann gegeben durch
\begin{eqnarray}
	l(\theta, x) &=& n \log \theta - \theta \sum_{i}^{} x_i \\
	\frac{\partial}{\partial \theta} l(\theta,x) &=&  \frac{n}{\theta } - \sum_{i}^{} x_i \\
	\frac{\partial^2}{\partial \theta^2} l(\theta,x) &=& - \frac{n}{\theta^2} < 0
\end{eqnarray}
Wir setzen $\frac{\partial}{\partial \theta} l(\theta,x) = 0$ und erhalten den MLE
\begin{equation}
	\hat \theta = \frac{1}{\frac{1}{n} \sum_{i}^{} x_i} = \frac{1}{\bar x}.
\end{equation}
Somit ist der MLE für $\left( \lambda, \mu \right) = \left( 1/\bar y, 1/\bar z \right)^\top$.


\problem{Weibull-Verteilung. MLE.} Seien $X_1,\ldots,X_n$ i.i.d.
mit der Dichte 
\begin{equation*}
	p_\theta(x)=
	\sqrt{\frac{2\theta^3}{\pi}} x^2 e^{ - \frac{\theta}{2}x^2} 1_{\R>0}(x)
\end{equation*}
wobei der Parameter $\theta>0$ unbekannt ist. Finden Sie
den Maximum-Likelihood-Schätzer für $ \theta$ und klären Sie, ob dieser
eindeutig ist.

\solution Wir berechnen den \textsc{mle} für die oben angegebene Familie von Weibull-Verteilungen
indem wir die Log-Likelihood-Funktion maximieren. 

Die Likelihood-Funktion der Familie ist gegeben durch
\begin{align*}
	L(\theta, x) &= \left( \frac{2\theta^3}{\pi} \right)^\frac{n}{2} 
	\prod_i x_i^2 \exp\left( - \frac{\theta}{2}x_i^2 \right).
\end{align*}
Für die Log-Likelihood-Funktion gilt
\begin{align*}
	l(\theta,x) &= \frac{n}{2}\left( \log 2 + 3 \log \theta - \log \pi \right) + 2\sum_{i}^{} \log x_i - \frac{\theta}{2}\sum_{i}^{} x_i^2 \\
	\frac{\partial}{\partial \theta} l(\theta,x) &= 
	\frac{3n}{2 \theta}  - \frac{1}{2}\sum_{i}^{} x_i^2 \\
	\frac{\partial^2}{\partial \theta^2} l(\theta,x) &= - \frac{3n}{2} \frac{1}{\theta^2} < 0 \quad \forall \theta\in\R_{>0}.
\end{align*}
Somit ergibt sich die Formel für \textsc{mle} 
\begin{align*}
	\frac{\partial}{\partial \theta} l(\theta,x) &= \frac{3n}{2 \theta}  - \frac{1}{2}\sum_{i}^{} x_i^2 = 0 \\
	\hat\theta &=  \frac{3}{\frac{1}{n} \sum_{i}^{} x_i^2}.
\end{align*}
Wegen $ \frac{\partial^2}{\partial \theta^2} l(\theta,x) <0$ ist der \textsc{mle} $\hat\theta$ auch eindeutig.





\problem{Mischung der Verteilungen. MLE.}
Seien $p_1$ und $p_2$ zwei Dichten. Für jedes $\theta\in\left[ 0,1 \right]$ ist dann die Mischung
der beiden Verteilungen durch die Dichte
\begin{equation*}
	p_\theta(x) = \theta p_1(x) + (1-\theta)p_2(x)
\end{equation*}
gegeben. 
\begin{enumerate}
    \item Betrachten Sie das parametrische Modell $\left\{ p_\theta : \theta
        \in \left[ 0,1 \right] \right\}$ und bestimmen Sie eine notwendige und
        hinreichende Bedingung dafür, dass die Likelihood-Gleichung eine Lösung
        besitzt.
    \item Weisen Sie nach, dass diese Lösung, falls sie existiert, der
        eindeutige Maximum-Likelihood-Schätzer für $\theta$ ist.
    \item Was ist der Maximum-Likelihood-Schätzer, wenn die
        Likelihood-Gleichung keine Lösung besitzt?
\end{enumerate}

\solution 
Betrachten wir zunächst die Log-Likelihood-Funktion und ihre Ableitungen
\begin{eqnarray}
	L(\theta, x) &=&  \prod_i \left( \theta p_1(x_i) + \left( 1-\theta \right)p_2(x_i) \right) \\
	l(\theta, x) &=& \sum_{i}^{} \log \left( \theta p_1(x_i) + \left( 1-\theta \right) p_2(x_i) \right) \\
	\frac{\partial}{\partial \theta} l(\theta,x) &=&  \sum_{i}^{} \frac{p_1(x_i)-p_2(x_i)}{\theta p_1(x_i) +\left( 1-\theta \right)p_2(x_i)} \\
	\frac{\partial^2}{\partial \theta^2} l(\theta,x) &=& \sum_{i}^{} \frac{ -\left\{ p_1(x_i)-p_2(x_i) \right\}^2 }{ \left\{ \theta p_1(x_i) +\left( 1-\theta \right)p_2(x_i) \right\}^2 }
\end{eqnarray}

Betrachten wir die Frage der Existenz des MLE. Im Falle $p_1(x_i)=p_2(x_i)$ für
einen Vektor $\left( x_1,\cdots,x_n \right)$, hängt die Likelihood-Funktion
$L\left( \theta ,x \right)$ nicht von $\theta$ ab und somit kann
$\hat\theta(x)$ beliebig gesetzt werden. $\hat\theta$ ist in diesem Fall nicht
eindeutig.

Wenn es einen Vektor $\left( x_1,\cdots,x_n \right)$ gibt, der uns erlaubt $p_1$ und $p_2$ zu unterscheiden,
können wir die Bedingungen für die Existenz und Eindeutigkeit des MLE wie folgt konstruieren. 
Die Gleichung
\begin{eqnarray}
	\frac{\partial}{\partial \theta} l(\theta,x) &=& 0
\end{eqnarray}
hat genau dann genau  eine Lösung wenn $ \frac{\partial}{\partial \theta} l(\theta,x)|_{\theta=0}\geq 0$ und
$ \frac{\partial}{\partial \theta} l(\theta,x)|_{\theta=1}\leq 0$ gilt, da die zweite Ableitung
der Log-Likelihood-Funktion stickt negativ ist und somit ist $ \frac{\partial}{\partial \theta} l(\theta,x)$ 
strikt monoton fallend in $\theta$.
Im Falle
\begin{equation}
	 \frac{\partial}{\partial \theta} l(\theta,x) > 0 \quad \forall \theta\in\left[ 0,1 \right]
\end{equation}
bzw.
\begin{equation}
	 \frac{\partial}{\partial \theta} l(\theta,x) < 0 \quad \forall \theta\in\left[ 0,1 \right]
\end{equation}
hat die Maximum-Likelihood-Gleichung keine Lösung und die
die Maxima werden jeweils am Rand angenommen, d.h.\ $\hat\theta=1$ bzw. $\hat\theta=0$ und 
der \textsc{mle} $\hat\theta(x)$ kann für $x$ eindeutig festgelegt werden.




\section{UMVUE}

\problem{Ein Model für das kein erwartungstreuer Schätzer existiert.}
Seien $X_1,\ldots,X_n$ i.i.d.\ bernoulliverteilt mit dem Parameter $p \in
(0,1)$. Zeigen Sie, dass es keinen erwartungstreuen Schätzer für $q(p) =
\frac{p}{1-p}$ gibt. 

\solution Sei $T(X)$ ein Schätzer. Nachdem es $2^n$ verschiedene
Werte gibt, die der Zufallsvektor $X = (X_1, \cdots, X_n)$ annehmen kann, können 
wir den Erwartungswert von $T$ explizit berechnen und erhalten
\begin{equation*}
    \E \, T(X) = \sum_{i = 1}^{2^n} T(i) p^{\nu(i)}(1-p)^{n - \nu(i)}.
\end{equation*}
Dabei ist $\nu(i)$ eine Funktion, die die Anzahl der Einser in der
Binärdarstellung von $i\in \bN$ angibt. $T(X)$ ist erwartungstreu, wenn 
\begin{equation*}
    \E \, T(X) = \frac{p}{1-p}
\end{equation*}
gilt. Nachdem aber $\E\, T(X)$ ein Polynom in $p$ ist und $\frac{p}{1-p}$ nicht
als Polynom darstellbar ist, kann es keinen erwartungstreuen Schätzer für
$q(p)$ geben. 


\problem{Verschobene Gleichverteilung. } 
Seien $X_1,\ldots,X_n$ i.i.d.\ mit $X_1\sim U(\theta,\theta+1)$. Der Parameter
$\theta$ sei  unbekannt und $X_{(1)}=\min\left\{ X_1,\ldots,X_n \right\}$ die
kleinste Ordnungsgröße der Daten und $\bar X= \frac{1}{n}\sum_{i=1}^{n}X_i$.
Betrachten Sie die beiden Schätzer 
\begin{align*}
	T_1(X) &= \bar X - \frac{1}{2} \\
	T_2(X) &=  X_{(1)} - \frac{1}{n+1}.
\end{align*}
Zeigen Sie, dass beiden Schätzer erwartungtreu sind. Berechnen Sie 
die Varianz der beiden Schätzer.

\solution Wir berechnen jeweils Erwartungswert und Varianz der beiden Schätzer. 
Zunächst ist 
\begin{eqnarray}
	E X_i &=&  \int_{\R}^{} x p_\theta(x) dx = \int_{\theta}^{\theta+1} x dx = \theta + \frac{1}{2}. \\
	E T_1(X) &=&  \frac{1}{n} \sum_{i}^{} X_i - \frac{1}{2} = \frac{1}{n} \sum_{i}^{} \left( \theta + \frac{1}{2} \right) -\frac{1}{2} = \theta.
\end{eqnarray}
Das heißt $T_1$ ist erwartungstreu. Die Varianz von $T_1$ berechnen wir folgendermassen
\begin{eqnarray}
	\Var T_1 &=& \Var \left[ \frac{1}{n} \sum_{i}^{} x_i - \frac{1}{2} \right] \\
	&=& E\left[ \left(  \frac{1}{n} \sum_{i}^{} X_i - \frac{1}{2} \right)^2  \right] - \left( E T_1 \right)^2 \\
	&=& E \left[  \left( \frac{1}{n} \sum_{i}^{} X_i \right)^2  - \frac{1}{n}\sum_{i}^{} X_i +\frac{1}{4} \right]-\theta^2 \\
	&=& E \left[ \left( \frac{1}{n} \sum_{i}^{} X_i \right)^2 \right] - \left( \theta + \frac{1}{2} \right) + \frac{1}{4}-\theta^2.
\end{eqnarray}
Nun ist 
\begin{eqnarray}
	E \left[ \left( \frac{1}{n} \sum_{i}^{} X_i \right)^2 \right] &=& \frac{1}{n^2} \sum_{ij}^{} E X_i X_j \\
	E X_i X_j &=& \int_{[\theta,\theta+1]^2}^{} xy dx dy = \left( \int_{\theta}^{\theta+1} t dt \right)^2 = \left( \theta+\frac{1}{2} \right)^2 \\
	E X_i^2 &=& \int_{\theta}^{\theta+1} x^2 dx = \frac{1}{3}\left( \left( \theta+1 \right)^3 - \theta^3 \right )\\
	&=& \theta^2 + \theta + \frac{1}{3}.
\end{eqnarray}
Also gilt
\begin{eqnarray}
	\Var T_1 &=& \left( 1-\frac{1}{n} \right)\left( \theta+\frac{1}{2} \right)^2+\frac{1}{n}\left( \theta^2+\theta+\frac{1}{3} \right)     - \left( \theta+\frac{1}{2} \right)^2 \\
	&=& \frac{1}{12 n} = \mathcal O \left( n^{-1} \right).
\end{eqnarray}

Nun kommen wir zu $T_2 = X_{(1)}-\frac{1}{n+1}$. Die Berechnung des Erwartungswertes führt auf
\begin{eqnarray}
	E T_2(X) &=&  -\frac{1}{n+1} + E X_{(1)} \\
	E X_{(1)} &=& \int_{[\theta,\theta+1]^n}^{} \min\left\{ x_1,\cdots,x_n \right\} dx_1\cdots d x_n \\
	&=& \int_{[0,1]^n}^{} \min\left\{ x_1+\theta,\cdots,x_n+\theta \right\} dx_1\cdots d x_n \\
	&=& \theta + \int_{[0,1]^n}^{} \min\left\{ x_1,\cdots,x_n \right\} dx_1\cdots d x_n \\
	&=& \theta +\sum_{\sigma\in S_n}^{} \int_{0\leq x_{\sigma(1)}\leq \ldots \leq x_{\sigma(n)}\leq 1} x_{\sigma(1)} dx_1 \cdots dx_n 
	\label{}
\end{eqnarray}
wobei die Summe über alle $n!$ Permutationen der $n$ Elemente geführt wird.
Es gilt
\begin{equation}
	\int_{0\leq x_1\leq \ldots\leq x_n\leq 1} x_1 dx_1 \cdots d x_n = 
	\int_{0}^{1}\cdots \int_{0}^{x_2} x_1 dx_1 \cdots dx_n = 
	\frac{1}{\left( n+1 \right)!}
\end{equation}
und
\begin{eqnarray}
	\sum_{\sigma\in S_n}^{} \int_{0\leq x_{\sigma(1)}\leq \ldots \leq x_{\sigma(n)}\leq 1} x_{\sigma(1)} dx_1 \cdots dx_n 
	=\frac{1}{n+1}.
\end{eqnarray}
Wir erhalten also
\begin{eqnarray}
	E X_{(1)} &=& \frac{1}{n+1} + \theta
\end{eqnarray}
und 
\begin{eqnarray}
	E T_2 = -\frac{1}{n+1} + E X_{(1)} = \theta.
\end{eqnarray}
Für die Berechnung der Varianz können wir folgendermassen vorgehen. 
\begin{eqnarray}
	\Var T_2 &=& E \left( T_2^2 \right) - \left( E T_2 \right)^2 \\
	E T_2^2 &=& E \left[ \left( X_{(1) - \frac{1}{n+1}} \right)^2 \right] = \\
	&=& E \left[ X_{(1)}^2 - \frac{2}{n+1}X_{(1)}+ \left( \frac{1}{n+1} \right)^2 \right] \\
	E\left[  X_{(1)}^2 \right] &=&  E \left[ \min\left\{ X_1,\cdots,X_n \right\}^2 \right] \\
	&=& \int_{[\theta,\theta+1]^n}^{} \min\left\{ x_1,\cdots,x_n \right\}^2 d \lambda^n \\
	&=& \int_{[0,1]^n}^{} \min\left\{ x_1+ \theta,\cdots,x_n+ \theta \right\}^2 d \lambda^n \\
	&=& \int_{[0,1]^n}^{} \theta^2 +2\theta\min\left\{ x_1,\cdots,x_n \right\}+ \min\left\{ x_1,\cdots,x_n \right\}^2 d\lambda^n\\
\end{eqnarray}
Mit der Hilfsrechnung
\begin{eqnarray}
	\int_{[0,1]^n}^{} \min\left\{ x_1,\cdots,x_n \right\}^2 d\lambda^n 
	&=& \sum_{\sigma\in S_n}^{} \int_{0\leq x_{\sigma(1)}\leq \ldots \leq x_{\sigma(n)}\leq 1} x_{\sigma(1)}^2 d\lambda^n \\
	&=& n! \frac{1}{\left( n+2 \right)\cdots 3} = \frac{2}{\left( n+1 \right)\left( n+2 \right)}
\end{eqnarray}
erhalten wir
\begin{eqnarray}
	E\left[ X_{(1)}^2 \right] &=& \theta^2 +  \frac{2\theta}{n+1} + \frac{2}{\left( n+1 \right)\left( n+2 \right)}
\end{eqnarray}
und
\begin{eqnarray}
	\Var T_2 &=& E \left[ X_{(1)}^2 - \frac{2}{n+1}X_{(1)}+ \left( \frac{1}{n+1} \right)^2 \right] -\theta^2 \\
	&=& \theta^2 + \frac{2\theta}{n+1} + \frac{2}{\left( n+1 \right)\left( n+2 \right)} 
	- \frac{2}{n+1}\left( \theta + \frac{1}{n+1} \right) + \left( \frac{1}{n+1} \right)^2 -\theta^2 \\
	&=& \frac{n}{\left( n+1 \right)^2 \left( n+2 \right)} = \mathcal O (n^{-2}).
\end{eqnarray}
Die Varianz von $T_1$ ist also um eine Größenordung höher als die von $T_2$.




\problem{UMVUE. Bernoulli-Verteilung. } 
Seien $X_1,\ldots,X_n$ i.i.d.\ und $X_1$ Bernoulli$(\theta)$-verteilt. Zeigen
Sie, dass der MLS $\bar X$ ein \textsc{umvue}-Schätzer ist.

\solution Betrachten wir den Schätzer $\bar X$. Aus bereits
gelösten Übungsblättern wissen wir, dass $\bar X$ unverzerrt für $\theta$ ist.
Wir zeigen, dass die Statistik $\sum_{i}^{} X$ suffizient und vollständig ist
und die Behauptung folgt mit dem Satz von Lehmann-Scheff\'e. 

Dazu sei $g$ eine messbare reellwertige Funktion mit
\begin{equation*}
	E_\theta \left[ g\left( \sum_{i}^{} X_i \right) \right] = 0 
\end{equation*}
für alle $\theta\in \left[ 0,1 \right]$. Das ist äquivalent zu
\begin{equation*}
	\sum_{k=0}^{n} g\left( k \right) P\left( \sum_{i}^{}X_i = k \right) = 0 \quad \forall\theta.
\end{equation*}
Mit Hilfe der Identität
$P\left( \sum_{i}^{} X_i = k \right)= \binom{n}{k}\theta^k \left( 1-\theta \right)^{n-k}$
folgern wir, dass die Funktion 
\begin{equation*}
	f_n(\theta ) = \sum_{k=0}^{n} g\left( k \right) \binom{n}{k}\theta^k \left( 1-\theta \right)^{n-k}
\end{equation*}
ein Polynom vom Grad höchstens $n$ in $\theta$ ist, und höchstens $n$
verschiedene Nullstellen $\theta\in \left[ 0,1 \right]$ haben kann. Als
Normalform dieses Polynoms erhalten wir
\begin{align*}
	f_n(\theta) 
	&= \sum_{k=0}^{n} g\left( k \right) \binom{n}{k}\theta^k \left( 1-\theta \right)^{n-k} \\ 
	&= \sum_{k=0}^{n} g\left( k \right) \binom{n}{k}\theta^k \sum_{i=0}^{n-k}\binom{n-k}{i}(-1)^{n-k-i}\theta^{n-k-i} \\
\end{align*}
und mit $\binom{n-k}{i}=\binom{n-k}{n-k-i}$ schließlich
\begin{align*}
	f_n(\theta) 
	&= \sum_{k=0}^{n} g\left( k \right) \binom{n}{k}\theta^k \sum_{i=0}^{n-k}\binom{n-k}{n-k-i}(-1)^{n-k-i}\theta^{n-k-i} \\
	&= \sum_{k=0}^{n} g\left( k \right) \binom{n}{k}\theta^k \sum_{i=0}^{n-k}\binom{n-k}{i}(-1)^i\theta^i \\
	&= \sum_{k=0}^{n} \sum_{i=0}^{n-k}\binom{n-k}{i} \binom{n}{k} (-1)^i  g\left( k \right)  \theta^{i+k} \\
	&= \sum_{p=0}^{n} \left[ \sum_{k=0}^{p} \binom{n-k}{p-k}\binom{n}{k}(-1)^{p-k} g(k) \right] \theta^p.
\end{align*}
Durchs Betrachten der Koeffizienten dieses Polynoms in aufsteigender Reihenfolge erhalten wir
induktiv $g(0)=0$, $g(1)=0,\ldots$ und nach $n$ Schritten $g(n)=0$.
Damit ist $g \equiv 0$ und die Statistik $\sum_{i}^{} X_i$
vollständig. Aus den bereits gelösten Übungsblättern wissen wir, dass $\sum_{i}^{} X_i$
suffizient für $\theta$ ist. Mit
\begin{equation*}
	E\left[ \bar X | \sum_{i}^{} X_i \right] = \bar X
\end{equation*}
und dem Satz von Lehmann-Scheff\'e folgt die Behauptung.






\problem{Vollständigkeit und UMVUE. }  Seien $X_1,\ldots,X_n$ i.i.d.,
wobei $X_1$ eine diskrete Zufallsvariable mit Wahrscheinlichkeitsfunktion
\begin{equation*}
	p_\theta(x)=P_\theta(X_1=x)= 
	\left( \frac{\theta}{2} \right)^{|x|} (1-\theta)^{1-|x|},\  
	x\in\left\{ -1,0,1 \right\},
\end{equation*}
und unbekanntem Parameter $\theta\in (0,1)$ sei. Untersuchen Sie die beiden
Schätzer $T_1(X)=X_1$ und $T_2(X)=|X_1|$ auf Vollständigkeit. Bestimmen Sie
einen \textsc{umvue}-Schätzer für $\theta$.

\solution 
Wir untersuchen zuerst die Vollständigkeit der beiden Schätzer $T_1$ und $T_2$. 
Die Dichte von $X_1,\ldots,X_n$ ist
\begin{eqnarray}
	p_\theta(x_1, \cdots, x_n) &=& \prod_i \left( \frac{\theta}{2} \right)^{|x_i|} \left( 1-\theta \right)^{1-|x_i|} \\
	&=& \left( \frac{\theta}{2} \right)^{\sum_{i}^{} |x_i|} \left( 1-\theta \right)^{n-\sum_{i}^{} |x_i|}.
\end{eqnarray}
Nehmen wir an, dass für alle $\theta$
\begin{equation}
	E_\theta g\left( T_1\left( X \right) \right) = 0
\end{equation}
gilt. Das ist äquivalent zu 
\begin{eqnarray}
	E_\theta g\left( X_1 \right) &=&  \sum_{x_i\in\left\{ -1,0,1 \right\}}^{} g(x_1) p_\theta \left( x_1,\cdots,x_n \right) \\
	&=& \sum_{x_i\in\left\{ -1,0,1 \right\}}^{} g(x_1) \left( \frac{\theta}{2} \right)^{\sum_{i}^{} |x_i|} \left( 1-\theta \right)^{n-\sum_{i}^{} |x_i|} \\
	&=& \sum_{x_1\in\left\{ -1,0,1 \right\}}^{} g(x_1) \left[  \sum_{x_{2,\ldots,n}\in\left\{ -1,0,1 \right\}}^{}\left( \frac{\theta}{2} \right)^{\sum_{i}^{} |x_i|}\left( 1-\theta \right)^{n-\sum_{i}^{} |x_i|}   \right].
\end{eqnarray}
Sei nun
\begin{eqnarray}
	A(x_1, \theta) &=& \sum_{x_{2,\ldots,n}\in\left\{ -1,0,1 \right\}}^{}\left( \frac{\theta}{2} \right)^{\sum_{i}^{} |x_i|}\left( 1-\theta \right)^{n-\sum_{i}^{} |x_i|}.
\end{eqnarray}
Es ist also 
\begin{equation}
	E_\theta g\left( T_1\left( X \right) \right) = \sum_{x_i\in\left\{ -1,0,1 \right\}}^{} g(x_1) A\left( x_1, \theta \right).
\end{equation}
Nachdem aber $A\left( x_1,\theta \right) = A(-x_1,\theta)$ wählen wir $g\left( x \right)=x$ und erhalten
\begin{equation}
	E_\theta g\left( T_1\left( X \right) \right) = 0 \quad \forall \theta\in\Theta
\end{equation}
obwohl $g\nequiv 0$. $T_1(X)=X_1$ ist demzufolge nicht vollständig. 

Für $T_2\left( X \right)$ führen wir gleiche Rechnung durch und erhalten
\begin{eqnarray}
	E_\theta(g\left( T_2\left( X \right) \right)) &=& \sum_{x_i\in\left\{ -1,0,1 \right\}}^{} g(|x_1|) A\left( x_1, \theta \right) \\
	&=& 2 g(1) A(1,\theta) + g(0) A(0,\theta) = 0 \quad \forall\theta\in (0,1).
\end{eqnarray}
$A(1,\theta)$ und $A(0,\theta)$ sind Polynome vom Grad $n$. $A(0,\theta)$ hat aber im Gegensatz zu $A(1,\theta)$ 
einen nicht verschwindenden konstanten Koeffizienten mit Wert $1$. Dieser Koeffizient kommt vom Summanden $(x_2,\ldots,x_n)=0$
in der Darstellung
\begin{equation}
	A(0, \theta) = \sum_{x_{2,\ldots,n}\in\left\{ -1,0,1 \right\}}^{}\left( \frac{\theta}{2} \right)^{\sum_{i}^{} |x_i|}\left( 1-\theta \right)^{n-\sum_{i}^{} |x_i|}.
\end{equation}
Daher ist $g\equiv 0$ und die Statistik $T_2(X)$ vollständig. 

Für die Berechnung eines \textsc{umvue}-Schätzers sind die obigen Berechnungen nur indirekt 
hilfreich, da sowohl $T_1(X)$ als auch $T_2(X)$ nicht suffizient für $\theta$ sind. 

Für das Auffinden eines \textsc{umvue}-Schätzers benötigen wir einiger Beobachtungen. 
Zunächst ist die Zufallsvariable $|X_i|$ Bernoulli verteilt und daher gilt $E_\theta |X_i|=\theta$.
$T_3(X)= \frac{1}{n} \sum_{i=1}^{n} |X_i|$ ist also ein unverzerrter Schätzer für $\theta$ und
binomialverteilt. 
$T_3$ ist auch suffizient für $\theta$, denn es gilt
\begin{eqnarray}
	P( X=\left( x_1,\ldots,x_n \right)| T_3=y) &=& \frac{ 1_{ \left\{ \sum_{i}^{} |x_i|=y \right\} } \left( \frac{\theta}{2} \right)^{\sum_{i}^{} |x_i|}  \left( 1-\theta \right)^{n-\sum_{i}^{} |x_i| }   }{ \binom{n}{k} \theta^{\sum_{i}^{} |x_i|   } (1-\theta)^{n-\sum_{i}^{} |x_i|}        } \\
	&=& 1_{ \left\{ \sum_{i}^{} |x_i|=y \right\} } \binom{n}{k}^{-1} \left( \frac{1}{2} \right)^{\sum_{i}^{} |x_i|}.
\end{eqnarray}
Daher ist $T_3$ ein \textsc{umvue}-Schätzer für $\theta$.





\problem{Stichprobenvarianz als UMVUE bei Normalverteilung.} 
Seien $X_1,\ldots,X_n$ i.i.d.\ mit $X_i \sim \mathcal N(\mu,\sigma^2)$ und
$\mu\in\R$ sowie $\sigma>0$.  Dann ist die Stichprobenvarianz 
\begin{equation*}
	s^2(X) = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar X)^2
\end{equation*}
ein \textsc{umvue}-Schätzer für $\sigma$, falls $\mu$ unbekannt ist. 
Ist $\mu$ hingegen bekannt, so ist $s^2(X)$ kein \textsc{umvue}-Schätzer
von $\sigma$. 

\solution 
Die erste Aussage wurde bereits in der Vorlesung behandelt. 

Sei also $\mu=\mu_0$ bekannt und fix.  Wir wollen die Normalverteilung mit
bekanntem Erwartungswert als exponentielle Familie schreiben und mit Hilfe
eines Satzes aus der Vorlesung eine vollständige und suffiziente Statistik
gewinnen. Es gilt
\begin{eqnarray}
	p_\sigma(x_i) &=& \prod_i \frac{1}{\sqrt{2 \pi \sigma^2}} 
	\exp\left( -\frac{1}{2} \left( \frac{x_i - \mu_0}{\sigma}\right)^2 \right) \\
	&=& \exp\left( -\frac{1}{2 \sigma^2} \sum_{i}^{} \left( x_i - \mu_0 \right)^2 +
		\log\left( 2 \pi \sigma^2 \right)^{-\frac{n}{2}}  \right).
\end{eqnarray}
Damit erhalten wir $c(\sigma^2) = -\frac{1}{2 \sigma^2}$ und $T(X) = \sum_{i}^{} \left( X_i - \mu_0 \right)^2$.
Da $c\left( \Theta \right) = c(\R_{>0})= \R_{<0}$ ein offenes Rechteck enthält, ist $T\left( X \right)$
eine vollständige und suffiziente Statistik für $\sigma^2$.

Da die Stichprobenvarianz
\begin{equation}
	\hat\sigma^2(X) = \frac{1}{n} \sum_{i=1}^{n} (X_i - \mu_0)
\end{equation}
bei festgehaltenem $\mu_0$ ein unverzerrter Schätzer für $\sigma^2$ ist, können 
wir den Satz von Lehmann-Scheff\'e anwenden und berechnen 
\begin{equation}
	T^*\left( X \right) = E\left[ \sum_{i=1}^{n} \frac{1}{n} \left( X_i-\mu_0 \right)^2 | \sum_{i=1}^{n} \left( X_i-\mu_0 \right)^2 \right] = \sum_{i=1}^{n} \frac{1}{n} \left( X_i-\mu_0 \right)^2 = \hat\sigma^2(X).
\end{equation}
Die Stichprobenvarianz $\hat\sigma^2(X)$ ist somit der eindeutige \textsc{umvue}-Schätzer
für $\sigma^2$. Daher kann $s^2(X)$ in diesem Kontext kein
\textsc{umvue}-Schätzer sein.







\problem{UMVUE. Normalverteilung. } Sei $X_1,\ldots,X_n$ eine i.i.d.\
Stichprobe mit $X_1\sim\mathcal N(\mu,1)$. Finden Sie den 
\textsc{umvue}-Schätzer für
\begin{equation*}
	P_\mu (X_1>0).
\end{equation*}
Betrachten Sie dazu die gemeinsame Verteilung von $(X_1,\bar X)$.

\solution Der Satz von Lehmann-Sch\'effe ist das wichtigste Tool
zur Gewinnung der \textsc{umvue}-Schätzern das uns derzeit zur Verfügung. Wir
versuchen ihn auch diesmal anzuwenden. 

Dazu brauchen wir einen unverzerrten Schätzer für $P\left( X_1 >0 \right)$, also einen
Schätzer $S(X)$ mit der Eigenschaft
\begin{equation*}
	E_\mu \left[ S(X) \right] = P_\mu(X_1 > 0).
\end{equation*}
Dies ist aber genau die Eigenschaft der Zufallsvariable $1_{ \left\{ X_1 > 0 \right\}  }$, denn
\begin{equation*}
	E_\mu \left[   1_{ \left\{ X_1 > 0 \right\}  }  \right] = P_\mu( X_1 > 0).
\end{equation*}
Als eine suffiziente und vollständige Statistik verwenden wir $\bar X$. Nun mit
Hilfe der Rao-Blackwellisierung erhalten wir einen \textsc{UMVU} Schätzer
\begin{equation*}
	E_\mu \left[ 1_{ \left\{ X_1 > 0 \right\}  } | \bar X \right]
	= P_\mu \left( X_1 > 0 | \bar X \right) 
	= P_\mu \left( X_1 - \bar X > -\bar X | \bar X \right).
\end{equation*}
Da $X_1$ und $\bar X$ normalverteilt sind mit $E X_1 = E \bar X = \mu$, 
$\Var \left( X_1 \right)=1$ und $\Var\left( \bar X \right) = \frac{1}{n}$ sowie
$\cov \left( X_1- \bar X, \bar X \right) = \cov(X_1, \bar X) - \cov(\bar X, \bar X) = 0$ gilt, 
sind die Zufallsvariablen $X_1 - \bar X$ und $\bar X$ unkorreliert und damit unabhängig. 
So ist
\begin{equation*}
	P_\mu \left( X_1 - \bar X > -\bar X | \bar X = \bar x \right) = P_\mu \left( Z > -\bar x  \right)
\end{equation*}
für $Z=X_1 - \bar X$. Nachdem $EZ=0$ und 
\begin{align*}
	\Var Z &= \Var \left( X_1- \bar X \right) \\
	&= \Var \left( \frac{n-1}{n} X_1 - \frac{1}{n} \sum_{i\geq 2}^{} X_i  \right) \\
	&= \left( \frac{n-1}{n} \right)^2 + \frac{1}{n^2} \sum_{i\geq 2}^{} \Var \left( X_i  \right) \\
	&= \frac{n-1}{ n}
\end{align*}
is $\frac{Z}{\sqrt{\frac{n-1}{2}}}$ standardnormalverteilt. So gilt
\begin{align*}
	P_\mu \left( Z > -\bar X \right) &= 
	P_\mu \left(  \frac{Z}{ \sqrt{\frac{n-1}{n}} } > \frac{-\bar X}{ \sqrt{\frac{n-1}{n}}   } \right) \\
	&= 1- \Phi\left( \frac{-\bar X}{ \sqrt{\frac{n-1}{n}}}    \right) 
\end{align*}
und ein \textsc{umvue}-Schätzer für $P\left( X_1 >0 \right)$ ist bestimmt.




\problem{Cram\'er-Rao Regularität und exponentielle Familien. }
Für eine einparametrige exponentielle Familie mit
\begin{equation*}
	p(x, \theta) = \exp \left( c(\theta) T(x) + d(\theta) +S(x) \right)1_A(x),
\end{equation*}
sodass $c$ differenzierbar mit $\frac{\partial}{\partial \theta} c(\theta)\neq 0$ 
für alle $\theta\in \Theta$ und $\Theta$ offen in $\R$ ist, sind die Bedingungen (\textsc{cr})
erfüllt.

\emph{Hinweis:} Sie können für die Vertauschbarkeit von $\frac{\partial}{\partial \theta}$ und $\int_{}^{}$
den Satz von der monotonen Konvergenz verwenden. 

\solution
Für eine Statistik $T(X)$ mit $E_\theta (|T(X)|) < \infty$ zeigen wir, dass für alle $\theta\in\Theta$
\begin{equation}
	\frac{\partial}{\partial \theta} \int_{\R}^{} T(x) p(x,\theta) dx = 
	\int_{\R}^{} \frac{\partial}{\partial \theta} p(x,\theta) T(x) dx
	\label{main-interchange}
\end{equation}
gilt.

Zunächst können wir die exponentielle Darstellung von $p(x,\theta)$
reparametrisieren und die Dichte als natürliche exponentielle Familie 
\begin{equation}
	p(x,\theta) = \exp \left( \theta T(x) + d_0(\theta) + S(x) \right)1_A(x)
\end{equation}
schreiben.

Wir vereinfachen nun die Darstellung von $p(x,\theta)$ indem wir das eindimensionale
Lebesguemass durch ein absolut stetiges Mass auf $\R$ ersetzen. So ist 
\begin{eqnarray}
	\int_{\R}^{} T(x) p(x,\theta) dx &=& 
	\int_{\R}^{} T(x) \exp \left( \theta T(x) + d_0(\theta) + S(x) \right)1_A(x) dx \\
	&=& \int_{\R}^{} T(x) \exp \left( \theta T(X) + d_0(\theta) \right) d\mu 
\end{eqnarray}

Wir zeigen nun ein Hilfsresultat. Es gilt
\begin{equation}
	\frac{\partial}{\partial \theta} \int_{\R}^{} e^{\theta T(x)} d\mu 
	= \int_{\R} \frac{\partial}{\partial \theta} e^{\theta T(x)} d\mu.
	\label{simple-interchange}
\end{equation}
falls die entsprechenden Integrale existieren und endlich sind.
Die Ableitung von $\int e^{\theta T(x)}$ nach $\theta$ ist der Grenzwert des Quotienten
\begin{equation}
	\int \frac{e^{\theta_1 T(x)} - e^{\theta_2 T(x)}}{\theta_1 - \theta_2}.
\end{equation}
Den Integranden schätzen wir mit $|e^y - e^x|\leq e^x |y-x|$ wie folgt
\begin{eqnarray}
	\left| \frac{e^{\theta_1 T(x)} - e^{\theta_2 T(x)}}{\theta_1 - \theta_2} \right| &\leq& 
	\frac{e^{\theta_2 T(x)} | (\theta_2 -\theta_1) T(x) | }{ |\theta_1 - \theta_2| }
\end{eqnarray}
ab. Die letzte Funktion ist integrierbar was zusammen mit dem Satz von der majorisierten Konvergenz
die Gleichung (\ref{simple-interchange}) rechtfertigt. Beachten Sie, dass wir bei dieser Überlegung
die Voraussetzungen $\Theta$ ist offen und $\frac{\partial}{\partial \theta}c(\theta)\neq 0$
benutzt haben.

Da $p(x,\theta)$ eine Dichte ist und somit integrierbar, können wir aus
\begin{equation}
	\int_{\R}^{} e^{\theta T(x) + d_0(\theta)}d \mu = 1
\end{equation}
die Darstellung
\begin{equation}
	e^{-d_0(\theta)} = \int_{\R}^{} e^{\theta T(x)} d \mu
\end{equation}
ableiten und stellen mit Hilfe von (\ref{simple-interchange}) die Differenzierbarkeit von $d_0(\theta)$ fest.


$E_\theta \left( | T(X) | \right) < \infty$ impliziert, dass $T(X)$ als Differenz zweier
positiver integrierbarer Funktionen $T(X) = T^+(X) - T^-(X)$ geschrieben werden kann.
Das führt uns zu
\begin{eqnarray}
	\int_{\R}^{} T(x) e^{ \theta T(X) + d_0(\theta) } d\mu &=& 
	\int_{\R}^{} T(x)^+ e^{ \theta T(X) + d_0(\theta) } d\mu - \\
	&& \int_{\R}^{} T(x)^- e^{ \theta T(X) + d_0(\theta) } d\mu  \\
	&=&  e^{d_0(\theta)} \int_{\R}^{} e^{ \theta T(X) } d\mu^+ - e^{d_0(\theta)} \int_{\R}^{} e^{ \theta T(X) } d\mu^-
\end{eqnarray} 
indem wir $T^+$ und $T^-$ in den neuen Massen $\mu^+$ und $\mu^-$ verschwinden lassen.
Nun können wir das Resultat (\ref{simple-interchange}) und die Differenzierbarkeit von $d_0(\theta)$
anwenden und erhalten (\ref{main-interchange}).

Mit Hilfe der obigen Überlegungen können wir auch die Existenz der Ableitung 
$\frac{\partial}{\partial \theta} \log p(x,\theta)$ sicherstellen.
Die Übrigen Bedingungen aus (\textsc{cr}) sind in unseren Voraussetzungen enthalten.






\problem{UMVUE. Exponentialverteilung.}
Seien $X_1,\ldots,X_m$ i.i.d.\ und $X_i\sim \textrm{Exp}(\theta)$
exponentialverteilt mit der Dichte $\theta e^{-\theta x}1_{\R_{>0}}(x)$ und
$\theta>0$.
\begin{enumerate}
    \item Finden Sie einen \textsc{umvue}-Schätzer für $q(\theta) =
        \frac{1}{\theta^2}$.
    \item Zeigen Sie, dass dieser die untere Schranke der Informationsungleichung
        nicht annimmt.
\end{enumerate}
 
\solution 
\begin{enumerate}
    \item
Wir stellen die Dichte der Exponentialverteilung als exponentielle Familie 
\begin{equation*}
    p_\theta(x) = \exp \left( -\theta x + \log \theta \right) 1_{\R_{>0}}(x)
\end{equation*}
dar. Damit ist $T(X)=\sum_{i}^{} X_i$ eine vollständige und suffiziente Statistik.

Laut Satz von Lehmann-Sch\'effe muss der eindeutige \textsc{umvue}-Schätzer
eine Funktion $h(T(X))$ der vollständigen Statistik $T(X)$ sein. Nachdem der
Schätzer $h(T(X))$ erwartungstreu ist, muss
\begin{equation*}
    E_\theta h(T(X)) = \frac{1}{\theta^2}
\end{equation*}
gelten. Mit Hilfe von $\sum_{i}^{} X_i \sim \textrm{Gamma}(n,\theta)$, können
wir eine Gleichung für $h$ konstruieren.
\begin{equation*}
    E_\theta h(T(X)) =  \int_{\R} h(x) \frac{\theta^n}{\Gamma(n)} x^{n-1}e^{-\theta x} 1_{\R_{>0}}(x) dx =\frac{1}{\theta^2} 
\end{equation*}
\begin{equation*}
    \int_{0}^{\infty} h(x) \frac{\theta^{n+2}}{\Gamma(n)}  x^{n-1} e^{-\theta x} dx = 1
\end{equation*}
Die Gleichheit gilt für $h(x)= \frac{x^2}{n(n+1)}$. Damit ist 
\begin{equation*}
    S(X) = h\left( \sum_{i}^{} X_i \right) = \frac{\left( \sum_{i}^{} X_i \right)^2}{n(n+1)}
\end{equation*}
der eindeutige \textsc{umvue}-Schätzer für $\frac{1}{\theta^2}$.

    \item 
Wir berechnen nun die Schranke der Informationsungleichung -- die \textsc{(cr)}
Bedingungen sind erfüllt, da $\textrm{Exp}(\theta)$ eine exponentielle Familie
mit $c'(\theta)\neq 0$ und offenem Parameterraum ist. Es gilt
$\Psi(\theta)=\frac{1}{\theta^2}$ und $\left( \Psi'(\theta) \right)^2=
\frac{4}{\theta^6}$, sowie
\begin{align*}
    I_1(\theta) &= E_\theta \left( \frac{\partial}{\partial \theta} \log p(X, \theta) \right)^2 \\
    &= E_\theta \left( \frac{\partial}{\partial \theta} \log \theta -\theta x \right)^2 = \frac{1}{\theta^2}.
\end{align*}
Damit bekommen wir die Schranke für die Varianz
\begin{equation*}
    \frac{\left( \Psi'(\theta) \right)^2}{n I_1(\theta) } = \frac{4}{n \theta^4}.
\end{equation*}
Mit dieser Schranke können wir die Varianz des Schätzers $S(X)$ in Beziehung setzen.
\begin{align*}
    E_\theta S(X)^2 &= \int_{0}^{\infty} \left( \frac{t^2}{n(n+1)} \right)^2 \frac{\theta^n}{\Gamma(n)} t^{n-1} e^{-\theta t} dt \\
    &= \frac{(n+2)(n+3)}{n(n+1)\theta^4} \int_{0}^{\infty} \frac{\theta^4}{\Gamma(n+4)}t^{n+4-1} e^{-\theta t} dt \\
    &=  \frac{(n+2)(n+3)}{n(n+1)\theta^4} \\
    \Var_\theta S(X) &= \frac{(n+2)(n+3)}{n(n+1)\theta^4} - \frac{1}{\theta^4}
\end{align*}
Nun zeigen wir
\begin{equation*}
    \Var_\theta S(X) > \frac{\left( \Psi'(\theta) \right)^2}{n I_1(\theta) }
\end{equation*}
\begin{equation*}
    \frac{(n+2)(n+3)}{n(n+1)\theta^4} - \frac{1}{\theta^4} > \frac{4}{n \theta^4}
\end{equation*}
\begin{equation*}
    \frac{(n+2)(n+3)}{n(n+1)\theta^4} > \frac{n+4}{\theta^4 n}
\end{equation*}
\begin{equation*}
    (n+2)(n+3) > (n+4)(n+1).
\end{equation*}
\end{enumerate}






\problem{UMVUE. Gamma-Verteilung. } Eine Stichprobe $X_1, \ldots, X_n$ sei i.i.d.\
und Gamma-verteilt mit bekanntem Parameter $a>0$ und unbekanntem Parameter 
$\lambda>0$, d.h.\ $X_1$ hat die Dichte
\begin{equation*}
    p_\lambda (x) = \frac{\lambda^a}{\Gamma(a)} x^{a-1}e^{-\lambda x} 1_{\R_{>0}}(x).
\end{equation*}
Finden Sie mit Hilfe der Informationsungleichung einen \textsc{umvue}-Schätzer für
$q(\lambda) = \frac{1}{\lambda}$.

\solution Die Lösungsmethode ist hier einen unverzerrten Schätzer mit der
kleinstmöglichen Varianz zu finden, d.h.\ der Varianz aus der Informationsungleichung.

Zuerst überprüfen wir die (\textsc{cr}) Bedingungen indem wir $p_\lambda(x)$ als
exponentielle Familie
\begin{equation*}
    p_\lambda(x) = \exp\left( a\log \lambda -\Gamma(a) +(a-1)\log x -\lambda x  \right) 1_{\R_{>0}}(x)    
\end{equation*}
darstellen. $c(\lambda)=-\lambda$ ist differenzierbar mit $c'(\lambda)=-1\neq 0$ und 
$\Theta=\R_{>0}$ ist offen. Damit sind \textsc{(cr)} Bedingungen erfüllt.

Nachdem $q(\lambda)=\frac{1}{\lambda}$ und $E_\lambda X_i=\frac{a}{\lambda}$ gilt, 
wählen wir den Schätzer 
\begin{equation*}
    T(X)=\frac{1}{a}\bar X.
\end{equation*}
$T(X)$ ist erwartungstreu und hat
die Varianz
\begin{align*}
    \Var_\lambda T(X) &=  \Var_\lambda \frac{1}{na} \sum_{i}^{} X_i \\ 
    &= \frac{1}{a^2 n^2} \Var_\lambda \sum_{i}^{} X_i \\
    &= \frac{1}{a^2 n^2} \frac{an}{\lambda^2} = \frac{1}{an \lambda^2}. 
\end{align*}
Letzes gilt wegen $\sum_{i}^{} X_i \sim \textrm{Gamma}(na,\lambda)$. Nun
berechnen wir die Fischer-Information für $X=X_i$.
\begin{align*}
    I_1(\lambda) &= \Var_\lambda \frac{\partial}{\partial \lambda} \log p(X,\lambda) \\
    &= \Var_\lambda \frac{\partial}{\partial \lambda} \log \frac{\lambda^a}{\Gamma(a)} X^{a-1}e^{-\lambda X} 1_{\R_{>0}}(X) \\
    &= \Var_\lambda \frac{\partial}{\partial \lambda} \left( a \log \lambda - \lambda X \right) \\
    &= \Var_\lambda \left( \frac{a}{\lambda} - X \right) = \frac{a}{\lambda^2}
\end{align*}
Mit 
\begin{equation*}
    \Psi(\lambda) = q(\lambda)=E_\lambda T(X) =\frac{1}{\lambda} \\
\end{equation*}
\begin{equation*}
    \left( \Psi'(\lambda) \right)^2 = \frac{1}{\lambda^4}
\end{equation*}
ergibt sich 
\begin{equation*}
    \frac{  \left( \Psi'(\lambda) \right)^2 }{n I_1(\lambda)} 
    = \frac{1}{\lambda^4} \frac{\lambda^2}{an} = \frac{1}{ an \lambda^2}
\end{equation*}
und
\begin{equation*}
    \Var_{\lambda} T(X) = \frac{  \left( \Psi'(\lambda) \right)^2 }{I(\lambda)}.
\end{equation*}
Damit ist $T(X)$ ein \textsc{umvue}-Schätzer für $q(\lambda)=\frac{1}{\lambda}$.







\problem{Cram\'er-Rao-Schranke und die Gleichverteilung.}   % Shao E 3.22
Seien $X_1,\ldots,X_n$ i.i.d.\ und $X_i \sim \mathcal U(0,\theta)$ mit unbekanntem 
$\theta > 0$. Es bezeichne $I(\theta)$ die Fischer-Information. 
\begin{enumerate}
    \item
Weisen Sie nach, dass
$T(X)=\frac{n+1}{n}X_{(n)}$ ein erwartungstreuer Schätzer für $\theta$ ist und
\begin{equation*}
	\Var_\theta \left( T(X) \right) < \frac{1}{I(\theta)}, \quad \textrm{für alle } \theta>0. 
\end{equation*}
    \item Klären Sie, wieso dies nicht im Widerspruch zu der Cram\'er-Rao-Schranke
\begin{equation*}
	\Var_\theta\left( T(X) \right) \geq \frac{1}{I(\theta)}
\end{equation*}
steht.
\end{enumerate}


\solution Wir zeigen zuerst, dass $T(X)$ ein erwartungstreuer Schätzer
für $\theta$ ist. Dazu berechnen wir die Verteilung von $X_{(n)}$.
\begin{align*}
    P\left( X_{(n)} \leq t \right) &= P\left( X_1\leq t,\ldots, X_n\leq t \right) \\
    &= \left( P(X_1 \leq t) \right)^n
\end{align*}
Daraus ergibt sich
\begin{align*}
    E_\theta X_{(n)} &= \int_{0}^{\theta} x dP_{ X_{(n)} }(x) \\
    &= \int_{0}^{\theta} x \left( \frac{x^n}{\theta^n} \right)' dx \\
    &= \int_{0}^{\theta} n \frac{x^n}{\theta^n} dx \\
    &= \frac{n}{(n+1)\theta^n} \int_{0}^{\theta} (n+1)x^n dx \\
    &= \frac{n}{n+1} \frac{1}{\theta^n} \theta^{n+1} = \frac{n}{n+1} \theta.
\end{align*}
Daher ist $T(X) = \frac{n+1}{n} X_{(n)}$ erwartungstreu für $\theta$.

Die Fischer-Information für $X_1$ ist 
\begin{align*}
    I_1(\theta) &= 
    E_\theta\left( \frac{\partial}{\partial \theta} \log \frac{1}{\theta} \right)^2 \\
    &= E_\theta \left( \frac{\partial}{\partial \theta} -\log \theta \right)^2 \\
    &= E_\theta \frac{1}{\theta^2} = \frac{1}{\theta^2}.
\end{align*}
Somit ist
\begin{equation*}
    I(\theta) = n I_1 \left( \theta \right) = \frac{n}{\theta^2}.
\end{equation*}

Berechnen wir nun
\begin{align*}
    \Var T(X) &= \Var \frac{n+1}{n} X_{(n)} \\
    &= E\left( \frac{n+1}{n} X_{(n)} \right)^2 - \left( E \frac{n+1}{n} X_{(n)} \right)^2 \\
    &= \left( \frac{n+1}{n} \right)^2 E X_{(n)}^2 - \theta^2 \\
    E X_{(n)}^2 &= \int_{0}^{\theta} x^2 d P_{X_{(n)}}(x) \\
    &= \frac{n}{\theta^n} \int_{0}^{\theta} x^{n+1} = \frac{\theta^2 n}{n+2} \\
    \Var T(X) &= \left( \frac{n+1}{n} \right)^2 \frac{\theta^2 n }{n+2} - \theta^2 = \frac{\theta^2}{n(n+2)}.
\end{align*}
Damit ist 
\begin{equation*}
    \Var_\theta T(X) = \frac{\theta^2}{n(n+2)} < 
    \frac{\theta^2}{n} = \frac{1}{I(\theta)}.
\end{equation*}

Erklärung dafür ist die folgende. Der
Träger der Dichte von $X_i$ ist vom Parameter $\theta$ abhängig und daher sind
die \textsc{(cr)} Bedingungen verletzt.  Deswegen muss die
Informationsungleichung für dieses Modell nicht gelten. 







\problem{Cram\'er-Rao-Schranke ist nicht scharf.    } 
Es ist durchaus möglich, dass ein \textsc{umvue}-Schätzer eine größere Varianz als die untere
Schranke
\begin{equation*}
	\Var_\theta \left( T(X) \right) \geq \frac{1}{I(\theta)}
\end{equation*}
hat. Betrachtet werden dazu $X_1,\ldots,X_n$ i.i.d.\ mit $X_i \sim \textrm{Poiss}(\theta)$ für 
unbekanntes $\theta>0$. 
\begin{enumerate}
    \item Zeigen Sie, dass
\begin{equation*}
	T(X) = \left( 1- \frac{1}{n} \right)^{ \sum_{i=1}^{n} X_i}
\end{equation*}
ein \textsc{umvue}-Schätzer für $g(\theta)=e^{-\theta}$ ist.
    \item Zeigen Sie weiterhin, dass die
Varianz von $T(X)$ die Schranke in der Informationsungleichung
\begin{equation*}
	\Var_\theta\left( T(X) \right) \geq \frac{\Phi^{'}(\theta)^2}{I(\theta)}
\end{equation*}
für kein $\theta$ annimmt.
\end{enumerate}


\solution Wir stellen zuerst sicher, dass $T(X)$ ein
\textsc{umvue}-Schätzer für $g(\theta)=e^{-\theta}$. Nachdem die Summe von
poissonverteilten Zufallsvariablen wieder poissonverteilt ist, und zwar
$\sum_{i}^{} X_i \sim \textrm{Poiss}(\sum_{i}^{} \lambda_i)$ für $X_i \sim
\textrm{Poiss}(\lambda_i)$, erhalten wir
\begin{eqnarray*}
    E_\theta T(X) &=& \sum_{k\geq 0} \left( 1-\frac{1}{n} \right)^k P\left( \sum_{i}^{} X_i =k \right)\\
    &=& \sum_{k\geq 0}^{} \left( 1-\frac{1}{n} \right)^k e^{-n\theta} \frac{(n\theta)^k}{k!} \\
    &=& e^{-n\theta} \sum_{k\geq 0} \frac{ (n-1)^k\theta^k}{k!} \\
    &=& e^{-n \theta} e^{(n-1)\theta} = e^{-\theta}.
\end{eqnarray*}
$T(X)$ ist also erwartungstreu für $g(\theta)$.

Aus der Vorlesung wissen wir, dass $\sum_{i}^{} X_i$ eine vollständige und suffiziente
Statistik ist, wenn die Zufallsvariablen $X_i$ poissonverteilt sind. $T(X)$ ist 
eine Funktion von $\sum_{i}^{} X_i$ und damit ein \textsc{umvue}-Schätzer
für $e^{-\theta}$.

Wir berechnen nun die Varianz von $T(X)$.
\begin{eqnarray*}
    \Var_\theta T(X)&=& \Var_\theta \left( 1-\frac{1}{n} \right)^{\sum_{i} X_i} \\ 
    &=& E_\theta \left( \left( 1-\frac{1}{n} \right)^{\sum_{i} X_i} \right)^2 - \left( E_\theta \left( 1-\frac{1}{n} \right)^{\sum_{i} X_i} \right)^2 \\
    E_\theta \left( 1-\frac{1}{n} \right)^{\sum_{i} X_i} &=& e^{-\theta} \\
    E_\theta \left( \left( 1-\frac{1}{n} \right)^{\sum_{i} X_i} \right)^2 
    &=& \sum_{k\geq 0}^{} \left( 1-\frac{1}{n} \right)^{2k} P\left( \sum_{i}^{} X_i =k \right) \\
    &=& \sum_{k\geq 0}^{} \left( 1-\frac{1}{n} \right)^{2k} e^{-n\theta} \frac{(n\theta)^k}{k!} \\
    &=& e^{-n \theta} \sum_{k\geq 0} \frac{ \left( \frac{ (n-1)^2 \theta }{n} \right)^k   }{k!} \\
    &=& \exp\left( -n\theta \right) \exp\left( \frac{(n-1)^2\theta}{n} \right)
    = \exp\left( \theta\left( -2 + \frac{1}{n} \right) \right) \\
    \Var_\theta T(X) &=& \exp\left( \frac{\theta(1-2n)}{n} \right) - \exp\left( -2\theta \right). 
\end{eqnarray*}

Nun zur Informationsungleichung. Wir haben $\Psi(\theta)=g(\theta)=E T(X)$ und 
$\Psi'(\theta)=-e^{-\theta}$ und daher 
$\left( \Psi'(\theta) \right)^2 = e^{-2\theta}$. Die Fischer-Information 
$I(\theta)=\frac{n}{\theta}$ wurde bereits in der Vorlesung berechnet. Damit erhalten wir 
\begin{equation*}
    \frac{\left( \Psi'(\theta) \right)^2}{I(\theta)} = \frac{\theta e^{-2 \theta} }{n}.
\end{equation*}

Wir zeigen nun
\begin{equation*}
    \exp\left( \frac{\theta(1-2n)}{n} \right) - \exp\left( -2\theta \right) = \Var_\theta T(X)
    > \frac{\left( \Psi'(\theta) \right)^2}{I(\theta)} = \frac{\theta e^{-2 \theta} }{n}
\end{equation*}
indem wir äquivalente Ungleichungen betrachten.
\begin{eqnarray*}
    \exp\left( \frac{\theta(1-2n)}{n} \right) - \exp\left( -2\theta \right) 
    &>& \frac{\theta e^{-2 \theta} }{n} \\
    \exp\left( \frac{\theta}{n} -2\theta \right) &>& e^{-2\theta} \left( 1+\frac{\theta}{n} \right) \\
    \theta &>& \log \left( 1+\frac{\theta}{n} \right)^n.
\end{eqnarray*}
Wir wissen aber, dass
\begin{equation*}
    e^\theta = \lim_{n\to\infty} \left( 1+\frac{\theta}{n} \right)^n
\end{equation*}
gilt, und dass die Approximationsfolge streng monoton wachsend ist. Damit ist die Ungleichung
für alle $\theta>0$ bewiesen. Daraus können wir ableiten, dass die Schranke in der 
Informationsungleichung nie angenommen wird.

%\begin{figure}[htb]
%    \begin{center}
%        \includegraphics[width=\textwidth]{figures/ue-blatt-10-fvv.pdf}
%    \end{center}
%    \caption{Varianz von $T(X)$ und die Schranke der Informationsungleichung für $n=10$.}
%    \label{fig:fvv}
%\end{figure}





\section{Asymptotische Theorie}

\problem{Verschobene Gleichverteilung und Konsistenz.} 
Die Zufallsvariablen $X_1,\ldots,X_n$ seien i.i.d.\ mit $X_i\sim U(\theta,\theta+1)$.
Der Parameter $\theta$ sei unbekannt. Für die Schätzer $T_1(X) = \bar X -\frac{1}{2}$
und $T_2(X)=X_{(1)} - \frac{1}{n+1}$ haben wir bereits
\begin{align*}
    \Var_\theta T_1(X) &= \frac{1}{12 n} \\
    \Var_\theta T_2(X) &= \frac{n}{\left( n+1 \right)^2\left( n+2 \right)}
\end{align*}
gezeigt. Überprüfen Sie nun die beiden Schätzer auf schwache Konsistenz.

\solution 
Wir zeigen zunächst, dass die beiden Schätzer erwartungstreu sind. 
Es gilt
\begin{align*}
	E X_i &=  \int_{\R}^{} x p_\theta(x) dx 
        = \int_{\theta}^{\theta+1} x dx = \theta + \frac{1}{2}. \\
	E T_1(X) &=  \frac{1}{n} \sum_{i}^{} X_i - \frac{1}{2} 
        = \frac{1}{n} \sum_{i}^{} \left( \theta + \frac{1}{2} \right) -\frac{1}{2} = \theta.
\end{align*}
Das heißt $T_1$ ist erwartungstreu.

Nun kommen wir zu $T_2 = X_{(1)}-\frac{1}{n+1}$. Die Berechnung des Erwartungswertes führt auf
\begin{align*}
	E T_2(X) &= -\frac{1}{n+1} + E X_{(1)} \\
	E X_{(1)} &= \int_{[\theta,\theta+1]^n}^{} \min\left\{ x_1,\cdots,x_n \right\} dx_1\cdots d x_n \\
	&= \int_{[0,1]^n}^{} \min\left\{ x_1+\theta,\cdots,x_n+\theta \right\} dx_1\cdots d x_n \\
	&= \theta + \int_{[0,1]^n}^{} \min\left\{ x_1,\cdots,x_n \right\} dx_1\cdots d x_n \\
	&= \theta +\sum_{\sigma\in S_n}^{} \int_{0\leq x_{\sigma(1)}\leq \ldots \leq x_{\sigma(n)}\leq 1} x_{\sigma(1)} dx_1 \cdots dx_n 
\end{align*}
wobei die Summe über alle $n!$ Permutationen der $n$ Elemente geführt wird. Es
gilt
\begin{equation*}
	\int_{0\leq x_1\leq \ldots\leq x_n\leq 1} x_1 dx_1 \cdots d x_n = 
	\int_{0}^{1}\cdots \int_{0}^{x_2} x_1 dx_1 \cdots dx_n = 
	\frac{1}{\left( n+1 \right)!}
\end{equation*}
und
\begin{align*}
	\sum_{\sigma\in S_n}^{} \int_{0\leq x_{\sigma(1)}\leq \ldots \leq x_{\sigma(n)}\leq 1} x_{\sigma(1)} dx_1 \cdots dx_n 
	&=\frac{1}{n+1}.
\end{align*}
Wir erhalten also
\begin{align*}
	E X_{(1)} &= \frac{1}{n+1} + \theta
\end{align*}
und 
\begin{align*}
	E T_2 &= -\frac{1}{n+1} + E X_{(1)} = \theta.
\end{align*}

%Wir wissen bereits, dass die Schätzer $T_1$ und $T_2$ erwartungstreu sind 
%sowie $E X_i = \theta + \frac{1}{2}$ gilt. 


Es ist leicht zu sehen, dass beide dieser Schätzer
die Bedingungen 
\begin{enumerate}
    \item $E_\theta(T^2_n)< \infty$ für alle $\theta\in\Theta$ und alle $n\in\mathbb N$.
    \item $\lim_{n\to\infty}E_\theta(T_n) = \theta$ für alle $\theta\in\Theta$.
    \item $\lim_{n\to\infty} \Var_\theta T_n = 0$ für alle $\theta\in\Theta$.
\end{enumerate}
erfüllen. Es gilt für $i=1,2$
\begin{equation*}
    E T_i^2 = \Var T_i + (E T_i)^2 <\infty
\end{equation*}
sowie $E T_n \to \theta$ wegen Erwartungstreue und $\Var T_i \to 0$. Somit sind beide dieser
Schätzer schwach konsistent.

Die Aussage kann man auch direkt beweisen. Im Fall $T_1$ bietet sich die Anwendung
des schwachen Gesetzes der großen Zahl an. Die schwache Konvergenz von $T_2$ gegen $\theta$ 
folgt mit Hilfe der Markov-Ungleichung.





\problem{Konsistenz. Hinreichende Bedingungen. }
\label{Konsistenz-Hinreichende-Bedingungen}
Seien $X_1,\ldots,X_n$ i.i.d.\ mit Verteilung $\mathbb P_\theta$ und $\theta\in\Theta\subset\R$.
Für jedes $n\in\mathbb N$ sei $T_n = T(X_1,\ldots,X_n)$ ein Schätzer für $\theta$ mit 
folgenden Eigenschaften:
\begin{enumerate}
    \item $E_\theta(T^2_n)< \infty$ für alle $\theta\in\Theta$ und alle $n\in\mathbb N$.
    \item $\lim_{n\to\infty}E_\theta(T_n) = \theta$ für alle $\theta\in\Theta$.
    \item $\lim_{n\to\infty} \Var_\theta T_n = 0$ für alle $\theta\in\Theta$.
\end{enumerate}
Dann ist der Schätzer $T_n$ schwach konsistent, d.h.\ $T_n \to^{\mathbb P_\theta} \theta$
für $n\to\infty$.

\solution Der Schätzer $T_n$ ist schwach konsistent wenn
\begin{equation*}
    P\left( |T_n -\theta|\geq \varepsilon \right) \to 0
\end{equation*}
wenn $n\to\infty$ für alle $\varepsilon>0$ und $\theta\in\Theta$. Wir schätzen die obige
Wahrscheinlichkeit mit Hilfe der Markov-Ungleichung
\begin{equation*}
    P\left( |X|\geq \varepsilon \right) \leq \frac{E f(|X|)}{f(\varepsilon)}
\end{equation*}
ab und erhalten für feste $n,\theta$ und $\varepsilon$
\begin{align*}
    P\left( |T_n -\theta|\geq \varepsilon \right) &\leq \frac{E \left( T_n -\theta \right)^2}{\varepsilon^2} \\
    &= \frac{E \left( T_n^2 -2\theta T_n + \theta^2 \right) }{\varepsilon^2} \\
    &= \varepsilon^{-2}\left( E T_n^2 - 2\theta E T_n + \theta^2 \right) \\
    &= \varepsilon^{-2}\left( \Var T_n + \left( E T_n \right)^2 - 2\theta E T_n + \theta^2 \right).
\end{align*}
Laut Voraussetzung ist $T_n$ quadratisch integrierbar, daher macht die obige Berechnung Sinn.
Für $n\to\infty$ erhalten wir
\begin{equation*}
    \varepsilon^{-2}\left( \Var T_n + \left( E T_n \right)^2 - 2\theta E T_n + \theta^2 \right) \to 0
\end{equation*}
und daraus
\begin{equation*}
    P\left( |T_n -\theta|\geq \varepsilon \right) \to 0.
\end{equation*}
Der Schätzer $T_n$ ist also schwach Konsistent.







\problem{Asymptotische Verteilung von MLE.}   Seien $X_1,\ldots,X_n$ i.i.d.\ mit 
der Dichte $p(x,\theta)= \theta x^{\theta-1}1_{(0,1)}(x)$ für $\theta>0$. Finden Sie 
einen Maximum-Likelihood-Schätzer für $\theta$ und seine asymptotische Verteilung.

\solution Die Likelihoodfunktion der Stichprobe $x_1,\ldots,x_n$ ist
\begin{align*}
    L(x,\theta) &= \prod_i p(x_i, \theta) = \theta^n \prod_i x_i^{\theta-1}.
\end{align*}
Daraus erhalten wir
\begin{align*}
    l(x,\theta) &= \log L(x,\theta) \\
    &= n \log \theta + (\theta-1) \sum_{i}^{} \log x_i \\
    \frac{\partial}{\partial\theta} l(x,\theta) &= \frac{n}{\theta} + \sum_{i}^{} \log x_i \\
    \frac{\partial^2}{\partial \theta^2} l(x,\theta) &= -\frac{n}{\theta^2} < 0 \quad \forall\theta>0.
\end{align*}
Aus der Gleichung
\begin{equation*}
    \frac{\partial}{\partial\theta} l(x,\theta) = 0
\end{equation*}
erhalten wir den Maximum-Likelihood-Schätzer
\begin{equation*}
    \hat \theta(X_1,\ldots,X_n) = - \frac{n}{\sum_{i}^{} \log X_i}.
\end{equation*}

Wir berechnen die Fischer-Information pro Beobachtung. 
\begin{align*}
    I_1(\theta) &= E_\theta \left( \frac{\partial}{\partial \theta} \log p_\theta(X) \right)^2 \\
    &= E_\theta \left( \frac{\partial}{\partial\theta} \log \theta + (\theta-1) \log X \right)^2 \\
    &= E_\theta \left( \frac{1}{\theta} + \log X \right)^2 \\
    &= E_\theta \left( \frac{1}{\theta^2} + \frac{2}{\theta} \log X + (\log X)^2 \right)
\end{align*}
Es gilt nun
\begin{align*}
    E_\theta \log X &=  \int_{0}^{1} \log x \theta x^{\theta-1} dx = \int_{0}^{1} \log x (x^{\theta})' dx \\
    &= x^\theta \log x \Big|_0^1 - \int_{0}^{1} \frac{x^{\theta-1}}{\theta} dx = - \frac{1}{\theta} \\
    E_\theta (\log X)^2 &=&  \int_{0}^{1} (\log x)^2 \theta x^{\theta-1} dx \\ 
    &= x^\theta (\log x)^2 \Big|_0^1 - \int_{0}^{1} 2(\log x) x^{\theta-1} dx = \frac{2}{\theta^2} \\
\end{align*}
Die Fischer-Information ist nun
\begin{align*}
    I_1(\theta) &= \frac{1}{\theta^2}
\end{align*}
und die asymptotische Verteilung von $\hat \theta$ erfüllt
\begin{equation*}
    \sqrt{n}\left( \hat\theta_n(X) - \theta_0 \right) \xrightarrow{\mathcal{L}} \mathcal N(0, \theta^2)
\end{equation*}
falls die Bedingungen \textsc{(ar)} erfüllt sind. Das ist in der Tat der Fall denn der Parameterraum
$\Theta=\R_{>0}$ offen ist. Weiters gilt
\begin{align*}
    \int_{0}^{1} \frac{\partial^2}{\partial\theta^2} p(x,\theta) dx &= 
    \int_{0}^{1} \frac{\partial^2}{\partial\theta^2} \theta x^{\theta-1} dx \\
    &= \int_{0}^{1} \theta\,x^{\theta-1}\,\log ^2x+2\,x^{\theta-1}\,\log x dx\\
    &= \left[   x^{\theta} \log^2x  \right]_0^1 = 0
\end{align*}
und
\begin{align*}
    A(\theta,x) &= \frac{\partial^2}{\partial \theta^2} \log p(x,\theta) \\
    &= \frac{\partial^2}{\partial \theta^2} \log \theta + (\theta-1) \log x \\
    &= -\frac{1}{\theta ^2}. 
\end{align*}
$A(\theta,x)$ ist von $x$ unabhängig und daher gleichmässig beschränkt in einer
hinreichend kleinen Umgebung von $\theta_0$ die $0$ nicht enthält. $K$ kann daher konstant 
gewählt werden. Die Fisher-Information ist positiv und aus $p(x,\theta)=p(x,\theta_0)$ f.s.\ in
$x\in\R$ folgt $\theta=\theta_0$ denn $p(x,\theta)= \theta x^{\theta-1}$ ein Polynom in $x$ ist. 
Damit sind die \textsc{(ar)} Regularitätsbedingungen erfüllt und obige Aussage über die
asymptotische Verteilung des MLS gerechtfertigt.








\problem{Asymptotische Effizienz.} Seien $X_1,\ldots,X_n$ i.i.d.\ mit $E X_1=\mu\neq 0$,
$\Var X_1 = 1$ und $E X_1^4 < \infty$. Der Erwartungswert $\mu$ sei unbekannt. 
Ferner seien 
\begin{align*}
    T_1(X) &= \frac{1}{n} \sum_{i=1}^{n} \left( X_i^2 -1 \right) \\
    T_2(X) &= {\bar X}^2 - \frac{1}{n}
\end{align*}
zwei Schätzer für $\mu^2$. 
\begin{enumerate}
    \item Zeigen Sie, dass $T_1$ und $T_2$ asymptotisch normalverteilt sind und
        berechnen Sie deren asymptotische Erwartung und Varianz.
    \item Berechnen Sie die asymptotische Effizienz von $T_1$ zu $T_2$.
    \item Zeigen Sie, dass die asymptotische Effizienz von $T_1$ zu $T_2$ nicht
        größer ist als $1$, falls Verteilung von $X_1 -\mu$ um $0$ symmetrisch
        ist.
\end{enumerate}

\solution Betrachten wir zunächst den Schätzer $T_1$. Wir erhalten
\begin{align*}
    E X_i^2 &=  \Var X_i + \left( E X_i \right)^2 = 1 + \mu^2  \\
    \sqrt{n} \left( T_1 - \mu^2 \right) &= 
    \sqrt{n}\left( \frac{1}{n} \sum_{i}^{} X_i^2 -1 -\mu^2 \right) \to_d \mathcal N(0, \Var X_i^2)
\end{align*}
indem wir zentralen Grenzverteilungssatz auf Zufallsvariablen $X_i^2$ anwenden. $\Var X_i^2$ existiert
nach Voraussetzung.

Wir bestimmen die asymptotische Verteilung von $T_2$. Zentraler Grenzverteilungssatz liefert uns
\begin{equation*}
    \sqrt{n}\left( \bar X - \mu \right) \to_d \mathcal N(0,1).
\end{equation*}
Mit Hilfe der Delta-Methode und des Satzes von
\href{http://books.google.de/books?id=6anRT6Cz5GAC&lpg=PA101&dq=Slutsky's%20theorem&hl=de&pg=PA101#v=onepage&q&f=false}{Slutsky}
erhalten wir
\begin{equation*}
    \sqrt{n}\left( T_2 - \mu^2 \right) =
    \sqrt{n}\left( {\bar X}^2 - \mu^2 \right) - \frac{1}{\sqrt{n}} \to_d 
    2\mu \mathcal N(0,1) = \mathcal N(0, 4\mu^2).
\end{equation*}
%Im Falle $\mu=0$ ergibt sich aus $\sqrt{n}\left( \bar X \right) \to_d \mathcal N(0,1)$ 
%\begin{eqnarray}
%    n\left( T_2 - \mu^2 \right) &=&  n\left( {\bar X}^2 - \frac{1}{n} \right) \\
%    &=& n {\bar X}^2 -1 \to_d \mathcal N(0,1)^2 -1 = \chi_1^2 -1.
%\end{eqnarray}
%Sei $V$ eine $\chi_1^2$-verteilte Zufallsvariable. Es gilt
%\begin{eqnarray}
%    E \left( V-1 \right) &=& 0, \\
%    \Var \left( V-1 \right) &=& 2.
%\end{eqnarray}
Die asymptotische Effizienz von $T_1$ zu $T_2$ ist gegeben durch
\begin{equation*}
    e(\theta, T_1, T_2) = \frac{4\mu^2}{\Var X_1^2}
%        \left\{
%            \begin{array}{ll}
%                \frac{4\mu^2}{\Var X_1^2}  & \mbox{if } \mu\neq 0 \\
%                \frac{2}{n \Var X_1^2} & \mbox{if } \mu=0
%            \end{array}
%        \right.
\end{equation*}

Ist die Verteilung $X_1 - \mu$ symmetrisch um $0$, so ist die Schiefe der Verteilung
gleich $0$, d.h.\ $E \left( X_1 -\mu \right)^3=0$ und wir können mit Hilfe der
Jensenschen Ungleichung auf
\begin{equation*}
    \Var X_1^2 \geq 4 \mu^2
\end{equation*}
und $e\geq 1$ schliessen.






\section{Konfidenzintervalle}

\problem{Konfidenzintervall. Normalverteilung. } Seien $X_1,\ldots,X_n$ i.i.d.\ mit 
$X_i\sim \mathcal N(\mu,\sigma^2)$, wobei sowohl $\mu$ als auch $\sigma$ unbekannt seien.
Zeigen Sie, dass
\begin{equation*}
    \left[ \frac{n-1}{\chi^2_{n-1,1-\alpha/2}}s^2(X), \frac{n-1}{\chi^2_{n-1,\alpha/2}}s^2(X) \right]
\end{equation*}
ein $\left( 1-\alpha \right)$-Konfidenzintervall für $\sigma^2$ ist, wobei
$s^2(X)$ die Stichprobenvarianz und $\chi^2_{n,a}$ das $a$-Quantil der $\chi^2_n$-Verteilung bezeichnen.

\solution
Die Stichprobenvarianz ist definiert durch
\begin{equation*}
    s^2(X) := \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar X)^2.
\end{equation*}
Die Verteilung der Stichprobenvarianz ist bereits bekannt.
\begin{equation*}
    \frac{(n-1) s^2(X)}{\sigma^2} = \sum_{i=1}^{n} \left( \frac{ X_i -\bar X}{\sigma} \right)^2
    \sim \chi^2_{n-1}
\end{equation*}
Das Konfidenzintervall erhalten wir aus
\begin{align*}
    P \left( \frac{n-1}{\chi^2_{n-1,1-\frac{\alpha}{2}} } s^2(X) 
    \leq \sigma^2 \leq 
    \frac{n-1}{ \chi^2_{n-1,\frac{\alpha}{2}} } s^2(X)
    \right) &= \\
    P \left( \frac{n-1}{\sigma^2} s^{2}(X) \leq \chi^2_{n-1,1-\frac{\alpha}{2}} \wedge
    \chi^2_{n-1,\frac{\alpha}{2}} \leq \frac{n-1}{\sigma^2} s^2(X)
    \right) &= 1-\alpha.
\end{align*}








\problem{Konfidenzintervall. Exponentialverteilung.  }
Seien $X_1,\ldots,X_n$ i.i.d.\ exponentialverteilt mit der Dichte
\begin{equation*}
    p_\theta(x) = \theta e^{- \theta x} 1_{ \R_{>0} }(x).
\end{equation*}
Zeigen Sie, dass 
\begin{equation*}
    \left[  \frac{- \log \left( 1 - \frac{\alpha}{2} \right)}{n X_{(1)}}, 
    \frac{- \log \left( \frac{\alpha}{2} \right)}{n X_{(1)}} \right]
\end{equation*}
ein $(1-\alpha)$-Konfidenzintervall für $\theta$ ist.

\solution Zu zeigen ist folgende Gleichung, die das 
Konfidenzintervall charakterisiert:
\begin{equation*}
    P\left(   \frac{- \log \left( 1 - \frac{\alpha}{2} \right)}{n X_{(1)}} \leq \theta \leq 
    \frac{- \log \left( \frac{\alpha}{2} \right)}{n X_{(1)}} \right) = 1-\alpha.
\end{equation*}
Betrachten wir die Wahrscheinlichkeiten
\begin{align*}
    P \left( n\theta X_{(1)}\geq x \right) &=  \prod_i P \left( n\theta X_i \geq x \right) \\
    &= \prod_i P\left( X_i \geq \frac{x}{n\theta} \right) \\
    &= \prod_i e^{-\frac{x}{n}} = e^{-x}.
\end{align*}
Die Gleichung
\begin{equation*}
    P \left( c_1(\alpha) \leq n\theta X_{(1)} \leq c_2(\alpha) \right) = 
    e^{-c_1(\alpha)} - e^{-c_2(\alpha)} = 1-\alpha
\end{equation*}
ist unter anderem für $c_1 = -\log \left( 1-\frac{\alpha}{2} \right)$ und
$c_2 = -\log \left( \frac{\alpha}{2} \right)$ erfüllt.





\problem{Konfidenzintervall. Varianzenvergleich bei Normalverteilung. } 
Seien $X_1,\ldots,X_n,Y_1,\ldots,Y_n$ unabhängig und normalverteilt, mit 
$X_i \sim \mathcal N(0, \sigma^2_X)$, $Y_i \sim \mathcal N(0, \sigma^2_Y)$
und unbekannten Parametern $\sigma^2_X>0$ und $\sigma^2_Y>0$. 
Zeigen Sie, dass für $S^2_X = \sum_{i=1}^{n} X_i^2$ und $S^2_Y = \sum_{i=1}^{n} Y_i^2$
\begin{equation*}
    \left[ F^{-1}_{n,n} \left( \frac{\alpha}{2} \right) \frac{S_Y^2}{S_X^2},
    F^{-1}_{n,n} \left( 1 - \frac{\alpha}{2}  \right) \frac{S_Y^2}{S_X^2}     \right]
\end{equation*}
ein $(1-\alpha)$-Konfidenzintervall für den Quotienten $\frac{\sigma^2_Y}{\sigma^2_X}$ ist.
$F_{n,n}$ bezeichnet dabei die Verteilungsfunktion der $F_{n,n}$-Verteilung.

\solution Zu zeigen ist die Gleichung
\begin{equation*}
    P \left[ F^{-1}_{n,n} \left( \frac{\alpha}{2} \right) \frac{S_Y^2}{S_X^2} \leq 
    \frac{\sigma^2_Y}{\sigma^2_X} \leq
    F^{-1}_{n,n} \left( 1 - \frac{\alpha}{2}  \right) \frac{S_Y^2}{S_X^2}     \right]
    = 1-\alpha.
\end{equation*}
Es gilt aber 
\begin{equation*}
    \frac{\sigma^{-2}_X S^2_X  }{ \sigma^{-2}_Y S^2_Y} = 
    \frac{\sum_{i}^{} \left( \frac{X_i}{\sigma_X} \right)^2  }{ \sum_{i}^{} \left( \frac{Y_i}{\sigma_Y} \right)^2 } \sim F_{n,n}.
\end{equation*}







\section{Testtheorie}

\problem{Test. Mittelwertvergleich unter Normalverteilung. } 
Seien die Zufallsvariablen $X_1,\ldots,X_n$, $Y_1,\ldots,Y_n$ unabhängig und normalverteit, 
mit $X_i\sim\mathcal N(\mu_X, \sigma^2_X)$ und $Y_i\sim\mathcal N(\mu_Y, \sigma^2_Y)$.
Die Mittelwerte $\mu_X$ und $\mu_Y$ sind dabei unbekannt, die Varianzen dagegen
$\sigma_X^2>0$, $\sigma^2_Y>0$ bekannt und fix.
\begin{enumerate}
    \item Zeigen Sie, dass 
        \begin{equation*}
            \bar X - \bar Y \pm \sqrt{ \frac{\sigma^2_X + \sigma^2_Y}{n}  }z_{1-\frac{\alpha}{2}}
        \end{equation*}
        ein $(1-\alpha)$-Konfidenzintervall für die Differenz der Mittelwerte $\mu_X - \mu_Y$ ist.
    \item Konstruieren Sie einen Test zu dem Signifikanzniveau von $5\%$ für die Hypothese
        $H_0: \mu_X = \mu_Y$ gegen die Alternative $H_1: \mu_X \neq \mu_Y$.
    \item Drücken Sie die zum obigen Test gehörige Gütefunktion in Abhängigkeit von
        $\Delta = \mu_X -\mu_Y$ aus und skizzieren Sie die Gütefunktion. 
\end{enumerate}

\solution Wir betrachten
\begin{equation*}
    P\left( \bar X - \bar Y - \sqrt{ \frac{\sigma^2_X + \sigma^2_Y}{n}  }z_{1-\frac{\alpha}{2}} \leq \mu_X - \mu_Y \leq \bar X - \bar Y + \sqrt{ \frac{\sigma^2_X + \sigma^2_Y}{n}  }z_{1-\frac{\alpha}{2}}   \right) = 1-\alpha.
\end{equation*}
Und daraus
\begin{eqnarray*}
    P \left(  - \sqrt{ \frac{\sigma^2_X + \sigma^2_Y}{n}  }z_{1-\frac{\alpha}{2}} \leq
    \bar Y -\mu_Y -\left( \bar X - \mu_X \right) \leq 
    \sqrt{ \frac{\sigma^2_X + \sigma^2_Y}{n}  }z_{1-\frac{\alpha}{2}}
    \right) &=& \\ 
    P \left(  - z_{1-\frac{\alpha}{2}} \leq
    \frac{ \bar Y -\mu_Y -\left( \bar X - \mu_X \right)  }{  \sqrt{ \frac{\sigma^2_X + \sigma^2_Y}{n}  } } 
    \leq  z_{1-\frac{\alpha}{2}} 
    \right).
\end{eqnarray*}
Mit
\begin{equation*}
    \bar Y - \mu_Y - \left( \bar X - \mu_X \right) \sim 
    \mathcal N\left(0, \frac{\sigma^2_X}{n} + \frac{\sigma^2_Y}{n}   \right)
\end{equation*}
erhalten wir die erste Gleichung.  
Wir setzen
\begin{eqnarray*}
    \theta_1 &=& \bar X - \bar Y - \sqrt{ \frac{\sigma^2_X + \sigma^2_Y}{n}  }z_{1-\frac{\alpha}{2}} \\
    \theta_2 &=& \bar X - \bar Y + \sqrt{ \frac{\sigma^2_X + \sigma^2_Y}{n}  }z_{1-\frac{\alpha}{2}} 
\end{eqnarray*}
und konstruieren den Test
\begin{equation*}
    \delta\left( X,0 \right) = 1_{ \left\{ 0 \nin \left[ \theta_1,\theta_2 \right] \right\} }.
\end{equation*}
Mit $\alpha=0.05$ ist $\delta(X,0)$ der gesuchte Test mit Signifikanzniveau $5\%$.

Berechnen wir die Gütefunktion des Tests $\delta\left( X,0 \right)$. Mit $\theta=\left( \mu_X,\mu_Y \right)$
und $\sigma= \sqrt{\frac{\sigma_X^2 + \sigma_Y^2}{n}}$ erhalten wir
\begin{eqnarray*}
    G_\delta(\theta) &=&  E_\theta \left( \delta \left( X,0) \right)  \right) \\
    &=& P_\theta \left( \delta(X,0) = 1 \right) \\
    &=& P_\theta \left( 0 \leq \bar X - \bar Y - \sigma z_{1-\frac{\alpha}{2}} \vee
        \bar X - \bar Y + \sigma z_{1-\frac{\alpha}{2}} \leq 0\right) \\
    &=& P_\theta \left( \frac{\bar X - \bar Y}{\sigma} \geq z_{1-\frac{\alpha}{2}} \vee
    \frac{\bar X - \bar Y}{\sigma} \leq z_{\frac{\alpha}{2}}\right).
\end{eqnarray*}
Sei nun $Z\sim\mathcal N(0,1)$ und $\Delta=\mu_X - \mu_Y$. 
Da $\frac{\bar X -\bar Y}{\sigma}\sim \mathcal N(\mu_X-\mu_Y,1)$ können wir die Gütefunktion des 
Test folgendermassen notieren
\begin{eqnarray*}
    G_\delta (\Delta) &=& P \left( Z \leq z_{\frac{\alpha}{2}} -\Delta \vee
    Z \geq z_{1-\frac{\alpha}{2}} -\Delta \right)  \\
    &=& \Phi\left( z_{\frac{\alpha}{2}} - \Delta\right) 
    + 1 - \Phi\left( z_{1-\frac{\alpha}{2}} -\Delta \right). 
\end{eqnarray*}


%\begin{figure}[htb]
%    \begin{center}
%        \includegraphics[width=\textwidth]{figures/ue-blatt-12-power.pdf}
%    \end{center}
%    \caption{Die Gütefunktion des Tests $\delta(X,0)$ mit Signifikanzniveau $\alpha=0.05$.}
%    \label{fig:power}
%\end{figure}


\begin{lstlisting}[language=R,caption={R Code zum Plotten der Gütefunktion}]
require(ggplot2)
alpha=.05
X=seq(-6,6,0.05)
g <- function(delta) {
    return( pnorm(qnorm(alpha/2) - delta ) +1 -pnorm(qnorm(1-alpha/2)-delta))  
}
qplot(X,g(X), geom="line", xlab="Delta", ylab="Guete")
\end{lstlisting}







\problem{Poisson-Verteilung. Test.}
Seien $X_1,\ldots,X_n$ i.i.d.\ Poisson-verteilt mit unbekanntem Parameter $\lambda>0$.
\begin{enumerate}
    \item Verwenden Sie die natürliche suffiziente Statistik, um einen Test mit
        Signifikanzniveau $\alpha$ für die Hypothese $H_0:
        \lambda\leq\lambda_0$ gegen die Alternative $H_1: \lambda>\lambda_0$ zu
        finden. Konstruieren Sie dazu zunächst einen Test für die Hypothese
        $\lambda=\lambda_0$ und zeigen Sie, dass die Gütefunktion streng
        monoton wachsend in $\lambda$ ist. 
     \item Benutzen Sie den zentralen Grenzwertsatz, um eine Approximation für
         den kritischen Wert zu finden.
     \item Seien $\alpha=0.05$, $n=200$, $\sum_{i=1}^{200} X_i= 2085$ und
         $\lambda_0=10$. Klären Sie, ob die Hypothese $H_0:
         \lambda\leq\lambda_0$ verworfen wird und bestimmen Sie den $p$-Wert.
\end{enumerate}

\solution
Wir konstruieren zunächst einen Test für 
\begin{equation*}
    H_0 : \lambda=\lambda_0 \quad \textrm{gegen} \quad H_1 : \lambda>\lambda_0
\end{equation*}
und werden anschließend zeigen, dass $H_0: \lambda=\lambda_0$ durch
$H_0: \lambda<\lambda_0$ ersetzt werden kann.

Die Wahrscheinlichkeitsfunktion der Poisson-Verteilung ist
\begin{align*}
    p_\lambda(x) &= P\left( X_i=x \right) \\
    &= e^{-\lambda} \frac{\lambda^x}{x!} \\
    &= \exp \left( x \log \lambda -\lambda - \log x! \right),
\end{align*}
woraus wir die natürliche suffiziente Statistik $T(X)= \sum_{i}^{} X_i$ ablesen können.
Betrachten wir also den Test
\begin{equation*}
    \delta_j (X) = 1_{ \left\{ T(X) >j \right\}  } \left( X \right)
\end{equation*}
für $j>0$. Die Güte dieses Tests ist 
\begin{align*}
    G_{\delta_j} (\lambda) &=  E_\lambda \delta(X) \\
    &= P_\lambda \left( \delta(X)=1 \right) \\
    &= P_\lambda \left( T(X) > j \right) \\
    &= 1-P_\lambda \left( T(X) \leq j \right) \\
    &= 1 - \sum_{i=0}^{j} e^{-\lambda n} \frac{\left( \lambda n \right)^i}{i!}, \\
\end{align*}
da $T(X)\sim$ Poiss($n\lambda$). Wir legen $j=j(\alpha)$ so fest, dass der Test $\delta_j$ 
Signifikanzniveau $\alpha$ hat, d.h.\ 
\begin{equation*}
    G_\delta (\lambda_0) = 1 - \sum_{i=0}^{j} e^{-\lambda_0 n} 
    \frac{\left( \lambda_0 n \right)^i}{i!} \leq \alpha.
\end{equation*}
Die Gütefunktion ist streng monoton wachsend, denn die Ableitung nach $\lambda$ 
\begin{align*}
    \frac{\partial}{\partial \lambda} G_\delta (\lambda) &= 
    n e^{-\lambda n} \sum_{i=0}^{j} \frac{\left( \lambda n \right)^i}{i!} 
    - n e^{ -\lambda n } \sum_{i=1}^{j} \frac{ i \left( \lambda n  \right)^{i-1} }{i!} \\
    &= e^{-\lambda n }n \left[ \sum_{i=0}^{j} \frac{\left( \lambda n \right)^i}{i!}   
    - \sum_{i=0}^{j-1} \frac{ \left( \lambda n  \right)^{i} }{i!} \right] > 0 
\end{align*}
ist positiv, solange $\lambda>0$ gilt. Die Hypothese $H_0 : \lambda=\lambda_0$ kann also 
durch $H_0 : \lambda\leq \lambda_0$ ersetzt werden, ohne dass Anpassung des Signifikanzniveaus 
notwendig ist.

Berechnen wir nun eine Approximation für $j=j(\alpha)$ vom zentralen Grenzwertsatz ausgehend.
\begin{align*}
    P_{\lambda_0} \left( T(X) > j \right) &= P_{\lambda_0} \left( \sum_{i}^{} X_i > j \right) \\
    &= P_{\lambda_0} \left( \sum_{i}^{} X_i -\lambda_0 > j - \lambda_0 \right) \\
    &= P_{\lambda_0} \left( \frac{1}{\sqrt{n}} \sum_{i}^{} \frac{ X_i - \lambda_0}{ \sqrt{\lambda_0}} > \frac{j-n\lambda_0}{\sqrt{\lambda_0}}\frac{1}{\sqrt{n}} \right) \\
    &\simeq 1-\Phi \left( \frac{j-n\lambda_0}{\sqrt{\lambda_0}}\frac{1}{\sqrt{n}}  \right). 
\end{align*}
Aus
\begin{equation*}
    1-\Phi \left( \frac{j-n\lambda_0}{\sqrt{\lambda_0 n}} \right) = \alpha
\end{equation*}
erhalten wir
\begin{align*}
    \frac{j-n\lambda_0}{\sqrt{\lambda_0 n}} &=  z_{1-\alpha} \\
    j &=  \sqrt{\lambda_0 n} z_{1-\alpha} + n\lambda_0.
\end{align*}
Für $\alpha=0.05$, $n=200$, $\sum_{i}^{}X_i=2085$ und $\lambda_0=10$ ist $j\simeq 2073.56$,
und somit wird in diesem Fall die Nullhypothese nicht verworfen.

% j <- function(alpha, n, sumXi, lambda) { return(  sqrt(lambda*n)*qnorm(1-alpha) + n*lambda    )  } 
% j(0.05,200,2085,10) 








\problem{Exponentialverteilung. Test.} Seien $X_1,\ldots,X_n$ i.i.d.\ und exponentialverteilt
zum Parameter $\theta$. Der Mittelwert werde mit $\mu=\theta^{-1}$ bezeichnet.
Man interessiert sich für den Test $H_0 : \mu \leq \mu_0$ gegen die Alternative $H_1 : \mu > \mu_0$.
\begin{enumerate}
    \item Sei $c_{1-\alpha}$ das $(1-\alpha)$-Quantil der $\chi^2_{2n}$-Verteilung. Zeigen Sie, dass
        ein Test mit Verwerfungsbereich
        \begin{equation*}
            \left\{ \bar X \geq \frac{\mu_0 c_{1-\alpha}}{2n} \right\}
        \end{equation*}
        ein Test mit Signifikanzniveau $\alpha$ ist.
    \item Bestimmen Sie die Güte des Tests aus Punkt 1 an der Stelle $\mu$.
    \item Zeigen Sie, dass 
        \begin{equation*}
            \Phi\left( \frac{\mu_0 z_\alpha}{\mu} + \sqrt{n} \frac{\mu-\mu_0}{\mu} \right) 
        \end{equation*}
        eine Approximation der Güte des Tests aus Punkt 1 an der Stelle $\mu$ ist, wobei $\Phi$ die
        Verteilungsfunktion und $z_\alpha$ das $\alpha$-Quantil der Standardnormalverteilung bezeichnen.
    \item Gegeben sei folgende Stichprobe:
        \begin{equation*}
            3,150,40,34,32,37,34,2,31,6,5,14,150,27,4,6,27,10,30,27.
        \end{equation*}
        Berechnen Sie den $p$-Wert zum Test aus Punkt 1 und interpretieren Sie diesen für gegebenes $\mu_0=25$.
\end{enumerate}

\solution Die Verteilungen in dieser Aufgabe gehören zur Gamma-Familie, denn
$\chi^2_{2n}=$ Gamma($n,\frac{1}{2}$) und Exp$(\theta) =$ Gamma($1,\theta$). Die Dichte von $\chi^2_n$ ist ja 
\begin{equation*}
    p_{\chi^2_n} (x) = 
    \frac{1}{2^{\frac{n}{2}} \Gamma\left( \frac{n}{2} \right)} x^{ \frac{n}{2} -1}e^{-\frac{x}{2}} 1_{\R_{>0}}(x).
\end{equation*}
Das Signifikanzniveau des Tests ist also
\begin{eqnarray*}
    P_{\mu_0} \left( \bar X \geq \frac{ \mu_0 c_{1-\alpha} }{2n} \right)  
    &=& P_{\mu_0} \left( \frac{2}{\mu_0} \sum_{i}^{} X_i  > c_{1-\alpha}   \right) \\
    &=& P_{\mu_0} \left( 2 \theta_0 \sum_{i}^{} X_i > c_{1-\alpha} \right) = \alpha
\end{eqnarray*}
denn 
\begin{eqnarray*}
    X_i & \sim & \textrm{Exp}(\theta) \\
    \sum_{i}^{} X_i & \sim & \textrm{Gamma}(n,\theta) \\
    2\theta \sum_{i}^{} X_i & \sim & \textrm{Gamma}(n,\frac{1}{2}).
\end{eqnarray*}

Die Güte des Tests ist
\begin{eqnarray*}
    G_\delta (\theta) &=&  E_\theta \delta(X) \\
    &=& P_\theta \left( \bar X  \geq \frac{\mu_0 c_{1-\alpha}}{2n} \right) \\
    &=& P_\theta \left( 2 \sum_{i}^{} X_i \geq \frac{c_{1-\alpha}}{\theta_0} \right).
\end{eqnarray*}
Nachdem $2 \sum_{i} X_i \sim $ Gamma($n, \frac{\theta}{2}$) gilt, können wir 
\begin{equation*}
    Z = 2 \theta \sum_{i}^{} X_i \sim \textrm{Gamma}(n,\frac{1}{2})
\end{equation*}
setzen und erhalten
\begin{equation*}
    G_\delta(\theta) = P \left( Z \geq \frac{\theta}{\theta_0} c_{1-\alpha} \right).
\end{equation*}

Um die Güte des Tests zu approximieren, entwickeln wir zunächst eine approximative 
Version des Tests selbst. Das hat den Vorteil, dass hier das Signifikanzniveau 
genauer kontrolliert werden kann. Wir berechnen zunächst mit Hilfe der Gleichung
\begin{eqnarray*}
    P_{\mu_0} \left( \sum_{i}^{} X_i \geq c \right) \leq \alpha
\end{eqnarray*}
den approximativen Verwerfungsbereich $\left[ c,+\infty \right]$.
\begin{eqnarray*}
    P_{\mu_0} \left( \frac{1}{\sqrt{n}} \sum_{i}^{} \frac{X_i - \mu_0}{\mu_0} \geq \frac{1}{\sqrt{n}} 
    \frac{c -n\mu_0}{ \mu_0}\right) &\leq& \alpha \\
    \Phi \left( \frac{1}{\sqrt{n} } \frac{n \mu_0 - c}{\mu_0}  \right) &=&  \alpha \\
    n \mu_0 - \mu_0 \sqrt{n} z_\alpha &=&  c
\end{eqnarray*}
Daraus errechnet sich wie folgt die approximative Güte des Tests.
\begin{eqnarray*}
    P_\mu \left( \frac{1}{\sqrt{n}} \sum_{i}^{} \frac{X_i - \mu}{\mu} \geq \frac{1}{\sqrt{n}} \frac{c-n \mu}{ \mu}   \right) &=&  \Phi \left( \frac{1}{\sqrt{n}} \frac{n \mu - c}{ \mu} \right) \\
    &=& \Phi \left( \frac{1}{\sqrt{n}} \frac{n \mu - \left( n \mu_0 -\mu_0 \sqrt{n} z_\alpha \right)}{\mu} \right) \\
    &=& \Phi \left( \sqrt{n} \frac{\mu - \mu_0}{\mu} + z_\alpha \frac{\mu_0}{\mu} \right).
\end{eqnarray*}

Für eine feste Beobachtung $X$ ist der $p$-Wert des Tests das kleinste Signifikanzniveau, 
bei welchem der Test die Nullhypothese $H_0$ verwirft. Für die Berechnung des $p$-Werts
benutzen wir die approximative Version des Tests. Dieser Test verwirft $H_0$, wenn
$\sum_{i}^{} X_i \geq c = n \mu_0 - \mu_0 \sqrt{n} z_\alpha$. Das ist äquivalent zu
\begin{eqnarray*}
    \sum_{i}^{} X_i &\geq & n \mu_0 - \mu_0 \sqrt{n} z_\alpha \\ 
    \Phi \left(  - \frac{ \sum_{i}^{} X_i -n \mu_0   }{\sqrt{n} \mu_0} \right) &\leq& \alpha \\
\end{eqnarray*}
Das kleinste $\alpha$, das diese Ungleichung erfüllt, ist der $p$-Wert
\begin{equation*}
    \Phi \left(  - \frac{ \sum_{i}^{} X_i -n \mu_0   }{\sqrt{n} \mu_0} \right).
\end{equation*}
Für die oben angegebene Stichprobe und $\mu_0=25$ ist der $p$-Wert gleich $0.0653$, was als
Evidenz gegen die Nullhypothese interpretiert werden kann.

% X=c(3,150,40,34,32,37,34,2,31,6,5,14,150,27,4,6,27,10,30,27)
% n=length(X); mu_0=25
% pnorm((n*mu_0 - sum(X))/(sqrt(n)*mu_0) )







\problem{Rayleigh-Verteilung. UMP-Test.} Seien $X_1,\ldots,X_n$ i.i.d.\ und Rayleigh-verteilt
zum unbekannten Parameter $\theta>0$, d.h.\ $X_1$ hat die Dichte 
\begin{equation*}
    p_\theta(x) = \frac{x}{\theta^2} e^{- \frac{x^2}{2\theta^2}} 1_{\R_{>0}}(x).
\end{equation*}
\begin{enumerate}
    \item Finden Sie eine optimale Teststatistik $T_n$ für
        \begin{equation*}
            H_0 : \theta\leq 1 \quad \textrm{gegen} \quad H_1 : \theta >1.
        \end{equation*}
    \item Konstruieren Sie unter Benutzung von $T_n$ einen \textsc{ump}-Test mit Signifikanzniveau 
        $\alpha$, wobei der kritische Wert $c$ approximativ mit Hilfe des zentralen Grenzwertsatzes
        bestimmt werden soll.
\end{enumerate}

\solution Der Likelihoodquotient für das Testproblem $H_0: \theta=1$ gegen $H_1 : \theta=\nu$
\begin{eqnarray*}
    L(x,1,\nu) &=& 
    \frac{\prod_i \frac{x_i}{\nu^2} \exp \left( -\frac{x_i^2}{2 \nu^2} \right)}{ \prod_i x_i \exp \left( - \frac{x_i^2}{2} \right)  } \\
    &=& \frac{1}{ \nu^{2n}} \exp\left( \left( \frac{1}{2} -\frac{1}{2 \nu^2} \right) \sum_{i}^{} x_i^2 \right)\\
\end{eqnarray*}
ist eine optimale Teststatistik. $T(X)=f(L(x,1,\nu))= \sum_{i}^{} X_i^2$ 
ist auch eine optimale Teststatistik, denn
\begin{equation*}
    f(\eta) = \frac{ \log \left( \nu^{2n} \eta \right) }{\frac{1}{2} - \frac{1}{2\nu^2}}
\end{equation*}
ist monoton wachsend. Der Test
\begin{equation*}
    \delta_k(X) = 1_{  \left\{ \sum_{i}^{}X_i^2 \geq k \right\} }
\end{equation*}
ist ein \textsc{np}-Test für $H_0: \theta=1$ gegen $H_1 : \theta= \nu$ für alle $\nu>1$.
Daher ist $\delta_k$ auch ein \textsc{ump}-Test für das Testproblem $H_0: \theta=1$ gegen $H_1 : \theta>1$.
Um dieses Resultat auf das Testproblem $H_0: \theta \leq 1$ gegen $H_1 : \theta>1$ zu verallgemeinern, zeigen
wir, dass $G_{\delta_k}(1)\geq G_{\delta_k} (\theta)$ für alle $\theta \in (0,1)$ gilt und somit wird das
Signifikanzniveau auf der nun erweiterten Menge $\Theta_0 = \left\{ \theta\leq 1 \right\}$ gehalten.

Für ein positives $k$ ist 
\begin{eqnarray*}
    P \left( X_i^2 \geq k \right) &=& P \left( X \geq \sqrt{k}  \right) \\
    &=& \int_{\sqrt{k}}^{\infty} \frac{x}{\theta^2} e^{ - \frac{x^2}{ 2 \theta^2}} d x \\
    &=& \int_{\frac{k}{\theta^2}}^{\infty} e^{-z} dz
\end{eqnarray*}
monoton fallend, also gilt $P_\theta \left( X_i^2 \geq k \right) \leq \alpha$ für $\theta\in (0,1)$,
wenn $P_1 \left( X_i^2 \geq k \right)\leq \alpha$ gilt.

Konstruieren wir nun mit Hilfe des zentralen Grenzwertsatzes einen zum Signifikanzniveau 
$\alpha$ gehörigen Verwerfungsbereich $\left\{ \theta>k \right\}$. Zunächst berechnen wir
\begin{eqnarray*}
    E X^{2n} &=&  \int_{0}^{\infty} x^{2n} \frac{x}{\theta^2} \exp \left( - \frac{x^2}{2\theta^2} \right) dx \\
    &=&  \int_{0}^{\infty} \left( 2 \theta^2 z \right)^n e^{-z} dz \\
    &=& 2^n \theta^{2n} \int_{0}^{\infty} z^{(n+1)-1}e^{-z} dz  \\
    &=& 2^n \theta^{2n} \Gamma(n+1) = 2^n \theta^{2n} n!
\end{eqnarray*}
und daraus $E X^2 = 2 \theta^2$, $E X^4 = 8 \theta^4$, $\Var X^2 = 4 \theta^4$ sowie
\begin{eqnarray*}
    P \left( \sum_{i} X_i^2 \geq k \right)&=&  
    P \left( \sum_{i}^{} \frac{X_i^2 - \mu}{\sqrt{n} \sigma} \geq \frac{k -n \mu}{\sqrt{n} \sigma} \right) \\
    &=& P \left( \sum_{i}^{} \frac{X_i^2 - 2}{2\sqrt{n}} \geq \frac{k-2n}{2\sqrt{n}} \right) \\
    &\simeq & \Phi \left( \frac{2n -k}{2 \sqrt{n}} \right). 
\end{eqnarray*}
Schließlich erhalten wir aus $P \left( T(X) \geq k \right) \leq \alpha$ die Approximation
\begin{eqnarray*}
    \Phi\left( \frac{2n - k}{ 2\sqrt{n}}  \right) &\leq & \alpha \\
    k &=& 2n - 2 \sqrt{n} z_\alpha.
\end{eqnarray*}





\problem{Pareto-Verteilung. Optimaler Test.}  Eine Zufallsvariable heißt Pareto-Verteilt
zu den Parametern $k>0,a>0$, falls sie die Dichte 
\begin{equation*}
    p(x) = a k^a x^{-a-1} 1_{\R_{>k}}(x)
\end{equation*}
besitzt. $X_1,\ldots,X_n$ seien i.i.d.\ Pareto$(k,a)$-verteilt, wobei $k=1$ und $a$ unbekannt sei.
Zeigen Sie, dass $T(X_1,\ldots,X_n)= \sum_{i=1}^{n} \log\left( X_i \right)$ eine optimale 
Teststatistik für
\begin{equation*}
    H_0 : \frac{1}{a} \leq \frac{1}{a_0} \quad \textrm{gegen} \quad H_1 : \frac{1}{a} > \frac{1}{a_0}
\end{equation*}
ist.

\solution Substituieren wir zunächst $\tilde a = \frac{1}{a}$. Das Testproblem lässt sich
nun schreiben als $H_0: \tilde a \leq \tilde a_0$ gegen $H_1: \tilde a > \tilde a_0$. Die gemeinsame
Dichte der Zufallsvariablen $X_1,\ldots,X_n$ kann in exponentieller Form notiert werden
\begin{eqnarray*}
    p_{\tilde a} &=& \prod_i \frac{1}{\tilde a} x_i^{ -\frac{1}{\tilde a} -1 } 1_{\R_{>1}}(x_i) \\
    &=& \exp \left( \left( -\frac{1}{\tilde a} -1 \right) \sum_{i}^{}\log x_i
    -n \log \tilde a \right) 1_{\R_{>1}^n}(x_1,\ldots,x_n), \\
\end{eqnarray*}
woraus wir $c(\tilde a)=-\frac{1}{\tilde a} -1$ und $T(X)= \sum_{i}^{} \log X_i$ ablesen können.
Die Funktion $c(\tilde a)$ ist streng monoton wachsend, denn $c'(\tilde a ) >0$ für alle $\tilde a>0$. 
Die Statistik $T(X)$ ist daher optimal für das Testproblem. 




\problem{Poissonverteilung. Optimaler Test.} Seien $X_1, \cdots, X_n$ i.i.d.\
poissonverteilt mit dem Parameter $\lambda>0$ und der Wahrscheinlichkeitsfunktion
\begin{equation*}
    P(X_1 = k) = e^{-\lambda} \frac{ \lambda^k}{k!}, \quad k \in \left\{ 0, 1, \cdots \right\}.
\end{equation*}
Für das Testproblem $H_0 : \lambda=\lambda_0$ gegen $H_1 : \lambda=\lambda_1$
mit $0<\lambda_0<\lambda_1$ soll ein optimaler Test entwickelt werden.
\begin{enumerate}
    \item Berechnen Sie die Likelihood-Quotienten-Statistik
        $L(X,\lambda_0,\lambda_1)$ basierend auf der Beobachtung von $X=\left(
        X_1,\ldots,X_n \right)$.
    \item Zeigen Sie, dass die Statistik $T(X)= \sum_{i=1}^{n} X_i$ ebenfalls
        eine optimale Statistik für dieses Testproblem ist.
    \item Zeigen Sie, dass die Summe $X_1+X_2$ Poissonverteilt mit dem
        Parameter $2 \lambda$ ist und schliessen Sie daraus auf $X_1 + \cdots +
        X_n \sim \text{Poiss}(n \lambda)$. 
    \item Seien nun $\lambda_0=1$ und $\lambda_1=2$. Es wurde folgende Stichprobe beobachtet:
        \begin{lstlisting}
2 1 2 0 5 1 0 1 1 0
        \end{lstlisting}
        Testen Sie mit Hilfe von 
        \begin{equation*}
            \delta_l(X) = 1_{   \left\{ T(X) \geq l \right\}},
        \end{equation*}
        ob die Nullhypothese $H_0 : \lambda_0=1$ bei Signifikanzniveau
        $\alpha=0.05$ verworfen werden kann. Dabei können Sie die nachstehende
        Liste der Werte der Verteilungsfunktion der Poissonverteilung mit dem
        Parameter $\kappa=10$ verwenden.  D.h.\ jede Zeile der Liste
        enthält das Paar  $\left( k, \sum_{i=0}^{k} e^{-\kappa}
        \frac{\kappa^{i}}{i!}\right)$.
% Distribution function of Poisson(10)
\begin{lstlisting}
 0 0.00004539993
 1 0.00049939923
 2 0.00276939572
 3 0.01033605068
 4 0.02925268808
 5 0.06708596288
 6 0.13014142088
 7 0.22022064660
 8 0.33281967875
 9 0.45792971447
10 0.58303975019
11 0.69677614630
12 0.79155647639
13 0.86446442262
14 0.91654152707
15 0.95125959670
16 0.97295839022
17 0.98572238640
18 0.99281349540
19 0.99654565802
\end{lstlisting}
\end{enumerate}

\solution
\begin{enumerate}
    
    \item Die Likelihood-Funktion für dieses Testproblem ist 
        \begin{equation*}
            L(X, \lambda_0, \lambda_1) = \frac{\prod_i e^{-\lambda_1} \frac{\lambda_1^{x_i}}{x_i!}}
            {\prod_i e^{-\lambda_0} \frac{\lambda_0^{x_i}}{x_i!}} =
            \prod_i \left(  \frac{\lambda_1}{\lambda_0} \right)^{x_i} e^{\lambda_0 - \lambda_1}.
        \end{equation*}

    \item Die nachstehenden Statistiken sind ebenfalls optimal, da sie monotone Transformationen der
        Likelihood-Quotienten-Statistik sind.
        \begin{align*}
            T_1(X) &= \left( \frac{\lambda_1}{\lambda_0} \right)^{\sum_{i}^{} x_i} \\
            T (X) &= \sum_{i}^{} x_i.
        \end{align*}
    
    \item Wir berechnen
        \begin{align*}
            P \left( X_1+X_2 = k \right) &= \sum_{i=0}^{k} P \left( X_1=i \wedge X_2 = k-i \right) \\
            &= \sum_{i=0}^{k} e^{-\lambda} \frac{\lambda^i}{i!} e^{-\lambda} \frac{\lambda^{k-i}}{(k-i)!} \\
            &= e^{-2\lambda} \lambda^k \sum_{i=0}^{k} \frac{1}{i! (k-i)!} \\
            \sum_{i=0}^{k} \frac{1}{i! (k-i)!} &= \sum_{i=0}^{k} \binom{k}{i} \frac{1}{k!} \\
            &= \frac{1}{k!} 2^k \\
            P \left( X_1+X_2 = k \right) &= e^{-2\lambda} \frac{ \lambda^k 2^k   }{k!}. 
        \end{align*}
        Das heisst $X_1+X_2\sim\text{Poiss}(2\lambda)$ und $X_1, \cdots, X_n
        \sim \text{Poiss}(n\lambda)$.

    \item Die oben angeführte Stichprobe liefert $n=10$ und $\sum_{i}^{} x_i =
        13$. Den kritischen Bereich des optimalen Tests mit Signifikanzniveau
        $\alpha=0.05$ erhalten wir aus
        \begin{align*}
            P_{\lambda_0} \left( T(X) \geq l \right) & \leq \alpha \\
            P_{\lambda_0} \left( \sum_{i}^{} X_i \geq l \right) 
            &= 1 - P\left( \sum_{i}^{} X_i < l \right).
        \end{align*}
        Die Liste liefert $l=16$. Die Nullhypothese kann also nicht verworfen werden.

\end{enumerate}




\problem{Geometrische Verteilung. Neyman-Pearson-Test.} Seien $X_1,\ldots,X_n$ 
i.i.d.\ geometrisch verteilt mit der Wahrscheinlichkeitsfunktion
\begin{equation*}
    \pi(k) = \bP(X_i = k) = p \left( 1-p \right)^k, \quad k\in\left\{ 0,1,2, \ldots \right\}
\end{equation*}
und Parameter $p\in \left( 0,1 \right)$.
Für das Testproblem $H_0 : p=p_0$ gegen $H_1 : p=p_1$ mit $1>p_0 > p_1 >0$ 
soll ein Neyman-Pearson-Test
\begin{equation*}
    \delta_k^\star (X) = 1_{ \left\{ L(x,p_0,p_1)\geq k \right\}  }
\end{equation*}
entwickelt werden.
\begin{enumerate}
    \item Berechnen Sie die Likelihood-Quotienten-Statistik $L(X,p_0,p_1)$ basierend auf der
        Beobachtung von $\left( X_1,\ldots,X_n \right)$.
    \item Zeigen Sie, dass die Statistik $T(X)= \sum_{i=1}^{n} X_i$ ebenfalls eine
        optimale Statistik für dieses Testproblem ist.
    \item Seien nun $p_0=\frac{2}{3}$ und $p_1=\frac{1}{3}$. Es wurde folgende Stichprobe ($n=7$) beobachtet:
        \begin{lstlisting}
1 0 1 0 0 0 4
        \end{lstlisting}   % rgeom(7, 0.66666666)
        Testen Sie mit Hilfe von 
        \begin{equation*}
            \delta_l(X) = 1_{   \left\{ T(X) \geq l \right\}},
        \end{equation*}
        ob die Nullhypothese $H_0 : p_0=\frac{2}{3}$ bei Signifikanzniveau $\alpha=0.05$
        verworfen werden kann. In der nachstehenden Tabelle finden Sie die Werte der Verteilungsfunktion
        von $T(X)$ für $n=7$ und $p=\frac{2}{3}$. D.h.\ jede Zeile enthält ein Paar 
        $\left( k, \bP_{\frac{2}{3}} \left( T(X) \leq k \right) \right)$.
        \begin{lstlisting}
0   0.05852766
1   0.1950922
2   0.3771783
3   0.5592643
4   0.7110027
5   0.8222775
6   0.8964608
7   0.9423837
8   0.9691721
9   0.9840545
10  0.9919918
\end{lstlisting}   % for( q in 0:25 ) { print(c(q, pnbinom(q, 7, 0.66666666666)))     }         
\emph{Bemerkung.} $T(X)$ ist negativ binomialverteilt mit Parametern $\left( n,p \right)$.
\end{enumerate}

\solution
\begin{enumerate}
    \item Die Wahrscheinlichkeitsfunktion von dem Vektor $\left( X_1, \ldots, X_n \right)$ ist
        \begin{eqnarray*}
            \pi\left( k_1, \ldots, k_n \right) &=& \prod_i p(1-p)^{k_i} \\
            &=& p^n (1-p)^{\sum_{i}^{} k_i}.
        \end{eqnarray*}
        Somit ist die Likelihood-Quotienten-Statistik gegeben durch
        \begin{eqnarray*}
            L( (k_i), p_0, p_1) &=& \frac{p_1^n (1-p_1)^{\sum_{i} k_i}}{ p_0^n (1-p_0)^{\sum_{i} k_i}} \\
            &=& \left( \frac{p_1}{p_0} \right)^n \left( \frac{1-p_1}{1-p_0} \right)^{\sum_{i} k_i}.
        \end{eqnarray*}
    \item Da 
        \begin{equation*}
            \frac{1-p_1}{1-p_0} > 1,
        \end{equation*}
        entsteht $T(X) = \sum_{i}^{} X_i$ aus $L( (X_i), p_0, p_1 )$ durch eine monotone Transformation.
        Damit ist $T(X)$ ebenfalls eine optimale Statistik für dieses Testproblem.
    \item Wir berechnen zunächst den zu dem Signifikanzniveau $\alpha=0.05$ gehörigen Verwerfungsbereich
        $[l, +\infty)$. 
        \begin{eqnarray*}
            \bP_{p_0} \left( T(X) \geq l \right) & \leq & \alpha \\
            \bP_{p_0} \left( \sum_{i}^{} X_i < l \right) & \geq & 1-\alpha \\ 
            \bP_{p_0} \left( \sum_{i}^{} X_i \leq l-1 \right) & \geq & 0.95 \\ 
        \end{eqnarray*}
        Aus der Tabelle lesen wir $l-1=8$ ab und somit $l=9$. Nachdem $T(X)=6$, kann die Nullhypothese
        nicht verworfen werden.
\end{enumerate}







\problem{Bernoulli-Verteilung. Neyman-Pearson-Test.} Seien $X_1,\ldots,X_n$ 
i.i.d.\ Bernoulli($p$) Zufallsvariablen mit Wahrscheinlichkeitsfunktion
\begin{equation*}
    P(X_i = k) = p^{k} \left( 1-p \right)^{1-k}, \quad k\in\left\{ 0,1 \right\}.
\end{equation*}
Für das Testproblem $H_0 : p=p_0$ gegen $H_1 : p=p_1$ mit $0<p_0<p_1<1$ 
soll ein Neyman-Pearson-Test
\begin{equation*}
    \delta_k^\star (X) = 1_{ \left\{ L(x,p_0,p_1)\geq k \right\}  }
\end{equation*}
entwickelt werden.
\begin{enumerate}
    \item Berechnen Sie die Likelihood-Quotienten-Statistik $L(X,p_0,p_1)$ basierend auf der
        Beobachtung von $\left( X_1,\ldots,X_n \right)$.
    \item Zeigen Sie, dass die Statistik $T(X)= \sum_{i=1}^{n} X_i$ ebenfalls eine
        optimale Statistik für dieses Testproblem ist.
    \item Seien nun $p_0=\frac{1}{2}$ und $p_1=\frac{3}{4}$. Es wurde folgende Stichprobe beobachtet:
        \begin{lstlisting}
0 1 1 0 1 0 1 1 1 1 1 1 1
        \end{lstlisting}
        Testen Sie mit Hilfe von 
        \begin{equation*}
            \delta_l(X) = 1_{   \left\{ T(X) \geq l \right\}},
        \end{equation*}
        ob die Nullhypothese $H_0 : p_0=\frac{1}{2}$ bei Signifikanzniveau $\alpha=0.05$
        verworfen werden kann. Dabei können Sie die nachstehende Liste der Paare 
        $\left( k, \sum_{i=k}^{n} \binom{n}{i} p_0^i \left( 1-p_0 \right)^{n-i} \right)$ verwenden.
        \begin{lstlisting}
0  1
1  0.9998779296875
2  0.998291015625
3  0.98876953125
4  0.953857421875
5  0.8665771484375
6  0.70947265625
7  0.5
8  0.29052734375
9  0.1334228515625
10 0.046142578125
11 0.01123046875
12 0.001708984375
13 0.0001220703125
\end{lstlisting}            

\end{enumerate}


\solution
\begin{enumerate}

    \item Die Likelihood-Quotienten-Statistik ist 
        \begin{eqnarray*}
            L(x,p_0,p_1) &=& \frac{ \prod_{i} p_1^{x_i}\left( 1-p_1 \right)^{1-x_i} }{ \prod_{i} p_0^{x_i}\left( 1-p_0 \right)^{1-x_i} } \\
            &=& \left( \frac{p_1}{p_0} \right)^{\sum_{i} x_i} \left( \frac{1-p_1}{1-p_0} \right)^{n - \sum_{i} x_i} \\ 
            &=& \left( \frac{1-p_1}{1-p_0} \right)^n 
            \left( \frac{ p_1 \left( 1-p_0 \right)}{ p_0 \left( 1-p_1 \right)} \right)^{\sum_{i}^{} x_i}.
        \end{eqnarray*}
    \item Optimale Statistiken sind auch
        \begin{eqnarray*}
            T_1(X) &=&  \left( \frac{ p_1 \left( 1-p_0 \right)}{ p_0 \left( 1-p_1 \right)} \right)^{\sum_{i}^{} x_i} \\
            T_2(X) &=& \left( \sum_{i}^{} x_i \right) \log \frac{ p_1 \left( 1-p_0 \right)}{ p_0 \left( 1-p_1 \right)}. 
        \end{eqnarray*}
        Die Ungleichung $p_1(1-p_0)> p_0(1-p_1)$ impliziert
        \begin{equation*}
            \log \frac{ p_1 \left( 1-p_0 \right)}{ p_0 \left( 1-p_1 \right)} >0 
        \end{equation*}
        und
        \begin{eqnarray*}
            T(X) &=& \sum_{i}^{} x_i
        \end{eqnarray*}
        ist ebenfalls optimale Statistik.
    \item Für die angegebene Beobachtung ist $T(X)=10$. Der zum Signifikanzniveau $\alpha=0.05$ 
        gehörige kritische Wert folgt aus
        \begin{eqnarray*}
            P_{p_0} \left( T(X) \geq k \right) &\leq& \alpha \\
            P_{p_0} \left( \sum_{i=1}^{n} X_i \geq k \right) &\leq& \alpha \\
            \sum_{i=k}^{n} \binom{n}{i} p_0^i \left( 1-p_0 \right)^{n-i} & \leq & \alpha
        \end{eqnarray*}
        Laut Tabelle wählen wir $k=10$. Die Nullhypothese wird also verworfen.
\end{enumerate}





\section{Lineare Regression}


\problem{Lineare Regression mit Schalter.} Gegeben sei das Model
\begin{align*}
    Y_i &=  \beta_0 x_i + \varepsilon_i \quad\quad i=1,\ldots ,m \\
    Y_i &= \beta_1 + \varepsilon_i \quad\quad  i=m+1, \ldots , m+n
\end{align*}
mit $m,n \in \bN$ und $\varepsilon_i \in \cN \left( 0, \sigma^2 \right)$, i.i.d.\ $\sigma>0$.
\begin{enumerate}
    \item Geben Sie die Designmatrix $X$ für dieses Model explizit an.
    \item Berechnen Sie den Kleinste-Quadrate-Schätzer $\left( \hat \beta_0, \hat \beta_1 \right)$ 
        basierend auf der Beobachtung von  
        $\left( Y_1, \ldots , Y_{m+n} \right)$ und $\left( X_1, \ldots , X_m \right)$.
    \item Berechnen Sie den Kleinste-Quadrate-Schätzer $\left( \hat \beta_0,
        \hat \beta_1 \right)$ basierend auf der Beobachtung von
        \begin{align*}
            Y &= \left( 1,4,2,8,6,0 \right)^\top \\
            X &= \left( 3,2,1 \right)^\top.
        \end{align*}
\end{enumerate}

\solution
\begin{enumerate}
    \item Die Desingmatrix für dieses Model ist
            \begin{equation*}
                X = \left(
                \begin{array}{cc}
                    x_1 & 0 \\
                    \vdots & \vdots \\
                    x_m & 0 \\
                    0 & 1 \\
                    \vdots & \vdots \\
                    0 & 1 
                \end{array}
                \right).
            \end{equation*}
        \item Die allgemeine Form des LSE ist 
            $\hat \beta = \left( X^\top X \right)^{-1} X^\top Y$. Wir berechnen
            \begin{align*}
                X^\top X &= \left(
                \begin{array}{cc}
                    \sum_{i=1}^{m} x_i^2 & 0 \\
                    0 & n 
                \end{array}
                \right) \\
                \left( X^\top X \right)^{-1} &= \left(
                \begin{array}{cc}
                    \frac{1}{\sum_{i=1}^{m} x_i^2} & 0 \\
                    0 & \frac{1}{n} 
                \end{array}
                \right) \\
                \left( X^\top X \right)^{-1} X^\top &= \left(
                \begin{array}{cccccc}
                    \frac{x_1}{\sum_{i=1}^{m} x_i^2} & \cdots & \frac{x_m}{\sum_{i=1}^{m} x_i^2} & 
                        0 & \cdots & 0 \\
                        0& \cdots &0& \frac{1}{n} & \cdots & \frac{1}{n} 
                \end{array}
                \right) \\
                \left( X^\top X \right)^{-1} X^\top Y &= 
                \left(
                \begin{array}{c}
                    \frac{1}{\sum_{i=1}^{m} x_i^2} \left( x_1 y_1 + \ldots + x_m y_m \right)  \\
                    \frac{1}{n} \left( y_{m+1}+ \ldots + y_{m+n} \right)
                \end{array}
                \right) = \left(
                \begin{array}{c}
                    \hat\beta_0 \\ \hat\beta_1
                \end{array}
                \right).
            \end{align*}
        \item Die Beobachtung impliziert $m=3$ und $n=3$. Mit der obigen Formel erhalten wir
            $\hat \beta_0 = \frac{13}{14}$ und $\hat \beta_1 = \frac{14}{3}$.
\end{enumerate}






\problem{$3$-Stichprobenproblem. } 
Gegeben sei das Modell
\begin{equation*}
    Y_{kl} = \beta_k + \varepsilon_{kl}
\end{equation*}
mit $k\in \left\{ 1,2,3 \right\}$, $l\in \left\{ 1,2 \right\}$ und 
$\varepsilon_{kl}\sim \mathcal N(0,\sigma^2)$, i.i.d.\  $\sigma>0$.
\begin{enumerate}
    \item Geben Sie die Designmatrix $X$ für dieses Model explizit an.
    \item Geben Sie eine koordinatenfreie Darstellung dieses Models an.
    \item Berechnen Sie Kleinste-Quadrate-Schätzwerte basierend auf der Beobachtung
        \begin{equation*}
            \left( Y_{11},Y_{12},Y_{21},Y_{22},Y_{31},Y_{32} \right) = \left(74, 92, 16, 22, 98, 16 \right).
        \end{equation*}
\end{enumerate}


\solution
\begin{enumerate}

    \item Das Gleichungsystem in der Kurzschreibweise:
        \begin{equation*}
            Y = X \beta + \varepsilon
        \end{equation*}
        Dabei ist $Y= \left( Y_1,Y_2,Y_3 \right)^\top$, $\beta = \left( \beta_1,\beta_2,\beta_3 \right)^\top$,
        $\varepsilon \sim\mathcal N\left( 0, \sigma^2 I_6 \right)$
        und die Designmatrix
        \begin{equation*}
            X = \left( {\begin{array}{ccc} 1&0&0\\ 1&0&1\\ 0&1&0\\ 0&1&0\\ 0&0&1\\ 0&0&1 \end{array}   } \right).
        \end{equation*}

    \item Koordinatenfreie Darstellung ist $Y = \zeta + \varepsilon$ mit 
        \begin{equation*}
            \zeta^\top = \left( \beta_1,\beta_1,\beta_2,\beta_2,\beta_3,\beta_3 \right).
        \end{equation*}

    \item Wegen Rang$(X)=p$ ist der Kleinste-Quadrate-Schätzer für $\beta$ gegeben durch 
        $\hat \beta = \left( X^\top X \right)^{-1} X^\top Y$. Das führt uns auf 
        \begin{eqnarray*}
            \hat\beta_1 &=&  \frac{1}{2} \left( Y_1+Y_2 \right)    \\
            \hat\beta_2 &=&  \frac{1}{2} \left( Y_3+Y_4 \right)    \\
            \hat\beta_3 &=&  \frac{1}{2} \left( Y_5+Y_6 \right).
        \end{eqnarray*}
        Entsprechende Werte sind also zu mitteln.

\end{enumerate}



\problem{KQS ist auch MLS im Normalverteilungsfall. }
Zeigen Sie, dass der Kleinste-Quadrate-Schätzer $\hat \beta$ auch Maximum-Likelihood-Schätzer 
im allgemeinen linearen Modell ist, falls $\varepsilon \sim \mathcal N(0, \sigma^2 I_n)$.

\solution Betrachten wir ein allgemeines lineares Model
\begin{equation*}
    Y_i =  X_i \beta+ \varepsilon_i, \quad i=1,\ldots,n.
\end{equation*}
Dabei ist $\beta$ der $p$-dimensionale Vektor der Modelparameter, den wir schätzen wollen,
$X_i$ sind Zeilen einer bekannten Matrix $X\in\R^{n\times p}$ und 
$\left( \varepsilon_1, \ldots, \varepsilon_n \right)$ sind i.i.d. mit 
$\varepsilon_i \in\mathcal N(0,\sigma^2)$, $\sigma>0$. Die Beobachtung
$Y=\left( Y_1,\ldots,Y_n \right)$ ist demnach multivariat normalverteilt, d.h.\ 
$Y\sim\mathcal N(X\beta,\sigma^2 I_n)$. $Y_i$ sind i.i.d.\ mit
$Y_i\sim\mathcal N(X_i \beta, \sigma^2 )$ und die Likelihoodfunktion der Beobachtung 
$y=\left( y_1,\ldots, y_n \right)$ ist 
\begin{eqnarray*}
    L(Y, \beta) &=&  \prod_i \frac{1}{ \sqrt{2\pi} \sigma} 
    \exp\left( - \frac{1}{2} \left( \frac{y_i - X_i \beta }{\sigma}  \right)^2 \right) \\
    &=& \left( \frac{1}{\sqrt{2\pi}\sigma} \right)^n 
    \exp \left( -\frac{1}{2 \sigma^2} \sum_{i}^{} \left( y_i - X_i\beta \right)^2 \right).
\end{eqnarray*}
Bis auf die positive multiplikative Konstante $\left( \sqrt{2 \pi}\sigma \right)^{-n}$ ist 
die Log-Likelihood-Funktion gleich
\begin{eqnarray*}
    l(Y, \beta) &=& -\frac{1}{2 \sigma^2}  \sum_{i}^{} \left( y_i - X_i \beta \right)^2 
\end{eqnarray*}
Die Maximierung von $l(Y, \beta)$ ist gleichbedeutend mit der Minimierung von 
\begin{equation*}
    \sum_{i}^{} \left( y_i - X_i \beta \right)^2 = \| y - X \beta \|^2, 
\end{equation*}
was genau die Eigenschaft des Kleinste-Quadrate-Schätzers ist. 

Beachten Sie, dass wir in den obigen Überlegungen nicht
$\textrm{Rang}\left( X \right)=p$ annehmen mussten.






\problem{Einfache lineare Regression. } Seien folgende Beobachtungen von
$(x_i)$ und $(y_i)$ 
gegeben.

\begin{lstlisting}
    X   26  23  27  28  24  25
    Y   179 150 160 175 155 150
\end{lstlisting}
\begin{enumerate}
    \item Berechnen Sie die Kleinste-Quadrate-Schätzer für $\beta_0$ und $\beta_1$
        der einfachen linearen Regression $Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$.
    \item Geben Sie die Designmatrix $X$ für dieses Model explizit an.
    \item Veranschaulichen Sie die Daten und die Regressionsgerade graphisch.
\end{enumerate}

\solution Das betrachtete lineare Model hat die Form
\begin{equation*}
    Y_i = X_i \beta + \varepsilon_i, \quad i=1,\ldots, 6
\end{equation*}
mit der Designmatrix
$$ X^\top = \left(\begin{matrix} 1&1&1&1&1&1\cr 26&23&27&28&24&25\cr \end{matrix} \right) $$
und der Zielvariable
\begin{equation*}
    Y^\top = \left( \begin{matrix} 179&150&160&175&155&150\cr   \end{matrix}  \right).
\end{equation*}
Die Designmatrix hat also den vollen $\textrm{Rang}(X)=2$ und der Kleinste-Quadrate-Schätzer
ist 
\begin{equation*}
    \hat \beta = \left( X^\top X \right)^{-1} X^\top Y.
\end{equation*}
Die Berechnung von $\hat \beta$ kann wie folgt erbracht werden. 
\begin{eqnarray*}
    X^\top X &=&  
    \left( {
    \begin{array}{cc}
        6 & 153 \\ 153 & 3919
    \end{array} }
    \right) \\
    \left(  X^\top X\right)^{-1} &=& \frac{1}{105} 
    \left( {
    \begin{array}{cc} 
        3919&-153 \\ -153&6
    \end{array} }
    \right) \\
    \left(  X^\top X\right)^{-1} X^\top Y &=& \left(
    \begin{array}{c}
        \frac{1343}{35} \\ \frac{169}{35}
    \end{array}
    \right)
\end{eqnarray*}
Wir erhalten also 
\begin{eqnarray*}
    \beta_0 &=&  38.37142857142857 \\
    \beta_1 &=&  4.828571428571428
\end{eqnarray*}

Die Daten und die Regressionsgerade können mit Hilfe des folgenden R-Codes veranschaulicht werden. 
\begin{lstlisting}
X=c( 26, 23, 27, 28, 24, 25 )
Y=c( 179, 150, 160, 175, 155, 150  )
b_0 = 38.37142857142857
b_1 = 4.828571428571428

t= seq(23,28,0.01)

plot(X,Y); lines(t, b_0 + b_1 * t, lw=2)
\end{lstlisting}

%\begin{figure}[htb]
%    \begin{center}
%        \includegraphics[width=\textwidth]{figures/ue-blatt-14-reg.pdf}
%    \end{center}
%    \caption{Regression von $X$ gegen $Y$}
%    \label{fig:reg}
%\end{figure}





\problem{Quadratisches Modell. KQS.} Es sei das Modell 
\begin{equation*}
    Y_i = \beta_1 x_i + \beta_2 x_{i}^2 + \varepsilon_i
\end{equation*}
für $i=1,\ldots,n$ gegeben. Die zufälligen Fehler $\varepsilon_1,\ldots,\varepsilon_n$ 
seien i.i.d.\ und $\varepsilon_i\sim \mathcal N(0,\sigma^2)$ mit $\sigma>0$. 
$\left(\beta_1, \beta_2 \right)$ sind die unbekannten Parameter. 
\begin{enumerate}
    \item Geben Sie die Designmatrix $X$ für dieses Model explizit an.
    \item Berechnen Sie den Kleinste-Quadrate-Schätzer $\left( \hat\beta_1,
        \hat\beta_2 \right)$ basierend auf der Beobachtung von
        \begin{align*}
            X &= \left( 2,0,1,-1 \right)^\top \\
            Y &= \left( 3,1,-1,1 \right)^\top.
        \end{align*}
        Für diese Beobachtung gilt ausserdem
        \begin{equation*}
            \left( 
                \begin{matrix}
                    \sum_{i}^{}x_i^2 & \sum_{i}^{} x_i^{3} \\
                    \sum_{i}^{} x_i^3 & \sum_{i}^{} x_i^4 \\
                \end{matrix}
            \right)^{-1} = 
            \frac{1}{22} \left( 
            \begin{matrix}
                9 & -4 \\
                -4 & 3 \\
            \end{matrix}
            \right).
        \end{equation*}
\end{enumerate}

\solution Das Modell lässt sich in der Form
\begin{equation*}
    Y = X \beta + \varepsilon
\end{equation*}
schreiben, wobei die Designmatrix durch 
\begin{equation*}
    X^\top = \left(
    \begin{array}{cccc}
        x_1     & x_2   & \ldots & x_n \\
        x_1^2   & x_2^2 & \ldots & x_n^2
    \end{array}
    \right)
\end{equation*} 
gegeben ist. Für $\textrm{Rang}(X) = 2$ ist der Kleinste-Quadrate-Schätzer
gegeben durch $\hat \beta = \left( X^{\top} X \right)^{-1} x^{\top} Y$. Die
Auswertung dieser Formel liefert
\begin{equation*}
    X^\top X = \left(
    \begin{array}{ccc}
        \sum_{i}^{} x_i^2 & \sum_{i}^{} x_i^3 \\
        \sum_{i}^{} x_i^3 & \sum_{i}^{} x_i^4 \\
    \end{array}
    \right)
\end{equation*}
und 
\begin{equation*}
    \left( X^{\top} X \right)^{-1} X^{\top} Y = \frac{1}{11} \left( 
    \begin{matrix}
        -6 \\ 10
    \end{matrix}
    \right).
\end{equation*}




\problem{Quadratisches Modell. Explizite Form des KQS.}  Es sei das Modell
\begin{equation*}
    Y_i = \beta_0 + \beta_1 x_i + \beta_2 x_{i}^2 + \varepsilon_i
\end{equation*}
für $i=1,\ldots,n$ gegeben. Die zufälligen Fehler $\varepsilon_1,\ldots,\varepsilon_n$ 
seien i.i.d.\ und $\varepsilon_i\sim \mathcal N(0,\sigma^2)$ mit $\sigma>0$.
Geben Sie die Kleinste-Quadrate-Schätzer für $\beta_i$, $i\in \left\{ 0,1,2 \right\}$ explizit an.

\solution Das Modell lässt sich in der Form
\begin{equation*}
    Y = X \beta + \varepsilon
\end{equation*}
schreiben. Wir nehmen an, dass die Beobachtungen $\left( x_1,\ldots, x_n \right)$ 
voneinander verschieden sind und erhalten die Designmatrix
\begin{equation*}
    X^\top = \left(
    \begin{array}{cccc}
        1       & 1     & \ldots & 1 \\
        x_1     & x_2   & \ldots & x_n \\
        x_1^2   & x_2^2 & \ldots & x_n^2
    \end{array}
    \right)
\end{equation*} 
mit $\textrm{Rang}(X) = 3$. Daraus ergibt sich
\begin{equation*}
    X^\top X = \left(
    \begin{array}{ccc}
        \sum_{i}^{} x_i^0 & \sum_{i}^{} x_i^1 & \sum_{i}^{} x_i^2 \\
        \sum_{i}^{} x_i^1 & \sum_{i}^{} x_i^2 & \sum_{i}^{} x_i^3 \\
        \sum_{i}^{} x_i^2 & \sum_{i}^{} x_i^3 & \sum_{i}^{} x_i^4 \\
    \end{array}
    \right)
\end{equation*}
und mit der Bezeichnung $T_k = \sum_{i}^{} x_i^k$ 
\begin{eqnarray*}
    \Delta &=&  \det \left( X^\top X \right) \\
    &=& { T_0}\,\left({ T_2}\,{ T_4}-{ T_3}^2\right)-{ T_1}\,\left({ T_1}\,{ T_4}-{ T_2}\,{ T_3}\right)+{ T_2}\,\left({ T_1}\,{ T_3}-{ T_2}^2\right).
\end{eqnarray*}
Die Regel von Cramer liefert die Inverse der Matrix $X^\top X$, nämlich
\begin{eqnarray*}
    \left( X^\top X \right)^{-1} &=& \Delta^{-1} \left(
    \begin{array}{ccc}
{ T_2}\,{ T_4}-{ T_3}^2&{ T_2}\,{ T_3}-
 { T_1}\,{ T_4}&{ T_1}\,{ T_3}-{ T_2}^2\cr { T_2}\,
 { T_3}-{ T_1}\,{ T_4}&{ T_0}\,{ T_4}-{ T_2}^2&
 { T_1}\,{ T_2}-{ T_0}\,{ T_3}\cr { T_1}\,{ T_3}-
 { T_2}^2&{ T_1}\,{ T_2}-{ T_0}\,{ T_3}&{ T_0}\,
 { T_2}-{ T_1}^2
    \end{array}
    \right). 
\end{eqnarray*}
Sei $Z = X^\top Y$, d.h.\
\begin{equation*}
    Z = \left(
    \begin{array}{c}
        Z_1 \\ Z_2 \\ Z_3
    \end{array}
    \right) = \left(
    \begin{array}{c}
        \sum_{i}^{} Y_i \\ \sum_{i}^{} Y_i x_i \\ \sum_{i} Y_i x_i^2
    \end{array}
    \right).
\end{equation*}
Der Kleinste-Quadrate-Schätzer 
$\hat \beta = \left( X^\top X \right)^{-1} X^\top Y = \left( X^\top X \right)^{-1} Z$ ist dann explizit
gegeben durch
\begin{equation*}
    \beta_0 = 
\frac{\left({ T_1}\,{ T_3}-{ T_2}^2\right)\,{ Z_2}+
 \left({ T_2}\,{ T_3}-{ T_1}\,{ T_4}\right)\,{ Z_1}+
 \left({ T_2}\,{ T_4}-{ T_3}^2\right)\,{ Z_0}}{
 \left({ T_0}\,{ T_2}-{ T_1}^2\right)\,{ T_4}-{ T_0}\,
 { T_3}^2+2\,{ T_1}\,{ T_2}\,{ T_3}-{ T_2}^3},
\end{equation*}
\begin{equation*}
    \beta_1 = 
-  \frac{\left({ T_0}\,{ T_3}-{ T_1}\,{ T_2}\right)\,
 { Z_2}+\left({ T_2}^2-{ T_0}\,{ T_4}\right)\,{ Z_1}+
 \left({ T_1}\,{ T_4}-{ T_2}\,{ T_3}\right)\,{ Z_0}
 }{\left({ T_0}\,{ T_2}-{ T_1}^2\right)\,{ T_4}-
 { T_0}\,{ T_3}^2+2\,{ T_1}\,{ T_2}\,{ T_3}-{ T_2}^
 3},
\end{equation*}
und
\begin{equation*}
    \beta_2 = 
 \frac{\left({ T_0}\,{ T_2}-{ T_1}^2\right)\,{ Z_2}+
 \left({ T_1}\,{ T_2}-{ T_0}\,{ T_3}\right)\,{ Z_1}+
 \left({ T_1}\,{ T_3}-{ T_2}^2\right)\,{ Z_0}}{
 \left({ T_0}\,{ T_2}-{ T_1}^2\right)\,{ T_4}-{ T_0}\,
 { T_3}^2+2\,{ T_1}\,{ T_2}\,{ T_3}-{ T_2}^3}.
\end{equation*}



\section{Prüfungsaufgaben}

\problem{Invers-Gauß-Verteilung als exponentielle Familie.}
Seien $X_1,\ldots,X_n$ i.i.d.\ Invers-Gauß-verteilte Zufallsvariablen gegeben. D.h.\ die Dichte
von $X_i$ ist 
\begin{equation*}
    \sqrt{\frac{\lambda}{2\pi}} x^{-\frac{3}{2}} \exp\left( \frac{-\lambda(x-\mu)^2}{2\mu^2 x} \right) 1_{\R_{>0}} (x)
\end{equation*}
wobei $\mu>0$, $\lambda>0$ und $\mu$ bekannt und fix sei.
\begin{enumerate}
    \item Zeigen Sie, dass die von dem Zufallsvektor $( X_1,\ldots,X_n)$
        erzeugte Familie von Verteilungen $\left\{ P_\lambda : \lambda>0
        \right\}$ eine exponentielle Familie ist, indem Sie die gemeinsame
        Dichte des Vektors $(X_1,\ldots,X_n)$ in die Form
        \begin{equation*}
            p_\lambda(x) = \exp\left( c(\lambda) T(x) + d(\lambda) + S(x) \right)1_A (x)
        \end{equation*}
        bringen. Geben Sie dabei explizit $c,T,d,S$ und $A$ an.
    \item Zeigen Sie mit Hilfe des Faktorisierungssatzes, dass die Statistik
        $T(X)$ suffizient ist.
\end{enumerate}

\solution Die gemeinsame Dichte von $X_1, \cdots, X_n$ 
lässt sich in folgender Form darstellen.
\begin{equation*}
    \exp\left(  -\frac{\lambda}{2}\left( \frac{x^2}{\mu^2} + \frac{1}{x} \right)    
    + \log x^{-\frac{3}{2}} + \log \sqrt{\frac{\lambda}{2\pi}} + 
    \lambda\mu  \right)1_{\R_{>0}}(x).
\end{equation*}



\problem{Pareto-Verteilung. Suffizienz und MLE.} Seien $X_1,\ldots,X_n$ i.i.d.\ und
Paretoverteilt mit der Dichte
\begin{equation*}
    p_\theta(x)= \frac{\theta a^\theta}{x^{\theta+1}}1_{\R_{>a}}(x),
\end{equation*}
wobei $\theta>0$ und $a$ fix und bekannt sei.
\begin{enumerate}
    \item Finden Sie eine reellwertige suffiziente Statistik $T(X_1,\ldots,X_n)$ für $\theta$.
    \item Finden Sie den Maximum-Likelihood-Schätzer $\hat \theta(X_1,\ldots,X_n)$ für $\theta$.
    \item Zeigen Sie, dass der von Ihnen gefundene Schätzer $\hat \theta$ suffizient ist.
\end{enumerate}

\solution
Suffiziente Statistik $T= \prod_i \frac{1}{x_i}$ erhalten wir aus 
\begin{equation}
    p_\theta(X_1,\ldots,X_n) = 
    \theta^n a^{n\theta} \left( \prod_i \frac{1}{x_i} \right)^{\theta-1} \prod_i 1_{\R_{>a}}(x_i)
\end{equation}
mit Hilfe des Faktorisierungstheorems.

Die Berechnung folgt der üblichen Prozedur des Minimierens der Log-Likelihood-Funktion.
\begin{eqnarray}
    \frac{\partial}{\partial \theta} l(x,a,\theta) &=& 
    \frac{n}{\theta} + n \log a -\sum_{j}^{} \log x_i \\
    \frac{\partial^2}{\partial \theta^2} l(x,a,\theta) &=& - \frac{n}{\theta^2} < 0 \\
    \hat\theta &=& \frac{n}{ \sum_{i}^{} \left( \log x_i - \log a \right)} 
    = \frac{n}{ \log \left( \prod_i \frac{x_i}{a}  \right) }
\end{eqnarray}

Die suffiziente Statistik $T=\eta (\hat\theta)$ ist eine Funktion des MLE, wobei 
\begin{equation}
    \eta(x) = \frac{1}{ \exp\left( \frac{n}{x} \right) a^n}.
\end{equation}
Daher ist der MLE $\hat\theta$ auch suffizient.



\problem{Diskrete Gleichverteilung. MLE und UMVUE.} Seien $X_1,\ldots,X_n$ i.i.d.\ 
diskret Gleichverteilt auf der Menge $\left\{ 1,2,\ldots,\theta \right\}$. 
\begin{enumerate}
    \item Zeigen Sie, dass der Maximum-Likelihood-Schätzer für $\theta$ gerade
        $\hat \theta(X_1,\ldots,X_n) = X_{(n)} = \max \left\{ X_1,\ldots,X_n
        \right\}$ ist.  
    \item Zeigen Sie, dass $\hat \theta = X_{(n)}$ vollständig und suffizient
        jedoch verzerrt ist.
%    \item Bestimmen Sie mit Momentenmethode einen Schätzer der unverzerrt ist.
    \item Konstruieren Sie einen \textsc{umvue}-Schätzer der folgenden Gestalt
        \begin{equation*}
            \frac{ X_{(n)}^{n+1} - ( X_{(n)} - 1)^{n+1}  }{ X_{(n)}^n - (X_{(n)} -1)^n }.
        \end{equation*}
\end{enumerate}

\solution
Die Likelihood-Funktion ist
\begin{equation*}
    L(\theta,x) = \frac{1}{\theta^n} \prod_i 1_{[0,\theta]}(x_i).
\end{equation*}
Für einen fixen Set von i.i.d.\ Beobachtungen $(x_1,\ldots,x_n)$ wird die
Likelihood-Funktion von $\hat\theta(X)=X_{(n)}$ minimiert, denn
\begin{equation*}
    L(\hat\theta(x),x) = \max \left\{ L(\theta,x) : \theta\in\Theta \right\}
\end{equation*}
und für jedes kleinere $\bar\theta<\hat\theta$ ist die Likelihood-Funtion
gleich Null, für alle größeren $\bar\theta$ entfernen wir uns von dem Maximum
wegen der in $\theta$ fallender Funktion $\frac{1}{\theta^n}$.

Der Erwartungswert von $X_{(n)}$ ist kleiner als $\theta$ denn 
\begin{equation*}
    E X_{(n)} = \sum_{X\in\left\{ 1,\ldots,\theta \right\}^n}^{} X_{(n)} \frac{1}{\theta^n}
\end{equation*}
kann abgeschätzt werden, indem $X_{(n)}$ durch $\theta$ ersetzt wird. Das liefert uns
$\E X_{(n)} < \theta$. Wegen
\begin{equation*}
    p_\theta(x) = \frac{1}{\theta^n} 1_{[0,\theta]} \left( X_{(n)} \right)
\end{equation*}
ist $X_{(n)}$ auch suffizient. Für den Beweis der Vollständigkeit setzen wir voraus, dass 
\begin{equation*}
    E_{\theta} g(X_{(n)}) =0
\end{equation*}
für alle $\theta\in\Theta$ und ein messbares $g$ gilt.  Mit $\theta=1$ erhalten
wir $g(1)=0$, $\theta=2$ impliziert $g(2)=0$ und so weiter. Damit folgt $g
\equiv 0$, $P( g(X_{(n)} =0))=1$ und die Vollständigkeit von $X_{(n)}$.

Ein \textsc{umvue}-Schätzer ist eine Funktion $h(X_{(n)})$ mit $E h(X_{(n)})=\theta$.
Diese Überlegung führt uns zu
\begin{align*}
    E h(X_{(n)}) &= 
    \sum_{x\in \left\{ 0,\ldots,\theta \right\}^n } h(x_{(n)}) \frac{1}{\theta^n} = \theta \\
    E h(X_{(n)}) &= \sum_{m=1}^{\theta} h(m)(m^n - (m-1)^n) = \theta^{n+1}
\end{align*}
und
\begin{equation*}
    h(m) = \frac{m^{n+1} - (m-1)^{n+1}}{m^n - (m-1)^{n}}.
\end{equation*}



\problem{Invers-Gamma-Verteilung als exponentielle Familie.}
Seien $X_1,\ldots,X_n$ i.i.d.\ Invers-Gamma-verteilte Zufallsvariablen gegeben.
D.h.\ die Dichte von $X_i$ ist 
\begin{equation*}
    f_{a}(x) = \frac{\lambda^a}{\Gamma(a)}x^{-a-1} e^{-\frac{\lambda}{x}}1_{\R_{\geq 0}}(x),
\end{equation*}
wobei $a>0$ unbekannt und $\lambda>0$ bekannt und fix sei.
\begin{enumerate}
    \item Zeigen Sie, dass die von dem Zufallsvektor $( X_1,\ldots,X_n)$
        erzeugte Familie von Verteilungen $\left\{ P_a : a>0 \right\}$ eine
        exponentielle Familie ist, indem Sie die gemeinsame Dichte des Vektors
        $(X_1,\ldots,X_n)$ in die Form
        \begin{equation*}
            p_a(x) = \exp\left( c(a) T(x) + d(a) + S(x) \right)1_A (x)
        \end{equation*}
        bringen. Geben Sie dabei explizit $c,T,d,S$ und $A$ an.
    \item Zeigen Sie mit Hilfe des Faktorisierungssatzes, dass die Statistik
        $T(X)$ suffizient ist.
\end{enumerate}

\solution
\begin{enumerate}
    \item Die gemeinsame Dichte von $\left( X_1,\ldots,X_n \right)$ ist 
        \begin{equation*}
            p_a(x_1,\ldots,x_n) = 
            \prod_i \frac{\lambda^a}{\Gamma(a)}x_i^{-a-1} e^{-\frac{\lambda}{x_i}}1_{\R_{\geq 0}}(x_i).
        \end{equation*}
        Die Darstellung als exponentielle Familie ist
        \begin{equation*}
            \exp \left( \left( -a-1 \right) \sum_{i}^{} \log x_i + \log \frac{\lambda^{na}}{\Gamma(a)^n} - \lambda \sum_{i}^{} \frac{1}{x_i}  \right) 1_{\R_{\geq 0}^n} (x_1,\ldots,x_n).
        \end{equation*}
        Dabei ist 
        \begin{eqnarray*}
            c(a)&=&  -a-1 \\
            T(X)&=& \sum_{i}^{} \log X_i \\
            d(a)&=& \log \frac{\lambda^{na}}{\Gamma(a)^n} \\
            S(x)&=&  -\lambda \sum_{i}^{} \frac{1}{x_i}\\
            A   &=& \R_{\geq 0}^n.
        \end{eqnarray*}

    \item $T(X)$ ist suffizient, denn die Dichte lässt sich in der Form
        $g(T(x),a)h(x)$ darstellen. Ebenso ist $T(X)$ in jeder exponentiellen
        Familie eine suffiziente Statistik. 
\end{enumerate}



\problem{Beta-Verteilung. MLE.} Seien $X_1,\ldots,X_n$ i.i.d.\ und Beta$(\theta+1, 1)$-verteilt
mit dem Parameter $\theta\in\Theta= \left\{ \eta\in\R : \eta>-1 \right\}$. Die Dichte von $X_i$ ist also
\begin{equation*}
    p_\theta(x) = (\theta+1)x^\theta 1_{ [0,1] }(x).
\end{equation*}
\begin{enumerate}

    \item Bestimmen Sie den Maximum-Likelihood-Schätzer für $\theta$ basierend
        auf der Beobachtung von $\left( X_1,\ldots,X_n \right)$. Zeigen Sie
        dabei, dass der von Ihnen gefundene Schätzer $\hat \theta$ das Maximum
        der Likelihood-Funktion ist. 

    \item Kann der Maximum-Likelihood-Schätzwert $\hat \theta\left( x_1,\ldots,x_n \right)$
        auf dem Rand des Parameterraumes liegen? 
\end{enumerate}

\solution
\begin{enumerate}

    \item Die Likelihoodfunktion für eine Beobachtung $(X_1,\ldots,X_n)$ ist 
        \begin{eqnarray}
            L(\theta,X) &=& \left( \theta+1 \right)^n \left( \prod_i X_i \right)^{\theta}  \\
            l(\theta,X) &=& n \log \left( \theta+1 \right)  + \theta \sum_{i}^{} \log X_i \\
            \frac{\partial}{\partial \theta} l(\theta,X)&=& \frac{n}{\theta+1} + \sum_{i}^{} \log X_i \\
            \frac{\partial^2}{\partial \theta^2} l(\theta,X) &=& 
            - \frac{n}{\left( \theta+1 \right)^2} < 0.
        \end{eqnarray}
        $\frac{\partial}{\partial \theta} l(\theta,X)=0$ liefert
        \begin{equation}
            \hat\theta = -1 - \frac{n}{\sum_{i}^{} \log X_i}.
        \end{equation}
        $\hat\theta$ ist eindeutiger MLE.

    \item Nein, denn $\Theta$ offen ist.

\end{enumerate}







\problem{Konfidenzintervall. Mittelwert der Normalverteilung. }
Seien $X_1,\ldots,X_n$ i.i.d.\ normalverteilt mit $X_i \sim \mathcal N \left(\theta, 1 \right)$,
wobei $\theta\in\R$ ein unbekannter Parameter ist.
\begin{enumerate}
    \item Zeigen Sie, dass die Verteilung von 
        \begin{equation*}
            \sqrt{n}\left( \bar X -\theta \right)
        \end{equation*}
        unabhängig von dem Parameter $\theta$ ist. Dabei ist $\bar X = \frac{1}{n} \sum_{i=1}^{n} X_i$.
    \item Berechnen Sie ein $\left( 1-\alpha \right)$-Konfidenzintervall für $\theta$.
\end{enumerate}


\solution
\begin{enumerate}

    \item Die Verteilung von $\sqrt{n} \left( \bar X -\theta \right)$ ist
        \begin{eqnarray*}
            \sum_{i=1}^{n} X_i &\sim& \mathcal N \left( n\theta, n \right) \\
            \frac{1}{n} \sum_{i=1}^{n} X_i &\sim& \mathcal N \left( \theta, \frac{ n}{n^2} \right) \\
            \sqrt{n}\left( \bar X -\theta \right) & \sim & \mathcal N \left( 0, 1 \right),
        \end{eqnarray*}
        also unabhängig von $\theta$.
    \item Konfidenzintervall
        \begin{eqnarray*}
            P_\theta \left( z_{\frac{\alpha}{2}} \leq \sqrt{n}\left( \bar X -\theta \right) \leq z_{1-\frac{\alpha}{2}}  \right) &=&  1-\alpha \\
            P_\theta \left( \theta\in \left[ \bar X - \frac{z_{1-\frac{\alpha}{2}}}{\sqrt{n}}, \bar X - \frac{z_{\frac{\alpha}{2}}}{\sqrt{n}} \right] \right) &=&  1-\alpha.
        \end{eqnarray*}

\end{enumerate}



\problem{Gamma-Verteilung. Momentenschätzer.}
Gegeben seien i.i.d.\ Gamma-verteilte Zufallsvariablen $X_1,\ldots,X_m$ mit der Dichte
\begin{equation*}
    p_{a,\lambda}(x) = \frac{\lambda^a}{\Gamma(a)}x^{a-1}e^{-\lambda x} 1_{\R_{\geq 0}}(x)
\end{equation*}
und Parametern $a>0$ und $\lambda>0$.
\begin{enumerate}
    \item Zeigen Sie für ein positives $n\in\mathbb N$
        \begin{equation*}
            E X_i^n = \frac{a\cdot\ldots\cdot (n+a-1)}{\lambda^n}.
        \end{equation*}
        Dabei können Sie die Identität 
        $\Gamma(a+1)=a \Gamma(a)$, $a\in\R \setminus \left\{ -1,-2,\ldots \right\}$, verwenden.
    \item Berechnen Sie einen Momentenschätzer für die Parameter $a$ und $\lambda$ basierend auf
        der Beobachtung von $\left( X_1,\ldots,X_m \right)$. 
\end{enumerate}

\solution
\begin{enumerate}

    \item \begin{eqnarray*}
            E X^n &=& \int_{0}^{\infty} x^n \frac{\lambda^a}{\Gamma(a)}x^{a-1}e^{-\lambda x} dx \\
            &=& \frac{a \cdot\ldots\cdot \left( a+n-1 \right)}{\lambda^n} 
            \int_{0}^{\infty} \frac{\lambda^{a+n}}{\Gamma(n+a)} x^{a+n-1} e^{-\lambda x} dx \\
            &=& \frac{a\cdot \ldots \cdot \left( a+n-1 \right)}{\lambda^n} \\
            &=& \frac{\Gamma\left( a+n \right)}{ \lambda^n \Gamma\left( a \right)}.
        \end{eqnarray*}

    \item Die Momentenschätzer für die Parameter $a$ und $\lambda$ sind
        \begin{eqnarray*}
            E X &=& \frac{a}{\lambda} \\
            E X^2 &=&  \frac{a(a+1)}{\lambda^2} \\
            E X^2 &=&  \frac{a(a+1)}{a^2} \left( E X \right)^2 \\
            E X^2 &=& \left( 1 + \frac{1}{a} \right) \left( E X \right)^2 \\
            a &=& \frac{ \left( E X \right)^2 }{ E X^2 - \left( E X \right)^2}  \\
            \lambda &=&  \frac{ EX }{ E X^2 - \left( E X \right)^2} 
        \end{eqnarray*}
        Die Schätzer erhält man mit Substitutionen $EX \to \frac{1}{n} \sum_{i}^{} X$ und 
        $E X^2 \to \frac{1}{n} \sum_{i}^{} X^2$.


\end{enumerate}



\problem{Vollständigkeit.} Seien die Statistiken $T$ und $S$ mit $S=\eta(T)$ und einer messbaren 
Abbildung $\eta$ gegeben. 
\begin{enumerate}
    \item Zeigen Sie: Falls $T$ vollständig ist, so ist $S$ ebenfalls vollständig.
    \item Geben Sie ein Beispiel einer nicht vollständigen Statistik, in einem von Ihnen
        gewählten statistischen Modell.
    \item Zeigen Sie, dass die Umkehrung der ersten Aussage falsch ist: Aus Vollständigkeit von $S$ 
        folgt nicht die Vollständigkeit von $T$. Konstruieren Sie ein Gegenbeispiel.
\end{enumerate}


\solution
\begin{enumerate}

    \item $T$ ist vollständig, d.h.\ für ein messbares $g$ gilt, $E_\theta g(T) = 0$ für alle $\theta$ impliziert 
        $P_\theta \left( g(T)=0  \right)=1$ für alle $\theta$. Für ein $g$ ist
        $E g(S)=0 \ \forall \theta$ genau dann wenn $E g(\eta(T))=0 \ \forall \theta$, impliziert
        $P\left( g(\eta(T))=0 \right)=1 \ \forall \theta$. Das ist aber gleichbedeutend mit 
        $P\left( g(S)=0 \right)=1 \ \forall \theta$. $S$ ist also vollständig. 

    \item Sei $\Theta$ einelementig, $X\sim \mathcal N(0,1)$ und $T(X)=X$. Für $g=\textrm{id}$ ist zwar
        $E_\theta g(T(X)) = E_\theta X = 0 \ \forall \theta$ aber nicht 
        $P_\theta \left( g(T(X))=0 \right)=  P_\theta \left( X=0 \right)=1$. $T(X)$ ist also nicht vollständig. 

    \item Für eine beliebige unvollständige Statistik $T$ und $\eta\equiv 0$ ist $S=\eta(T)$ vollständig. 

\end{enumerate}







\problem{Informationsungleichung und Normalverteilung. } Gegeben seien $X_1,\ldots,X_n$
i.i.d.\ normalverteilte Zufallsvariablen mit $X_i\sim \mathcal N(\theta, \sigma^2)$, wobei $\theta\in\R$
und $\sigma$ bekannt und fix ist.
\begin{enumerate}
    \item Zeigen Sie, dass dieses Modell Cram\'er-Rao-Regularitätsbedingungen erfüllt.
    \item Berechnen Sie die Fischer-Information für den Parameter $\theta$.
    \item Berechnen Sie die Cram\'er-Rao-Schranke für die Varianz des Schätzers
        \begin{equation}
            T(X)=\bar X = \frac{1}{n} \sum_{i=1}^{n}X_i.
        \end{equation}
\end{enumerate}



\problem{Gamma-Verteilung. MLE.} Seien $X_1, \ldots,
X_n$ i.i.d.\ Gamma-verteilt mit der Dichte
\begin{equation*}
    p_{a,\lambda}(x) = \frac{\lambda^a}{\Gamma(a)}x^{a-1}e^{-\lambda x} 1_{\R_{>0}}(x)
\end{equation*}
und Parametern $a$ und $\lambda$. Der Parameter $a>0$ ist bekannt und fix. 
Der Parameter $\lambda$ dagegen unbekannt und $\lambda > 0$.
\begin{enumerate}
    \item Berechnen Sie einen Maximum-likelihood Schätzer für $\lambda$ 
        basierend auf der Beobachtung von dem Vektor
        $(X_1, \ldots, X_n)$. Zeigen Sie, dass der von Ihnen gefundene Schätzer 
        $\hat \lambda$ das Maximum der Likelihood-Funktion ist.
    \item Ist der von Ihnen gefundene Maximum-likelihood Schätzer erwartungstreu? 
        Begründen Sie Ihre Antwort. Dabei können Sie auf die Beziehung 
        \begin{equation*}
            X_1+ \ldots + X_n \sim \textrm{Gamma}(na,\lambda)
        \end{equation*}
        zugreifen.
\end{enumerate}

\solution
\begin{enumerate}
    \item Der Standardmethode folgend, berechnen wir die Log-likelihood
        Funktion und finden ihr Maximum.
        \begin{eqnarray}
            L(a,\lambda; X) &=& \frac{\lambda^{an}}{\Gamma(a)^n} \prod_i x_i^{ a-1 } e^{-\lambda x_i} \\
            l(a,\lambda; X) &=& na \log \lambda - n \log \Gamma\left( a \right) + 
                                \left( a-1 \right) \sum_{i}^{} \log x_i - \lambda \sum_{i}^{} x_i \\
            \frac{\partial}{\partial \lambda} l(a,\lambda; X) &=&  \frac{na}{\lambda} - \sum_{i}^{} x_i = 0  \\
            \hat \lambda &=& \frac{na}{\sum_{i}^{} x_i} \\
            \frac{\partial^2}{\partial \lambda^2} l(a,\lambda; X) &=& - \frac{na}{ \lambda^2} <0 \quad \forall \lambda>0.
        \end{eqnarray}
        MLE ist also tatsächlich das Maximum der Likelihood Funktion. 
    \item Unter Benutzung von $X_1+ \ldots + X_n \sim \textrm{Gamma}(na,\lambda)$ berechnen wir
        \begin{eqnarray}
            \E \hat \lambda &=& \E \frac{na}{ \sum_{i}^{} X_i }  \\
            &=& \int_{0}^{\infty} \frac{na}{x} \frac{\lambda^{na}}{\Gamma(na)} x^{na-1} e^{-\lambda x} dx \\
            &=& \frac{na \lambda^{na}}{\Gamma(na)} \int_{0}^{\infty} x^{na -1-1} e^{-\lambda x}dx \\
            &=& \frac{na \lambda^{na}}{\Gamma(na)} \frac{\Gamma(na-1)}{\lambda^{na-1}} \\
            &=& \frac{na}{na-1} \lambda.
        \end{eqnarray}
        Der Schätzer $\hat\lambda$ ist somit nicht erwartungstreu.
\end{enumerate}










\problem{UMVUE-Schätzer für Poisson-Verteilung. } Seien $X_1, \ldots, X_n$ i.i.d.\ 
und Poissonverteilt mit der Wahrscheinlichkeitsfunktion
\begin{equation*}
    p(k) = \bP \left( X_i = k \right) = e^{-\lambda} \frac{\lambda^k}{ k!} \quad k \in \left\{ 0,1, \ldots \right\}
\end{equation*}
und $\lambda>0$.
\begin{enumerate}
    \item Zeigen Sie, dass die Statistik $T\left( X \right) = \sum_{i=1}^{n}
        X_i$ vollständig und suffizient für dieses Testproblem ist.
    \item Zeigen Sie, dass der Schätzer $S\left( X \right) = \frac{1}{n}
        \sum_{i=1}^{n} X_i$ von $\lambda$ unverzerrt ist.
    \item Zeigen Sie, dass $S(X)$ ein eindeutiger \textsc{umvue}-Schätzer für
        $\lambda$ ist.
\end{enumerate} 

\solution
\begin{enumerate}
    \item Die Wahrscheinlichkeitsfunktionen von $\left( X_1, \ldots, X_n \right)$ 
        bilden eine exponentielle Familie, denn
        \begin{align*}
            \prod_{i=1}^{n} p(x_i) &= \prod_{i=1}^n e^{-\lambda } \frac{ \lambda^{ x_i }}{ x_i!} \\
            &= \exp \left( -\lambda n + \log \lambda \cdot \sum_{i=1}^{ n} x_i 
            - \sum_{i=1}^{n} \log x_i! \right).
        \end{align*}
        Somit ist $T(X)= \sum_{i}^{} X_i$ eine suffiziente Statistik. $T(X)$
        ist auch vollständig, denn $\left\{ \log \lambda \ | \ \lambda>0
        \right\}$ enthält ein offenes Rechteck. Die Vollständigkeit kann man
        hier auch leicht aus der Definition herleiten. 
    \item Wir berechnen
        \begin{align*}
            \E X_i &= \sum_{k\geq 0}^{} k e^{-\lambda} \frac{\lambda^k}{ k!} = \lambda.
        \end{align*}
        Daraus ergibt sich
        \begin{align*}
            \E S(X) &= \frac{1}{n} \sum_{i}^{} \E X_i = \lambda.
        \end{align*}
        $S(X)$ ist also ein unverzerrter Schätzer von $\lambda$. 
    \item Nachdem $S(X) = \frac{1}{n} T(X)$ gilt, ist 
        \begin{equation*}
            \E \left( S(X) | T(X) \right) = S(X)
        \end{equation*}
        und somit ist $S(X)$ nach dem Satz von Lehmann-Scheffe ein
        \textsc{umvue}-Schätzer für $\lambda$. Die Eindeutigkeit folgt aus
        $\Var_\lambda T(X) < \infty$ für alle $\lambda >0$. Das ist der Fall
        denn
        \begin{align*}
            \E X^2 &= \lambda\left( \lambda +1 \right) \\
            \Var X &= \lambda.
        \end{align*}
\end{enumerate}









\problem{Gamma-Verteilung. Konfidenzintervall und Test.}
Seien $X_1, \ldots, X_n$ i.i.d.\ Gamma-verteilt mit der Dichte
\begin{equation*}
    p_{a,\lambda} (x) = \frac{\lambda^a}{ \Gamma(a)} x^{a-1} e^{-\lambda x} 1_{ \R_{>0}  } (x)
\end{equation*} 
und Parametern $a>0$ und $\lambda>0$. Der Parameter $a$ wird als bekannt und fix vorausgesetzt. 
Des weiteren gelten für $c>0$ die Beziehungen
\begin{eqnarray*}
    X_i + X_j &\sim& \textrm{Gamma}_{a+a, \lambda}, \\
    c X_i & \sim & \textrm{Gamma}_{a, \frac{\lambda}{c}}.
\end{eqnarray*}
\begin{enumerate}
    \item Zeigen Sie, dass \begin{eqnarray*}
            \left[  \frac{ \textrm{Gamma}^{-1}_{na, 1} ( \frac{\alpha}{2})  }{X_1 + \ldots + X_n}, 
            \frac{ \textrm{Gamma}^{-1}_{na, 1} ( 1- \frac{\alpha}{2})  }{X_1+ \ldots + X_n}  \right]
        \end{eqnarray*} ein $(1-\alpha)$-Konfidenzintervall für $\lambda$ ist.
        Dabei bezeichnet $\textrm{Gamma}^{-1}_{a,\lambda}$ die Inverse der Verteilungsfunktion 
        $\textrm{Gamma}_{a,\lambda}$.
    \item Konstruieren Sie aus dem obigen Konfidenzintervall einen Test zum Signifikanzniveau
        $\alpha$ für $H_0: \lambda=\lambda_0$ gegen $H_1: \lambda\neq \lambda_0$.
\end{enumerate} 

\solution
\begin{enumerate}
    \item Die Verteilung von $\lambda (X_1 + \ldots + X_n)$ ist $\textrm{Gamma}_{na, 1}$.
    \item Mit der Bezeichnung 
        \begin{equation*}
            \left[ L(X), U(X) \right] = 
            \left[  \frac{ \textrm{Gamma}^{-1}_{na, 1} ( \frac{\alpha}{2})  }{X_1 + \ldots + X_n}, 
            \frac{ \textrm{Gamma}^{-1}_{na, 1} ( 1- \frac{\alpha}{2})  }{X_1+ \ldots + X_n}  \right]
        \end{equation*}
        erhalten wir die Testfunktion
        \begin{equation*}
            \delta(X) = 1_{ \left\{ \lambda \nin \left[ L(X), U(X) \right] \right\}   }
        \end{equation*} die das Gewünschte erfüllt. 
\end{enumerate}












\problem{Beta-Verteilung. Momentenschätzer.} Gegeben seien $X_1, \ldots, X_n$ i.i.d. Beta-verteilte
Zufallsvariablen mit der Dichte
\begin{equation*}
    p_{a,b}(x) = \frac{1}{B(a,b)} x^{a-1} (1-x)^{b-1} 1_{ \left[ 0,1 \right]  } (x)
\end{equation*}
und Parametern $a>0$ und $b>0$. Dabei ist $B(a,b)$ die Betafunktion charakterisiert durch
\begin{equation*}
    B(a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}.
\end{equation*}
\begin{enumerate}
    \item Zeigen Sie, dass das $k$-te Moment der Beta-Verteilung die Gleichung
        \begin{equation*}
            \E X^{k} = \frac{a+k-1}{a+b+k-1} \E X^{k-1}
        \end{equation*}
        für $k\in \mathbb N$ und $k\geq 1$ erfüllt. 
    \item Berechnen Sie den Momentenschätzer für die Parameter $a$ und $b$ basierend auf dem ersten und zweiten Moment.
\end{enumerate}

\solution
\begin{enumerate}
    \item Durch direktes Berechnen des $k$-ten Moments erhalten wir 
        \begin{align*}
            \E X^k &= \int_{0}^{1} x^k \frac{1}{B(a,b)} x^{a-1}(1-x)^{b-1} dx \\
            &= \frac{B(a+k,b)}{B(a,b)} \int_{0}^{1} \frac{1}{B(a+k,b)} x^{a+k-1} (1-x)^{b-1} dx \\
            &= \frac{B(a+k,b)}{b(a,b)}.
        \end{align*}
        Und daraus
        \begin{align*}
            \E X^k &= \frac{B(a+k,b)}{B(a,b)} \\
            &= \frac{\Gamma(a+k)\Gamma(b)}{\Gamma(a+b+k)} \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \\
            &= \frac{ \Gamma(a+k) \Gamma(a+b) }{\Gamma(a+b+k) \Gamma(a)} \\
            &= \frac{a+k-1}{a+b+k-1} \E X^{k-1}.
        \end{align*}
        Es gilt auch
        \begin{equation*}
            \E X^k = \frac{ a \cdot \ldots \cdot (a+k-1) }{ (a+b)\cdot \ldots \cdot (a+b+k-1)}.
        \end{equation*}
    \item Die Substitutionen $E X = \hat m_1 = \frac{1}{n} \sum_{i=1}^{n} X_i$ und 
        $E X^2 = \hat m_2 = \frac{1}{n} \sum_{i=1}^{n} X_i^2$ führen zu 
        \begin{eqnarray*}
            \hat m_1 &=&  \frac{a}{a+b} \\
            \hat m_2 &=&  \frac{ a(a+1) }{ (a+b)(a+b+1) }.
        \end{eqnarray*}
        Die Auflösung nach $a$ und $b$ liefert 
        \begin{eqnarray*}
            \hat a &=& \frac{ \hat m_1^2 - \hat m_2^2 }{ \hat m_2 - \hat m_1^2  } \\
            \hat b &=& \frac{a (1-\hat m_1)}{\hat m_1} = \frac{ (\hat m_1^2 - \hat m_2^2)(1-\hat m_1) }{(\hat m_2 - \hat m_1^2)\hat m_1}.
        \end{eqnarray*}
\end{enumerate}









\problem{Turmeigenschaft der bedingten Erwartung.} Sei ein
Wahrscheinlichkeitsraum $\left( \Omega, P, \mathcal A \right)$, eine
integrierbare Zufallsvariable $X$ und $\mathcal G, \mathcal H$
sub-$\sigma$-Algebren von $\mathcal A$ gegeben. Es gilt $\mathcal H \subset
\mathcal G \subset \mathcal A$. $Y$ ist bedingte Erwartung von $X$ bezüglich $\cA$, notiert
als $Y = \E \left[ X | \cA \right]$, genau dann wenn $\E 1_A Y = \E 1_A X$ für alle $A\in\cA$ gilt.
\begin{enumerate}
    \item Zeigen Sie $\E \left[  \E \left[ X | \mathcal H \right] | \mathcal G \right] = \E \left[ X | \mathcal H \right]$ $P$-fast sicher.
    \item Zeigen Sie $\E \left[  \E \left[ X | \mathcal G \right] | \mathcal H \right] = \E \left[ X | \mathcal H \right]$ $P$-fast sicher.
\end{enumerate}

\solution Wir zeigen zunächst 
$\E\left[ \E\left[ X | \mathcal H \right] | \mathcal G \right]=\E \left[ X | \mathcal H \right]$.
Nach Definition des bedingten Erwartungswertes ist $\E\left[ X| \mathcal H \right]$ 
$\mathcal H$-messbar und daher $\mathcal G$-messbar. So bekommen wir 
\begin{equation}
	\E\left[ \E\left[ X | \mathcal H \right] | \mathcal G \right] =
	\E\left[ X| \mathcal H \right] \E\left[ 1|\mathcal G \right] = 
	\E \left[ X | \mathcal H \right ].
\end{equation}
Nun kommen wir zu 
$\E\left[ \E\left[ X | \mathcal G \right] | \mathcal H \right]=\E \left[ X | \mathcal H \right]$.
Dies gilt genau dann wenn 
\begin{equation}
	\E\left\{ X 1_A \right\} 
	= \E\left\{ \E\left[ \E\left[ X|\mathcal G \right]|\mathcal H \right] 1_A \right\}  
	\quad \forall A\in\mathcal H.
\end{equation}
Sei $Y=\E\left[ X|\mathcal G \right]$ eine $\mathcal G$-messbare Zufallsvariable und
$A\in\mathcal H$. Die Indikatorfunktion $1_A$ ist $\mathcal H$-messbar und daher
$\mathcal G$-messbar. Wir berechnen
\begin{eqnarray}
	\E\left\{ \E\left[ \E\left[ X|\mathcal G \right]|\mathcal H \right] 1_A \right\} &=& 
	\E\left\{ \E\left[ Y|\mathcal H \right] 1_A \right\} \\
	&=& \E\left\{ \E\left[ Y 1_A| \mathcal H \right] \right\} \\
	&=& \E\left\{ Y 1_A \right\} \\
	&=& \E\left\{ \E\left[ X|\mathcal G \right] 1_A \right\} \\
	&=& \E\left\{ \E\left[ X 1_A| \mathcal G \right] \right\} = \E\left\{ X 1_A \right\}. 
\end{eqnarray}
Daher ist nach Definition des bedingten Erwartungswertes
\begin{equation}
	\E\left[ \E\left[ X | \mathcal G \right] | \mathcal H \right]=\E \left[ X | \mathcal H \right].
\end{equation}



\problem{L\'evy-Verteilung. Dichte und MLS.} Die Dichte der L\'evy-Verteilung mit den
Parametern $(\mu,\sigma)\in \Theta = \R \times \R_{>0}$ ist
\begin{equation*}
    p_{(\mu, \sigma)}(x) = \sqrt{ \frac{\sigma}{2\pi}} (x-\mu)^{-\frac{3}{2}} 
    \exp\left( -\frac{\sigma}{2(x-\mu)} \right) 1_{\R_{\geq \mu}} (x).
\end{equation*}
\begin{enumerate}
    \item Sei $X\sim \cN (0, \frac{1}{\sigma})$ normalverteilt mit $\sigma>0$.
        Zeigen Sie, dass die Zufallsvariable $\frac{1}{X^2}$ L\'evy-verteilt mit den Parametern
        $(0,\sigma)$ ist.
    \item Sei $\mu=0$. Zeigen Sie, dass $(p_{(0,\sigma)})_{\sigma>0}$ eine
        exponentielle Familie ist. 
    \item Sei $\mu=0$. Berechnen Sie einen Maximum-Likelihood-Schätzer für $\sigma$, und
        klären Sie ob dieser eindeutig ist.
\end{enumerate}

\solution 
\begin{enumerate}
    \item Für ein $y>0$ gilt
        \begin{align*}
            P \left( \frac{1}{X^{2}} \leq y \right) &= P \left( 1 \leq y X^{2}   \right) \\
            &= P \left( \sqrt{\frac{1}{y}} \leq | X | \right) \\
            &= 2 \left( 1 - P \left( X < \frac{1}{\sqrt{y}} \right) \right) \\
            &= 2 \left( 1 - \Phi\left( \sqrt{ \frac{\sigma}{y}} \right) \right).
        \end{align*}
        Die Dichte von $\frac{1}{X^2}$ erhalten wir durchs Differenzieren:
        \begin{align*}
            \frac{\partial}{\partial y} P \left( \frac{1}{X^2} \leq y \right) &=
            -2 \phi \left( \sqrt{\frac{\sigma}{y}} \right) \sqrt{\sigma} 
            \left( -\frac{1}{2} y^{-\frac{3}{2}} \right) \\
            &= \sqrt{\frac{\sigma}{2 \pi}} \exp \left( - \frac{\sigma}{2 y} \right) y^{-\frac{3}{2}}.
        \end{align*}
        Daher ist $\frac{1}{X^2} \sim \text{L\'evy}(\sigma)$.

    \item Für $\mu=0$ lässt sich die Dichte der L\'evy-Verteilung in der Form
        \begin{equation*}
            p_{0,\sigma}(y) = \exp \left( -\frac{\sigma}{2y} - \frac{3}{2}\log y 
            + \frac{1}{2} \log \frac{\sigma}{2 \pi}  \right) 1_{\R_{\geq 0}} (y)
        \end{equation*}
        darstellen.

    \item Der MLS ist
        \begin{equation*}
            \hat \sigma = \frac{n}{ \sum_{}^{} 1/x_i}.
        \end{equation*}
\end{enumerate}


\problem{L\'evy-Verteilung.} Die Dichte der L\'evy-Verteilung mit den
Parametern $(\mu,\sigma)\in \Theta = \R \times \R_{>0}$ ist
\begin{equation*}
    p_{(\mu, \sigma)}(x) = \sqrt{ \frac{\sigma}{2\pi}} (x-\mu)^{-\frac{3}{2}} 
    \exp\left( -\frac{\sigma}{2(x-\mu)} \right) 1_{\R_{\geq \mu}} (x).
\end{equation*}
Zeigen Sie folgende Aussagen.
\begin{enumerate}
    \item Sei $X\sim \cN (0, \frac{1}{\sigma})$ normalverteilt mit $\sigma>0$.
        Die Zufallsvariable $\frac{1}{X^2}$ ist L\'evy-verteilt mit den Parametern
        $(0,\sigma)$.
    \item Wenn $\mu=0$ festgehalten wird, ist die Familie $(p_{(0,\sigma)})_{\sigma>0}$
        eine exponentielle Familie. 
    \item Die Familie $(p_{\mu,\sigma})_{(\mu,\sigma)\in\Theta}$ ist keine
        (zweiparametrige) exponentielle Familie. 
\end{enumerate}

\solution 
\begin{enumerate}
    \item Für ein $y>0$ gilt
        \begin{align*}
            P \left( \frac{1}{X^{2}} \leq y \right) &= P \left( 1 \leq y X^{2}   \right) \\
            &= P \left( \sqrt{\frac{1}{y}} \leq | X | \right) \\
            &= 2 \left( 1 - P \left( X < \frac{1}{\sqrt{y}} \right) \right) \\
            &= 2 \left( 1 - \Phi\left( \sqrt{ \frac{\sigma}{y}} \right) \right).
        \end{align*}
        Die Dichte von $\frac{1}{X^2}$ erhalten wir durchs Differenzieren:
        \begin{align*}
            \frac{\partial}{\partial y} P \left( \frac{1}{X^2} \leq y \right) &=
            -2 \phi \left( \sqrt{\frac{\sigma}{y}} \right) \sqrt{\sigma} 
            \left( -\frac{1}{2} y^{-\frac{3}{2}} \right) \\
            &= \sqrt{\frac{\sigma}{2 \pi}} \exp \left( - \frac{\sigma}{2 y} \right) y^{-\frac{3}{2}}.
        \end{align*}
        Daher ist $\frac{1}{X^2} \sim \text{L\'evy}(\sigma)$.

    \item Für $\mu=0$ lässt sich die Dichte der L\'evy-Verteilung in der Form
        \begin{equation*}
            p_{0,\sigma}(y) = \exp \left( -\frac{\sigma}{2y} - \frac{3}{2}\log y 
            + \frac{1}{2} \log \frac{\sigma}{2 \pi}  \right) 1_{\R_{\geq 0}} (y)
        \end{equation*}
        darstellen.

    \item Im allgemeinen Fall ist der Träger der Dichte $p_{\mu,\sigma}$ von $\mu$ abhängig und daher
        kann $p_{\mu,\sigma}$ keine exponentielle Familie sein.

\end{enumerate}






\problem{Log-Normal-Verteilung. MLS und UMVUE. }
Gegeben seien i.i.d.\ Zufallsvariablen $X_1, \cdots, X_n$  mit der Dichte
\begin{equation*}
    g_{\mu}(x) = \frac{1}{x \sqrt{2\pi}} \exp \left( - \frac{ \left( \log x - \mu \right)^2 }{2} \right) \ 1_{\R_{>0}}(x)
\end{equation*}
und dem unbekannten Parameter $\mu \in \R$. 
\begin{enumerate}
    \item Finden Sie einen Maximum-Likelihood-Schätzer (MLS) für den Parameter $\mu$. 
%    \item Zeigen Sie, dass die gemeinsame Dichte des Zufallsvektors $\left(
%        X_1, \cdots, X_n \right)$ einer einparametrigen exponentiellen Familie
%        angehört. 
    \item Zeigen Sie, dass der von Ihnen gefundene MLS erwartungstreu ist.
        Dabei können Sie die Darstellung als natürliche exponentielle
        Familie benutzen oder direkt den Erwartungswert von dem MLS berechnen. 
    \item Finden Sie einen UMVUE Schätzer für $\mu$. 
\end{enumerate}

\solution Einer Standardprozedur folgend berechnen wir den MLS. 
\begin{align*}
    L(x_1, \cdots, x_n) &= \prod x_i^{-1} \, \sqrt{2 \pi}^{-n} 
    \exp \left( - \frac{1}{2} \sum_{i=1}^{n} \left( \log x_i - \mu \right)^2 \right) \\
    l(x_1, \cdots, x_n) &= - \sum_{i}^{} x_i - \frac{n}{2} \log 2\pi \ -\frac{1}{2} \sum_{i}^{} \left( \log x_i -\mu \right)^2 \\
    \frac{\partial}{\partial \mu} l &= \sum_{i}^{} \left( \log x_i -\mu \right) = \sum_{i}^{} \log x_i - n\mu. \\
    \frac{\partial^2}{\partial \mu^2} l &= -n <0 \\
    \hat \mu &= \frac{1}{n} \sum_{i}^{} \log x_i.
\end{align*}
Die Darstellung als natürliche exponentielle Familie ist
\begin{equation*}
    g(x) = \exp \left( \mu \log x - \frac{\mu^2}{2} - \log x - \frac{\left( \log x \right)^2}{2} - \log \sqrt{2\pi} \right).
\end{equation*}
Daraus erhalten wir
\begin{align*}
    g(x_1, \cdots, x_n) &= \exp \left( c(\mu) T(X) + d_0 (\mu) + S(x) \right) 1_{\R_{>0}}(x) \\
    c(\mu) &= \mu \\
    T(X) &= \sum_{i}^{} \log x_i \\
    d_0(\mu) &= - \frac{n \mu^2}{2}.
\end{align*}
Mit Hilfe der Formel 
\begin{equation*}
    \E\ T(X) = - d_0'(\mu) = n \mu. 
\end{equation*}
können wir nun den Erwartungswert der natürlichen suffizienten Statistik $T(X)$
berechnen.  Damit ist der MLS $\hat \mu = \frac{1}{n} \sum_{i}^{} \log x_i$ ein
erwartungstreuer Schätzer. Nachdem $c(\R)$ offenes Rechteck enthält, ist die
Statistik $T(X)$ vollständig und suffizient, und nach dem Satz von
Lehmann-Scheff\'e ist $\hat \mu$ ein UMVUE Schätzer. 





\problem{Dreiecksverteilung. Momentenschätzer.} Gegeben seien
i.i.d.\ Zufallsvariablen $X_1,\ldots, X_n$ mit der Dichte 
\begin{align*}
    \Lambda_a(x) &= \left[ \frac{1}{a} - \frac{|x|}{a^2} \right] 1_{[-a,a]}(x)
\end{align*}
und dem unbekannten Parameter $a>0$. 
\begin{enumerate}
    \item Zeigen Sie, dass die geraden Momente von $X_1$ durch die Formel
        \begin{equation*}
            \E X_1^n = 2 a^n \left( \frac{1}{n+1} - \frac{1}{n+2} \right)
        \end{equation*}
        gegeben sind und alle ungeraden Momente verschwinden. 
    \item Geben Sie einen Momentenschätzer für den Parameter $a$ an.
\end{enumerate}

\solution 
\begin{enumerate}
    \item Die Momente ergeben sich aus
        \begin{align*}
            \E X_1^n &= \int_{-a}^{a} \left( \frac{1}{a} - \frac{|x|}{a^2} \right) x^n dx.
        \end{align*}
        Für gerade $n$ ist $\E X_1^n=0$. Für ungerade $n$ erhalten wir
        \begin{align*}
            \E X_1^{n} &= 2 \int_{0}^{a} \left( \frac{1}{a} - \frac{x}{a^2} \right) x^{n} \\
            &= \frac{2}{a} \frac{1}{n+1} a^{n+1} - \frac{2}{a^2} \frac{1}{n+2}a^{n+2} \\
            &= 2 a^{n} \left( \frac{1}{n+1}- \frac{1}{n+2} \right).
        \end{align*}
    \item Aus $\E X_1^2 = \frac{a^2}{6}$ ergibt sich ein Momentenschätzer für
        $a$ in der Form $\hat a = \sqrt{6 m_2}$ mit $m_2 = \sum_{i}^{} X_i^2$. 
\end{enumerate}





\problem{Gleichverteilung. Konfidenzintervall.} Seien $X_1, \cdots, X_n$
i.i.d.\ und stetig gleichverteilt auf dem Intervall $\left[ 0,\theta \right]$
mit $\theta>0$. D.h.\  die Dichte von $X_1\sim U(0,\theta)$ ist $p_\theta(x) = \theta^{-1}
1_{\left[ 0,\theta \right]}(x)$.
\begin{enumerate}
    \item Zeigen Sie, dass die Zufallsvariable $\frac{X_{\left( n \right)}}{\theta}$ ein
        Pivot ist. Dabei ist $X_{\left( n \right)} = \max \left\{ X_1, \cdots, X_n \right\}$. 
    \item Zeigen Sie, dass 
        \begin{equation*}
            \left[ \frac{X_{\left( n \right)}}{ \left( 1- \frac{\alpha}{2} \right)^{1/n}},
            \frac{X_{\left( n \right)}}{ \left( \frac{\alpha}{2} \right)^{1/n}}
            \right]
        \end{equation*}
        ein $(1-\alpha)$-Konfidenzintervall für $\theta$ ist. 
\end{enumerate}

\solution Wir berechnen zunächst die Verteilung von $X_{(n)}$. Für $0 \leq \eta \leq \theta$
gilt
\begin{align*}
    P\left( X_{(n)} < \eta \right) &= P \left( X_1 < \eta \wedge \cdots \wedge X_n < \eta \right) \\
    &= \prod_{i=1}^{n} P \left( X_i < \eta \right) \\
    &= \left( \frac{\eta}{\theta} \right)^n.
\end{align*}
Es gilt also $P \left( X_{(n)}/\theta < \eta \right) = \eta^{n}$. Damit erhalten wir 
\begin{equation*}
    P \left( \theta \in 
        \left[ \frac{X_{\left( n \right)}}{ \left( 1- \frac{\alpha}{2} \right)^{1/n}},
            \frac{X_{\left( n \right)}}{ \left( \frac{\alpha}{2} \right)^{1/n}}
        \right]
    \right) = 1 - \alpha.
\end{equation*}







